#@+leo-ver=5-thin
#@+node:ekr.20100120072650.6089: * @file ../doc/leoProjects.txt
#@+all
#@+node:ekr.20111103205308.9698: ** Unit tests
import leo.core.leoImport as leoImport
ic = c.importCommands
hs = leoImport.htmlScanner(importCommands=ic,atAuto=True)

s1 = '''
<table id="1"> <table id="2">
<contents/>
</table>
</table>
'''

s2 = '''
<table id="1"> 
<table id="2">
<contents/>
</table>
</table>
'''

t1 = 
assert result == expected,'expected...\n%s\ngot...\n%s' % (
    repr(expected),repr(result))
#@+node:ekr.20100907115157.5905: *3* @ignore Ancient tests
#@+node:ekr.20100208095423.5940: *4* @test leoCache
import leo.core.leoCache as leoCache

cacher = leoCache.cacher(c)

if 0:
    import os
    os.system('cls')

assert cacher.test()
#@+node:ekr.20100906165118.5915: *4* @test leoInkCommands
ic = c.inkscapeCommands
screenshot = r'c:\leo.repo\inkcall\some_screen_shot.png'
template_fn = r'c:\leo.repo\inkcall\template.svg'
png_fn = r'c:\leo.repo\inkcall\output.png'
svg_fn = r'c:\leo.repo\inkcall\temp.svg'
callouts = [
        "This goes here",
        "These are those, but slightly longer",
        "Then you pull this, but this text needs to be longer for testing",]
ic.run(
    screenshot,
    callouts=callouts,
    numbers=[2,4,17],
    edit_flag = True, # True: call inkscape to edit the working file.
    png_fn=png_fn, # Optional: Name of output png file.
    svg_fn=svg_fn, # Optional: Name of working svg file.
    template_fn=template_fn, # Optional: Name of template svg file.
)
#@+node:ekr.20111102123707.9629: *4* @ignore test of marked unit-test trees
#@+node:ekr.20111102123707.9630: *5* @test assert False
assert False
#@+node:ekr.20111102123707.9631: *5* @test assert True
assert True
#@+node:ekr.20111107092526.9799: *4* @test detection of external unit tests
# This test is redundant, and another test sets import_html_tags

# print('g.app.isExternalUnitTest',g.app.isExternalUnitTest)
if g.app.isExternalUnitTest:
    fn = c.shortFileName()
    assert fn.endswith('dynamicUnitTest.leo'),fn
    data = c.config.getData('import_html_tags')
    assert len(data) == 85 # length of data in leoSettings.leo.
else:
    data = c.config.getData('import_html_tags')
    assert len(data) == 85,len(data)
#@+node:ekr.20111107092526.9800: *5* doTests...
def doTests(c,all=None,marked=None,p=None,verbosity=1):

    trace = False ; verbose = False
    if all:
        p = c.rootPosition()
    elif not p:
        p = c.p
    p1 = c.p.copy() # 2011/10/31: always restore the selected position.
    
    g.trace(g.app.isExternalUnitTest)
    
    # This seems a bit risky when run in unitTest.leo.
    # c.save() # Eliminate the need for ctrl-s.
    
    if trace: g.trace('marked',marked,'c',c)

    try:
        g.unitTesting = g.app.unitTesting = True
        g.app.unitTestDict["fail"] = False
        g.app.unitTestDict['c'] = c
        g.app.unitTestDict['g'] = g
        g.app.unitTestDict['p'] = p and p.copy()

        # c.undoer.clearUndoState() # New in 4.3.1.
        changed = c.isChanged()
        suite = unittest.makeSuite(unittest.TestCase)

        # New in Leo 4.4.8: ignore everything in @ignore trees.
        last = None if all else p.nodeAfterTree()
        
        aList = findAllUnitTestNodes(c,p,last,all,marked,
            lookForMark=False,lookForNodes=True)
       
        found = False
        for p in aList:
            if isTestNode(p):
                if trace: g.trace('adding',p.h)
                test = makeTestCase(c,p)
            elif isSuiteNode(p): # @suite
                if trace: g.trace('adding',p.h)
                test = makeTestSuite(c,p)
            else:
                test = None
            if test:
                suite.addTest(test)
                found = True
        
        # Verbosity: 1: print just dots.
        if not found:
            # 2011/10/30: run the body of p as a unit test.
            test = makeTestCase(c,c.p)
            if test:
                suite.addTest(test)
                found = True
        if found:
            res = unittest.TextTestRunner(verbosity=verbosity).run(suite)
            # put info to db as well
            if g.enableDB:
                key = 'unittest/cur/fail'
                archive = [(t.p.gnx, trace) for (t, trace) in res.errors]
                c.cacher.db[key] = archive
        else:
            g.error('no %s@test or @suite nodes in %s outline' % (
                g.choose(marked,'marked ',''),
                g.choose(all,'entire','selected')))
    finally:
        c.setChanged(changed) # Restore changed state.
        if g.app.unitTestDict.get('restoreSelectedNode',True):
            c.contractAllHeadlines()
            c.redraw(p1)
        g.unitTesting = g.app.unitTesting = False
#@+node:ekr.20111107092526.9801: *6* class generalTestCase
class generalTestCase(unittest.TestCase):

    """Create a unit test from a snippet of code."""

    @others
#@+node:ekr.20111107092526.9802: *7* __init__ (generalTestCase)
def __init__ (self,c,p):

     # Init the base class.
    unittest.TestCase.__init__(self)

    self.c = c
    self.p = p.copy()
#@+node:ekr.20111107092526.9803: *7*  fail
def fail (self,msg=None):

    """Mark a unit test as having failed."""

    import leo.core.leoGlobals as g

    g.app.unitTestDict["fail"] = g.callers()
#@+node:ekr.20111107092526.9804: *7* tearDown
def tearDown (self):

    pass

    # Restore the outline.
    self.c.outerUpdate()
#@+node:ekr.20111107092526.9805: *7* setUp
def setUp (self):

    c = self.c ; p = self.p

    c.selectPosition(p.copy()) # 2010/02/03
#@+node:ekr.20111107092526.9806: *7* runTest (generalTestCase)
def runTest (self,define_g = True):

    trace = False
    c = self.c ; p = self.p.copy()
    script = g.getScript(c,p).strip()
    self.assert_(script)
    
    if c.shortFileName() == 'dynamicUnitTest.leo':
        c.write_script_file = True

    # New in Leo 4.4.3: always define the entries in g.app.unitTestDict.
    g.app.unitTestDict = {'c':c,'g':g,'p':p and p.copy()}

    if define_g:
        d = {'c':c,'g':g,'p':p and p.copy(),'self':self,}
    else:
        d = {'self':self,}

    script = script + '\n'
    if trace: g.trace('p: %s c: %s write script: %s script:\n%s' % (
        p and p.h,c.shortFileName(),c.write_script_file,script))

    # Execute the script. Let the unit test handle any errors!
    # 2011/11/02: pass the script sources to exec or execfile.
    if c.write_script_file:
        scriptFile = c.writeScriptFile(script)
        if g.isPython3:
            exec(compile(script,scriptFile,'exec'),d)
        else:
            execfile(scriptFile,d)
    else:
        exec(script,d)
#@+node:ekr.20111107092526.9807: *7* shortDescription
def shortDescription (self):

    s = self.p.h

    # g.trace(s)

    return s + '\n'
#@+node:ekr.20111107092526.9808: *6* makeTestSuite (leoTest)
@ This code executes the script in an @suite node.  This code assumes:
- The script creates a one or more unit tests.
- The script puts the result in g.app.scriptDict["suite"]
@c

def makeTestSuite (c,p):

    """Create a suite of test cases by executing the script in an @suite node."""

    p = p.copy()
    # g.trace('c.write_script_file',c.write_script_file)
    script = g.getScript(c,p).strip()
    if not script:
        print("no script in %s" % h)
        return None
    try:
        if 0: #debugging
            n,lines = 0,g.splitLines(script)
            for line in lines:
                print(n,line)
                n += 1
                
        # 2011/11/02: make script sources available.
        d = {'c':c,'g':g,'p':p}
        if c.write_script_file:
            scriptFile = c.writeScriptFile(script)
            if g.isPython3:
                exec(compile(script,scriptFile,'exec'),d)
            else:
                execfile(scriptFile,d)
        else:
            exec(script + '\n',d)
        suite = g.app.scriptDict.get("suite")
        if not suite:
            print("makeTestSuite: %s script did not set g.app.scriptDict" % p.h)
        return suite
    except Exception:
        print('makeTestSuite: exception creating test cases for %s' % p.h)
        g.es_exception()
        return None
#@+node:ekr.20111107092526.9809: *6* makeTestCase
def makeTestCase (c,p):

    p = p.copy()

    if p.b.strip():
        return generalTestCase(c,p)
    else:
        return None
#@+node:ekr.20111107092526.9810: *5* main & helpers (leoDynamicTest.py)
def main ():

    trace = False
    readSettings = True 
    tag = 'leoDynamicTests.leo'
    if trace: t1 = time.time()

    # Setting verbose=True prints messages that would be sent to the log pane.
    path,gui,silent = scanOptions()
    # print('(leoDynamicTest.py:main)','silent',silent)

    # Not loading plugins and not reading settings speeds things up considerably.
    bridge = leoBridge.controller(gui=gui,
        loadPlugins=False, # Must be False: plugins will fail when run externally.
        readSettings=True, # True adds about 0.3 seconds.  Is it useful?
        silent=True,
        verbose=False)

    if trace:
         t2 = time.time()
         print('%s open bridge:  %0.2fsec' % (tag,t2-t1))

    if bridge.isOpen():
        g = bridge.globals()
        g.app.silentMode = silent
        g.app.isExternalUnitTest = True
        path = g.os_path_finalize_join(g.app.loadDir,'..','test',path)
        c = bridge.openLeoFile(path)
        if trace:
            t3 = time.time()
            print('%s open file: %0.2fsec' % (tag,t3-t2))
        runUnitTests(c,g)
#@+node:ekr.20111107092526.9811: *6* runUnitTests
def runUnitTests (c,g):

    p = c.rootPosition()
    #g.es_print('running dynamic unit tests...')
    c.selectPosition(p)
    c.debugCommands.runAllUnitTestsLocally()
#@+node:ekr.20111107092526.9812: *6* scanOptions
def scanOptions():

    '''Handle all options and remove them from sys.argv.'''

    parser = optparse.OptionParser()
    parser.add_option('--path',dest='path')
    parser.add_option('--gui',dest="gui")
    parser.add_option('--silent',action="store_true",dest="silent")

    # Parse the options, and remove them from sys.argv.
    options, args = parser.parse_args()
    sys.argv = [sys.argv[0]] ; sys.argv.extend(args)

    # -- path
    # We can't finalize the path here, because g does not exist ye.
    path = options.path or 'dynamicUnitTest.leo'

    # -- gui
    gui = options.gui
    if gui: gui = gui.lower()
    if gui not in ('qttabs','qt'):
        gui = 'nullGui'

    # --silent
    silent = options.silent

    return path,gui,silent
#@+node:ekr.20111116161118.10248: *3* Recent tests
#@+node:ekr.20111104132424.9909: *4* @test assert True
# It's useful to have this do-nothing test.

assert True
#@+node:ekr.20111110085739.10265: *4* @test html string
s = '''\
<HTML>
<head>
    <title>Bodystring</title>
</head>
<body class='bodystring'>
<div id='bodydisplay'></div>
</body>
</html>
'''

html_tags = ('body','head','html','table',) # 'div',
setting = 'import_html_tags'

# Settings now work when run externally.
c.config.set(setting,'data',html_tags)
tags = c.config.getData(setting)
assert tags == html_tags,len(tags)

g.app.unitTestDict ['expectedErrors'] = 0

showTree = True

c.importCommands.htmlUnitTest(p,s=s,showTree=showTree)

if showTree:
    # g.cls()
    for p in p.subtree():
        print('\n***** %s\n' %p.h)
        print(p.b)
#@+node:ekr.20120112100822.10003: *5* @file c:/leo.repo/trunk/leo/core/html string
@language xml
@tabwidth -4
@others

#@+node:ekr.20120112100822.10004: *6* html
<HTML>
@others
</html>
#@+node:ekr.20120112100822.10005: *7* head

<head>
    <title>Bodystring</title>
</head>
#@+node:ekr.20120112100822.10006: *7* body
<body class='bodystring'>
<div id='bodydisplay'></div>
</body>
#@+node:ekr.20120203153754.10033: *5* @file c:/leo.repo/trunk/leo/core/html string
@language xml
@tabwidth -4
@others

#@+node:ekr.20120203153754.10034: *6* html
<HTML>
@others
</html>
#@+node:ekr.20120203153754.10035: *7* head

<head>
    <title>Bodystring</title>
</head>
#@+node:ekr.20120203153754.10036: *7* body
<body class='bodystring'>
<div id='bodydisplay'></div>
</body>
#@+node:ekr.20120204061120.10061: *5* @file c:/leo.repo/trunk/leo/core/html string
@language xml
@tabwidth -4
@others

#@+node:ekr.20120204061120.10062: *6* html
<HTML>
@others
</html>
#@+node:ekr.20120204061120.10063: *7* head

<head>
    <title>Bodystring</title>
</head>
#@+node:ekr.20120204061120.10064: *7* body
<body class='bodystring'>
<div id='bodydisplay'></div>
</body>
#@+node:ekr.20111109151106.9746: *4* @test htmlScanner.filterTokens
import leo.core.leoImport as leoImport
ic = c.importCommands
hs = leoImport.htmlScanner(importCommands=ic,atAuto=True)
strip = hs.stripTokens
dump  = hs.formatTokens

s1 = '''<table id="1"><table id="2">
<contents/>
</table>
</table>'''

s2 = '<table id="1"><table id="2"><contents/></table></table>'
    
t1 = hs.tokenize(s1)
t2 = hs.tokenize(s2)
f1 = hs.filterTokens(t1)
f2 = hs.filterTokens(t2)

assert strip(f1) == strip(f2),'f1...\n%s\nf2...\n%s' % (
    dump(f1),dump(f2))
    
if 0:
    print(dump(f1))
#@+node:ekr.20111110084957.10092: *4* @test import dataN.html
fn = r'c:\recent\data.html'

# fn = r'c:\recent\data-smaller.html'
# fn = r'c:\recent\data666.html'

# These all pass on data.html:
    # html_tags = ('html','head','body',)
    # html_tags = ('html','head','body','table',)
    # html_tags = ('html','head','body','table','div',)
    # html_tags = ('html','head','body','table','div','script',)
    # html_tags = ('html','head','body','table','div','script','link',)
    # html_tags = ('html','head','body','table','div','script','link','p',)

html_tags = ('html','head','body','table','div','script','p','td','tr',)

# Settings now work when run externally.
setting = 'import_html_tags'
c.config.set(setting,'data',html_tags)
tags = c.config.getData(setting)
assert tags == html_tags,len(tags)

g.cls()

c.importCommands.importFilesCommand(files=[fn], treeType='@file')
#@+node:ekr.20111109105907.9795: *4* @test unicode stuff
@first # -*- coding: utf-8 -*-

table = (
    'test',
    'Ä 궯 奠',
    'Ä 궯 奠 after', # fails with cp6501: after is duplicated.
)

print('*'*20)
print('isPython3: %s' % g.isPython3)

for s in table:
    if g.isPython3:
        s = s.encode('ascii','replace') # create bytes.
    g.es(repr(s))
    g.es(s)
    g.pr ('g.pr(s)       : %s' % s)
    g.pr ('g.pr(repr(s)) : %s' % repr(s))
    print('print(s)      : %s' % s)
    print('print(repr(s)): %s' % s)
#@+node:ekr.20111113064104.9841: *4* @test external text operations
assert g.app.isExternalUnitTest

body = c.frame.body
assert repr(body.widget).startswith('stringTextWidget')
assert body.widget == body.bodyCtrl

w = body.bodyCtrl
w.setAllText(p.b)
assert p.b == w.getAllText()
#@+node:ekr.20111112131605.9789: *4* @test nullBody text operations
# print('isExternalUnitTest',g.app.isExternalUnitTest)

if g.app.isExternalUnitTest:
    body = c.frame.body
else:
    import leo.core.leoCommands as leoCommands
    import leo.core.leoFrame as leoFrame
    import leo.core.leoGui as leoGui
    
    # Important: external unit tests should execute in this environment.
    nullGui   = leoGui.nullGui('null gui')
    nullFrame = leoFrame.nullFrame(title='nullFrame title',gui=nullGui)
    c2 = leoCommands.Commands(nullFrame,fileName='<empty fileName>')
    nullFrame.c = c2
    body = leoFrame.nullBody(frame=nullFrame,parentFrame=None)
    assert repr(body).startswith('<leo.core.leoFrame.nullBody')

# Now test some basic operations.
assert repr(body.widget).startswith('stringTextWidget')
assert body.widget == body.bodyCtrl
w = body.bodyCtrl

w.setAllText(p.b)
assert p.b == w.getAllText()
#@+node:ekr.20111115155710.9835: *4* @test g.python_tokenize
# h = 'g.python_tokenize'
# p = p.firstChild()
# assert p.h == h
tokens = g.python_tokenize(p.b,line_numbers=False)

# tokens = [(kind,val) for (kind,val,line_number) in tokens]

# First, the basic check
tokens1 = [val for kind,val in tokens]
s = ''.join(tokens1)
assert p.b == s,repr(s)

if 0:
    for z in tokens:
        kind,val = z
        print('%6s %s' % (kind,repr(val)))
        
# Next, start filtering.
tokens = [(kind,g.choose(kind=='string','"S"',val)) for kind,val in tokens]

if 0: # Delete whitespace.
    tokens = [(kind,val) for (kind,val) in tokens if kind != 'ws']
    tokens = [(kind,g.choose(kind=='id',val+' ',val)) for (kind,val) in tokens]

# Last: stringize.
tokens = [val for kind,val in tokens if kind != 'comment']
# print(''.join(tokens))

if 1: # Print lines containing '='
    s = ''.join(tokens)
    for ch in '()[]{}<>.,:=+-/':
        s = s.replace(' '+ch,ch)
    aList = [z for z in g.splitLines(s)
        if z.find('=') > -1] # and not z.find('+=')>-1 and not z.find('-=')>-1]
    print(''.join(aList))
    
#@+node:ekr.20111204110514.10287: *4* @test p.moveToFirst/LastChild
def setup(p):
    while p.hasChildren():
        p.firstChild().doDelete()

child = p.firstChild()
assert child
setup(child)
p2 = child.insertAfter()
p2.h = "test"
try:
    assert c.positionExists(p2),p2
    p2.moveToFirstChildOf(child)
    assert c.positionExists(p2),p2
    p2.moveToLastChildOf(child)
    assert c.positionExists(p2),p2
finally:
    if 1:
        setup(child)
    c.redraw(p)
#@+node:ekr.20111210100047.10306: *5* child
#@+node:ekr.20111116161118.10247: *3* Old lint tests
#@+node:ekr.20111116103733.9845: *4*  Naming tests
# http://docs.python.org/reference/executionmodel.html#naming-and-binding

if 0:
    def test():
        a = b
        b = 1 # UnboundLocalError
        
print('***')
          
def test():
    global g2
    g2 = 4
    def test2():
        print(g2)
    test2()
    g2 = 3
    print(g2)
    
g2 = 'g2'
print(g2)

test()
#@+node:ekr.20111116103733.9846: *5* << define s>>
s = '''



'''

s = g.adjustTripleString(s,-4)
#@+node:ekr.20111116103733.9839: *4* @test create lots of data structures
# A simple prototype of data-centric design.
# 0.023 sec to create 100,000 dicts.
# 0.230 sec to create 1,000,000 dicts.

import time

t1 = time.clock()
n = 1000000

d = {}
for z in range(n):
    d[n] = {'n':n,}

t2 = time.clock()
print('Created %s dicts in %2.3f sec.' % (n,t2-t1))
#@+node:ekr.20111116103733.9844: *4* @test dumper (to outNodes)
import leo.core.leoGlobals as g
import sys
import lintutils as u

fn = 'c:/leo.repo/trunk/leo/core/leoApp.py'
out = 'c:/leo.repo/new-pylint/outNodes.txt'

outStream = open(out,'w')
u.AstDumper().dumpFileAsNodes(fn,outStream)
#@+node:ekr.20111116103733.9840: *4* @test dumper (to outString)
import leo.core.leoGlobals as g
import sys
import lintutils as u

fn = 'c:/leo.repo/trunk/leo/core/leoApp.py'
out = 'c:/leo.repo/new-pylint/outString.txt'

outStream = open(out,'w')
u.AstDumper(brief=True).dumpFileAsString(fn,outStream)
#@+node:ekr.20111128103520.10237: *3* Tests of @shadow from unitTest.leo
#@+node:ekr.20111128103520.10238: *4* @@shadow ../test/unittest/at-shadow-test.py
@language python
@tabwidth -4
@others
#@+node:ekr.20111128103520.10239: *5* spam
def spam():
    pass
#@+node:ekr.20111128103520.10240: *5* eggs
def eggs():
    pass
#@+node:ekr.20111128103520.10241: *4* @@shadow unittest/at-shadow-line-number-test.py
@language python
@tabwidth -4
@others
#@+node:ekr.20111128103520.10242: *5* child
def child():
    pass
#@+node:ekr.20111128103520.10243: *4* @test @shadow: shape of tree
# Not valid for external tests: uses @<file> node.
if not g.app.isExternalUnitTest:

    h = '@shadow ../test/unittest/at-shadow-test.py'
    p = g.findNodeAnywhere(c,h)
    assert p
    
    table = (
        (p.firstChild(),'spam'),
        (p.firstChild().next(),'eggs')
    )
    
    assert not p.isDirty(),p.h # Do not ignore this failure!
    
    for p2,h2 in table:
        assert len(p2.h) == len(h2)
#@+node:ekr.20111128103520.10244: *4* @test goto-global-line @shadow
# Not valid for external tests: uses @<file> node.
if not g.app.isExternalUnitTest:

    h = '@shadow unittest/at-shadow-line-number-test.py'
    root1 = g.findNodeAnywhere(c,h)
    assert root1
    assert root1.isAnyAtFileNode()
    
    fileName,lines,n,root2 = c.goToLineNumber(c).setup_file(n=6,p=root1)
    assert fileName == h[8:],'fileName'
    assert root2 == root1
    
    if 0:
        print('root:%s, isRaw:%s, n:%s, len(lines): %s' % (
            root and root.h,isRaw,n,len(lines)))
#@+node:ekr.20120116073928.10114: *3* Unit tests for settings
#@+node:ekr.20120126050844.10386: *4* @ignore print dicts unit tests
#@+node:ekr.20120117095916.10124: *5* @test printMenusList
def printMenusList(aList,level=0):
    
    for z in aList:
        a,b,c = z
        print('*** kind',a)
        if type(b) in (type(()),type([])):
            for z2 in b:
                a1,b1,c1 = z2
                if a1.startswith('@menu') and type(b1) in (type(()),type([])):
                    print()
                    print('*** inner menu: %s' % (level+1))
                    print(a1)
                    for z3 in b1:
                        print(z3)
                    if c1: print(c1)
                else:
                    print(z2)
            if c: print(c)
        else:
            print(b)
        print()
        break #
        
printMenusList(c.config.getMenusList())
       
#@+node:ekr.20120117095916.10140: *5* @test printInverseBindingDict
print('\ninverseBindingDict...\n')

d = c.k.computeInverseBindingDict()

for key in sorted(list(d.keys())):

    if 1 == len(d.get(key)):
        print(key,d.get(key))
    else:
        print()
        print(key)
        print(d.get(key))
        print()
#@+node:ekr.20120123113111.10925: *5* @test printBindingsDict
import leo.core.leoConfig as leoConfig # for ShortcutInfo
    
partial = True

d = c.k.bindingsDict
    # Keys are shortcuts; values are *lists* leoConfig.ShortcutInfo objects.
    
print('\nk.bindingsDict%s...\n' % ' (partial)' if partial else '')
    
for key in list(sorted(d.keys())):
    aList = d.get(key,[])
    for b in aList:
        assert isinstance(b,leoConfig.ShortcutInfo)
        if not partial or b.kind != 'leosettings.leo':
            print(b)
#@+node:ekr.20120117095916.10141: *5* @test printMasterBindingsDict
partial = True

panes = ('all','body','button','log','tree','text',
    'command','insert','overwrite',)

d = c.k.masterBindingsDict
    # Keys are scope names (in panes) or mode names.
    # Values are dicts:
        # keys are strokes; values are leoConfig.ShortcutInfo objects.
        
print('\nk.masterBindingsDict%s...\n' % ' (partial)' if partial else '')

for pane in sorted(list(d.keys())):
    kind = 'pane' if pane in panes else 'mode'
    print('%s: %s...' % (kind,pane))
    d2 = d.get(pane)
    for stroke in sorted(list(d2.keys())):
        b = d2.get(stroke)
        if not partial or b.kind != 'leosettings.leo':
            print('%6s %25s %17s %s' % (b.pane,stroke,b.kind,b.commandName))
            assert b.pane == pane
            assert b.stroke == stroke
    print()
#@+node:ekr.20120126080450.10187: *4* @ignore passed
#@+node:ekr.20120126080450.10189: *5* @test mode-related info
@

g.app.config.modeCommandsDict
    Keys are command names: enter-x-mode.
    Values are inner dictionaries:
        Keys are command names, values are lists of ShortcutInfo nodes.
@c

d = g.app.config.modeCommandsDict
    
for key in sorted(d.keys()):
    print('*** mode ***',key)
    d2 = d.get(key)
    for key2 in sorted(d2.keys()):
        aList = d2.get(key2)
        print(key2)
        for si in aList:
            print('   ',si)
#@+node:ekr.20120120095156.10262: *5* @test types of contents of settings dicts
@
ivar                    Keys                Values
----                    ----                ------
c.commandsDict          command names (1)   functions
k.inverseCommandsDict   func.__name__       command names
k.bindingsDict          shortcuts           list of ShortcutInfo objects
k.masterBindingsDict    scope names (2)     inner masterBindingDicts (3)
k.masterGuiBindingsDict strokes             list of widgets in which stoke is bound
k.settingsNameDict (4)  settings.lower()    "Real" Tk specifiers
inverseBindingDict (5)  command names       lists of tuples (pane,key)
modeCommandsDict (6)    command name (7)    inner modeCommandsDicts (8)

Notes:
(1) Command names are minibuffer names (strings)
(2) Scope names are 'all','text',etc.
(3) inner masterBindingDicts: Keys are strokes; values are ShortcutInfo objects.
(4) k.settingsNameDict has no inverse.
(5) inverseBindingDict is **not** an ivar: it is computed by k.computeInverseBindingDict.
(6) A global dict: g.app.gui.modeCommandsDict
(7) enter-x-command
(8) Keys are command names, values are lists of ShortcutInfo objects.
@c

si_type = c.k.ShortcutInfo
disabled_func_type = None # Should be any bound method.
k = c.k

@others

test_dict_of_objects(c.commandsDict,type('s'),disabled_func_type,'commandsDict')
test_dict_of_objects(k.inverseCommandsDict,type('s'),type('s'),'inverseCommandsDict')
test_dict_of_lists(k.bindingsDict,si_type,'bindingsDict')
test_dict_of_dicts(k.masterBindingsDict,si_type,'masterBindingsDict')
test_dict_of_lists(k.masterGuiBindingsDict,None,'masterGuiBindingsDict')
test_dict_of_objects(k.settingsNameDict,type('s'),type('s'),'settingsNameDict')
test_dict_of_lists(k.computeInverseBindingDict(),type(tuple()),'inverseBindingDict')

# Test individual dicts separately.
d = g.app.config.modeCommandsDict
test_dict_of_dicts(d,None,'modeCommandsDict')
for key in sorted(d.keys()):
    d2 = d.get(key)
    test_dict_of_lists(d2,si_type,'inner modeCommandsDict')
        # This requires a hack to special-case the
        # '*entry-commands*' and '*command-prompt*' keys.
#@+node:ekr.20120126080450.10193: *6* test_dict_of_dicts
def test_dict_of_dicts(d,theType,tag):

    assert d,tag

    for key in d.keys():
        d2 = d.get(key)
        assert type(d2) == type({})
        for key in d2.keys():
            obj = d2.get(key)
            if theType:
                assert type(obj) == theType,repr(obj)
#@+node:ekr.20120126080450.10191: *6* test_dict_of_lists
def test_dict_of_lists(d,theType,tag):

    assert d,tag

    for key in d.keys():
        obj = d.get(key)
        if key in ('*entry-commands*','*command-prompt*'):
            # Special case for g.app.config.modeCommandsDict
            assert type(obj)==type([]),repr(obj)
        else:
            assert type(obj) == type([])
            # Don't check types of list elements if theType is None.
            if theType:
                for z in obj:
                    assert type(z)==theType,'key: %s obj: %s' % (key,repr(obj))
#@+node:ekr.20120126080450.10195: *6* test_dict_of_objects
def test_dict_of_objects(d,keyType,valueType,tag):

    assert d,tag

    for key in d.keys():
        assert type(key) == keyType,repr(key)
        obj = d.get(key)
        # Don't check type of obj if valueType is None.
        if valueType:
            assert type(obj) == valueType,'\nobj: %s\nvalueType: %s' % (repr(obj),valueType)
#@+node:ekr.20120126080450.10194: *6* Unused
# import types
# types.ListType does not exist in Python 3.x.
# assert isinstance(aList,list().__class__)
#@+node:ekr.20120127084215.10238: *5* @test merge_settings_dicts
@others

# import os ; os.system('cls')
    
d1 = g.app.config.immutable_leo_settings_shortcuts_dict
d2 = g.app.config.immutable_my_leo_settings_shortcuts_dict
d3 = g.app.config.merge_settings_dicts(d1,d2)

if False:
    patterns = (
        'backward-find-character-extend-selection',
    )
    for pattern in patterns:
        print(dump_dict(d1,pattern,tag='d1'))
        print(dump_dict(d2,pattern,tag='d2'))
        print(dump_dict(d3,pattern,tag='d3'))

test(d1,d2,d3)
#@+node:ekr.20120127145909.10227: *6* dump & dump_dict (@test merge_settings_dicts)
def dump(aList,pattern=None,tag=None):
    
    return '\n'.join([repr(z) for z in aList])
    

def dump_dict(d,pattern=None,tag=None):
    
    result = [] # '\ndump of %s...' % (tag)
    
    for key in d.keys():
        if pattern in (key,None):
            result.append(key)
            aList = d.get(key)
            for z in aList:
                result.append('    %s' % (z))
                
    return '\n'.join(result)
#@+node:ekr.20120127084215.10239: *6* test (@test merge_settings_dicts)
def test(old_d,new_d,result_d):
    
    '''Test that result_d is the result of upating old_d with new_d.
    
    This test is tricky: only inverted dicts have ShortcutInfo nodes as keys.'''
    
    invert,uninvert = g.app.config.invert,g.app.config.uninvert

    # Compute the inversions of all the dicts.
    inv_old,inv_new,inv_res = invert(old_d),invert(new_d),invert(result_d)
    
    # Part 1: Ensure we test all keys.
    keys = list(inv_old.keys())
    keys.extend(list(inv_new.keys()))
    keys.extend(list(inv_res.keys()))
    keys = sorted(list(set(keys)))
    assert None not in keys
    for key in inv_old.keys(): assert key in keys,key
    for key in inv_new.keys(): assert key in keys,key
    for key in inv_res.keys(): assert key in keys,key
    
    # Part 2: Carefully test the inverted result.
    def si_name_key(si): return si.commandName or ''

    for key in keys:
        # Compute the *sorted* list of 
        res_list = sorted(inv_res.get(key,[]),key=si_name_key)
        old_list = sorted(inv_old.get(key,[]),key=si_name_key)
        new_list = sorted(inv_new.get(key,[]),key=si_name_key)
        assert res_list,'no res_list.get(%s)' % (key)
        # if new_list: print(key,dump(new_list))
        if new_list:
            assert new_list == res_list,'key %s\nnew:\n%s\nres:\n%s' % (
                key,dump(new_list),dump(res_list))
        else:
            assert old_list == res_list,'key %s\nold:\n%s\nres:\n%s' % (
                key,dump(old_list),dump(res_list))
    
    # Part 3: Test that result_d == uninvert(invert(result_d)).
    # A.  They must have the same keys.
    unv_res = uninvert(inv_res)
    assert sorted(list(result_d.keys())) == sorted(list(unv_res.keys()))

    # B. The values of for each key must match after being sorted.
    def si_stroke_key(si): return si.stroke or ''
        
    for key in sorted(result_d.keys()):
        res_list = sorted(result_d.get(key,[]),key=si_stroke_key)
        unv_list = sorted( unv_res.get(key,[]),key=si_stroke_key)
        assert res_list == unv_list,'key %s\nres:\n%s\nunv:\n%s' % (
            key,dump(res_list),dump(unv_list))
   
#@+node:ekr.20120203153754.10032: *5* @test KeyStroke
ks = c.k.KeyStroke

@others

a1 = ks('a')
a2 = ks('a')
b1 = ks('b')
assert a1 == a2
d = {}
d[a1] = a1.s
d[a2] = a2.s
d[b1] = b1.s

for key in sorted(d):
    print(key,d.get(key))
#@+node:ekr.20120205022040.17748: *5* @test g.TypedDict
d = g.TypedDictOfLists('ks',type('s'),type(9))
d.add('a',1)
d.add('a',2)
d.add('b',3)

print(d)
for s in sorted(d.keys()):
    print(s,d.get(s,[]))

print('after replace...')
d.replace('a',[8,9,10])

for s in sorted(d.keys()):
    print(s,d.get(s,[]))
#@+node:ekr.20120215062153.14233: *3* @mark-for-unit-tests
#@+node:ekr.20120529105626.10139: *4* @settings
#@+node:ekr.20120529105626.10140: *5* @bool fixedWindow = False
#@+node:ekr.20120529105626.10141: *5* @bool enable-abbreviations = True
#@+node:ekr.20120529105626.10142: *5* @@@enabled-plugins
# Leo loads plugins in the order they appear here.

# Highly-recommended plugins:

plugins_menu.py
free_layout.py # needs to be early
viewrendered.py
mod_scripting.py
bigdash.py
#@+node:ekr.20120529105626.10143: *5* @shortcuts
run-selected-unit-tests-externally = Alt-4 # Standard binding, unchanged.
run-marked-unit-tests-externally = Alt-5
run-marked-unit-tests-locally = Alt-6
#@+node:ekr.20120329072206.9700: ** 4.10
#@+node:ekr.20120322073519.10401: *3* b1
#@+node:ekr.20120318110848.9734: *4* Added import-org-mode script
#@+node:ekr.20120318110848.9735: *5* import-org-mode (command, not used)
class ImportOrgMode:
    @others

def importOrgMode (self,event):
    c = self.c
    self.ImportOrgMode(c).go(c.p)
    c.bodyWantsFocus()

if False and g.app.inScript:
    print('='*40)
    ImportOrgMode(c).test()
    print('done')
#@+node:ekr.20120318110848.9736: *6* ctor
def __init__ (self,c):
    
    self.c = c
#@+node:ekr.20120318110848.9737: *6* go
def go (self,p):
    
    '''Prompt for a file and pass the contents to scan().'''
#@+node:ekr.20120318110848.9738: *6* scan
def scan (self,fn,p,s):

    self.c = c
    root = p.insertAsLastChild()
    root.h = fn
    level,stack = 0,[root]
    body = ['@others\n']
    
    for s in g.splitLines(s):
        g.trace(repr(s))
        if s.startswith('*'):
            i,level = 0,0
            while s[i] == '*':
                i += 1
                level += 1
            if level > len(stack):
                g.trace('bad level',repr(s))
                last = None
            elif level == len(stack):
                last = stack[-1]
                last.b = ''.join(body)
            else:
                last = stack[-1]
                last.b = ''.join(body)
                stack = stack[:level]
            parent = stack[-1]
            p = parent.insertAsLastChild()
            p.h = s.strip()
            stack.append(p)
            body = []
        else:
            body.append(s)
            
    # Finish any trailing lines.
    if body:
        parent = stack[-1]
        parent.b = ''.join(body)
        
    root.contract()
    c.redraw(root)
#@+node:ekr.20120318110848.9739: *6* test
def test (self):
    
    s = '''
* A1
    a1.1
    a1.2
** B11
** B12
b12.1
*** C121
c121.1
    c121.2
c121.3
* A2
a2.1
** B21
*** C211
c211.1
*** C212
** B22
    b22.1
b22.1
* A3
* A4
a4.1
* A5
** B51
*** C511
**** D5111
***** E51111
** B52
*** C521
c521.1
'''

    tag = 'test-import-org-mode'
    p = g.findNodeAnywhere(c,tag)
    s = g.adjustTripleString(s,-4)
    if p:
        try:
            self.scan('test-file',p,s)
        except Exception:
            c.redraw(p)
    else:
        print('not found: %s' % tag)
#@+node:ekr.20120318110848.9740: *5* @@button import-org-mode
'''Import each file in the files list after the presently selected node.'''


files = (
    r'c:\Users\edreamleo\test\import-org-mode.txt',
    r'c:\Users\edreamleo\test\import-org-mode.txt',
)

@others

for fn in files:
    try:
        root = c.p.copy()
        f = open(fn)
        s = f.read()
        scan(c,fn,s)
        c.selectPosition(root)
    except IOError:
        print('can not open %s' % fn)
#@+node:ekr.20120318110848.9741: *6* scan
def scan (c,fn,s):

    last = root = c.p.insertAsLastChild()
    last.h = g.shortFileName(fn)
    level,stack = 0,[root]
    body = ['@others\n']
    
    for s in g.splitLines(s):
        if s.startswith('*'):
            i,level = 0,0
            while s[i] == '*':
                i += 1 ; level += 1
            if level > len(stack):
                g.trace('bad level',repr(s))
            elif level == len(stack):
                last.b = ''.join(body)
            else:
                last.b = ''.join(body)
                stack = stack[:level]
            parent = stack[-1]
            last = parent.insertAsLastChild()
            last.h = s.strip()
            stack.append(last)
            body = []
        else:
            body.append(s)
            
    # Finish any trailing lines.
    if body:
        last.b = ''.join(body)
        
    root.contract()
    c.redraw(root)
#@+node:ekr.20120318110848.9742: *5* test-import-org-mode
#@+node:ekr.20120318110848.9747: *4* Code for displaying a function call hierarchy in Leo
From Brian Theado

The other day I stumbled across Ville's code in scripts.leo which displays the
output of python's trace module in a leo outline. The output of the trace module
is not very friendly and I didn't find the result very usable. I was inspired to
write some code to translate the output so the tree of function calls is
displayed via Leo headlines. Thanks to Ville for sharing that code. I never
would have figure this out without that starting point.

Just copy (Ctrl-Shift-V) the child outline into a leo outline and hit ctrl-b on
the "call tree" node. The execution tree of the 'scroll-outline-up-line'
minibuffer command will be displayed to stdout and also as a tree of leo
headlines.
#@+node:ekr.20120318110848.9748: *5* call tree
import trace

@language python
@others

# http://docs.python.org/library/trace.html for documentation
# on the trace module
tracer = trace.Trace(countcallers=1)

# Trace a minibuffer command.

# Any function call will work. Leo's minibuffer commands are easily discoverable
# via tab completion and the 'print-commands' command.

#tracer.runfunc(c.executeMinibufferCommand, 'goto-prev-node')
tracer.runfunc(c.executeMinibufferCommand, 'scroll-outline-up-line')

top = p.insertAsLastChild().copy()
top.h = 'trace session'
displayCalltree(top, tracer.results().callers.keys())
c.redraw()
#@+node:ekr.20120318110848.9749: *6* displayCalltree
def displayCalltree(p, callinfo):
   '''
   Converts the function call hierarchy in 'callinfo' into a tree of function
   calls.  The function call tree is displayed to stdout as indented text
   and is inserted as a tree of leo nodes rooted at the given position 'p'
   '''
   callers = [k[0] for k in callinfo]
   callees = [k[1] for k in callinfo]

   # The first set of children will be those that don't have any callers
   # listed in callinfo
   toplevels = list(set(callers) - set(callees))
   positions = {}
   path = []

   # Depth-first traversal of the call hierarchy represented by 'callinfo'
   # 'levels' is a stack which grows during descend and shrinks
   # during ascend.  Each element of 'levels' is a list of unprocessed
   # siblings of each other
   levels = [toplevels]
   while len(levels) > 0:
       while len(levels[-1]) > 0:
           # Process the first element in the 'deepest' (i.e. last) list of siblings
           cur = levels[-1][0]
           levels[-1] = levels[-1][1:]
           indent = " " * 4 * (len(levels)-1)
           if cur not in path:
               if cur in positions.keys():
                   # Function already seen, so make a clone
                   clone = positions[cur].clone()
                   clone.moveToLastChildOf(p)
                   print (indent + "%s %s ..." % cur[1:])
               else:
                   # Haven't seen this function, so insert a new headline
                   p = p.insertAsLastChild().copy()
                   p.h = "%s %s" % cur[1:]
                   print (indent + p.h)

                   # Remember the position so it can be cloned if seen again
                   positions[cur] = p

                   # Find all callees of this function and descend
                   levels.append([c[1] for c in callinfo if c[0] == cur])
                   path.append(cur)
           else:
               r = p.insertAsLastChild().copy()
               r.h = "(recursive call) %s %s" % (cur[1], cur[2])
               print(indent + r.h + "...")

       # Ascend back up one level
       path = path[0:-1]
       p = p.parent()
       levels = levels[0:-1]
#@+node:ekr.20120318110848.9750: *6* trace session
#@+node:ekr.20120314064059.9737: *4* Use ctrl-click to open url's
- (Done) Added the following commands:
    
    - ctrl-click-icon
    - ctrl-click-at-cursor
    - open-url
    - open-url-under-cursor
    
- (Done) Double-click *only* edits headline.
- (Done) Only look at first line of the body in @url nodes.
- (Done) Ctrl-click in body allows spaces in url's.

#@+node:ekr.20120322073519.10402: *3* final
#@+node:ekr.20120327163022.9737: *4* Bugs
#@+node:ekr.20120322073519.9785: *5* Fixed crasher in flattenOutline
Traceback (most recent call last):
  File "c:\leo.repo\trunk\leo\core\leoCommands.py", line 553, in doCommand
    val = command(event)
  File "c:\leo.repo\trunk\leo\core\leoCommands.py", line 2120, in flattenOutline
    c.importCommands.flattenOutline(fileName)
  File "c:\leo.repo\trunk\leo\core\leoImport.py", line 479, in flattenOutline
    theFile.write(s)
TypeError: must be str, not bytes
#@+node:ekr.20120323110755.9687: *5* Fix viewrendered crash
Traceback (most recent call last):
  File "c:\leo.repo\trunk\leo\core\leoPlugins.py", line 337, in callTagHandler
    result = handler(tag,keywords)
  File "c:\leo.repo\trunk\leo\plugins\viewrendered.py", line 560, in update
    f(s,keywords)
  File "c:\leo.repo\trunk\leo\plugins\viewrendered.py", line 655, in update_graphics_script
    pc.gs = QtGui.QGraphicsScene(pc.splitter)
AttributeError: 'ViewRenderedController' object has no attribute 'splitter'
#@+node:ekr.20120323124339.9722: *5* Fixed(mostly)scrolling problem with multiple editors
@language python
@language rest

Selecting body editor with clicks doesn't save/restore visual ivars.
The solution would be to create a new onClick event handler...

- Removed insert=None,new_p=None args from all versions of setAllText.
  These are entirely misguided, and may have contributed to scrolling problems.
  
  setAllText now *only* sets text, nothing else!

- All calls to leoMoveCursorHelper are follwed by code that updates
  v.insertSpot, v.selectionStart and v.selectionLength.
  
- v.restoreCursorAndScroll now *carefully* restores selection
  based on v.insertSpot, v.selectionStart and v.selectionLength.
  It also restores the scrollbar using v.scrollBarSpot.
  
- < < unselect the old node > > (selectHelper) now *only*
  sets v.scrollBarSpot.
  
#@+node:ekr.20120327062318.9731: *5* Ensure selected @test node is run
In earlier version of Leo if one runs test externally with the selected
position under @test node, that @test was executed with (run-marked-unit-tests-externally)

The fix was to the "important special case" in TM.findAllUnitTestNodes.
#@+node:ekr.20120327062318.9732: *5* Made sure the new load code loads plugins at most once
@language python
@language rest

new load code, double init. for free layout
http://groups.google.com/group/leo-editor/browse_thread/thread/dd16ac6dc1832eb2

bookmarks.py was the culprit. The code in onCreate must test to see if c.free_layout already exists.
#@+node:ekr.20120326061010.9726: *5* fixed problem with file:/// url's on Windows
@language rest

http://groups.google.com/group/leo-editor/browse_thread/thread/bb063866875a81c3#

In my installation, now on the latest revision ( r5195) I'm still
experiencing an issue with the '@url command' using 'File-URL' in a Windows
environement.

I'm able to create the Leo User documentation locally. - However, when I
try to read the documentation using the 'File-URL'

file:///D:/Branches/leo-editor/leo/doc/html/_build/html/leo_toc.html

I get the following message in the Leo-Log.

<log>

File 'D:\D:\Branches\leo-editor\leo\doc\html\_build\html\leo_toc.html' does not exist

</log>

However if I enter this URL directly into FF it is found and displayed properly.

EKR: Obviously, the 'D:\D:\' is the problem.

The fix is simply to special-case file:/// on Windows in g.computeFileUrl.
#@+node:ekr.20120327163022.9736: *5* fixed get_fn in viewrendered plugin
@language rest
groups.google.com/group/leo-editor/browse_thread/thread/bb063866875a81c3/6162e6108b09428e

The new code is much like g.computeFileUrl.
#@+node:ekr.20120327163022.9741: *5* Restored special case for run-selected-unit-tests
@language rest

Added code to findAllUnitTestNodes to look up the tree for @test & @suite nodes
if none have been found so far.  Only for the run-unit-tests-externally/locally.
#@+node:ekr.20120321174708.9744: *5* Fixed failing unit tests in distro
@nocolor-node

The @test at.readOneAtShadowNode retains @shadow links node
give fail1: test not set up properly.
The outline is then corrupted, causing other unit tests to fail.
The partial solution is not to call the undo command in the finally clause.
#@+node:ekr.20120327163022.9739: *4* Features
#@+node:ekr.20120324124808.9833: *5* Alt-Home & Alt-End collapse all possible nodes
#@+node:ekr.20120326061010.9728: *5* Added g.restore_selection_range
@nocolor-node

If off, only the insert point is restored.

It's kinda pointless to make this a user option.
#@+node:ekr.20120327163022.9738: *4* Home page
#@+node:ekr.20111027103125.16545: *5* Added link to home page from the TOC
#@+node:ekr.20120229173025.20629: *5* Removed online-tutorial link
http://groups.google.com/group/leo-editor/browse_thread/thread/2157d8bfc0f381f1

es, choosing Help-->"Open Online Tutorial" tries to go to a page on
3dTree.com the site for which is no longer held.

Should the Quick Intro be brought back instead? 
#@+node:ekr.20111027103125.16539: *5* Added search box to Leo's home page
<div id="searchbox" style="">
<h3>Quick search</h3>

<form class="search" method="get" action="search.html">

<p class="searchtip" style="font-size: 90%"> Enter search terms or a module, class or function name. </p>
</div>
#@+node:ekr.20111020120612.15896: *5* Added link to glossary from Leo's home page
#@+node:ekr.20111027103125.16544: *5* Added screen shot to Leo's home page
#@+node:ekr.20110930174206.15470: *5* Brought screen shots up to date
#@+node:ekr.20120326061010.9727: *5* Scaled the screenshot on home page
http://groups.google.com/group/leo-editor/browse_thread/thread/ea3c29888d8ac92b

> - Added a full-sized screenshot at the bottom of the page.
>  I'm not sure whether this is a good idea.  What do you think?

Scaled the screen-shot using:

http://stackoverflow.com/questions/3029422/image-auto-resize-to-fit-div-container
#@+node:ekr.20120323124339.9721: *4* Investigated problem with desktop shortcut
@language rest

http://groups.google.com/group/leo-editor/browse_thread/thread/a03661d8c1eec0c6

I'm experimenting with the latest version (WinXP). This is really odd. If I
launch Leo using the desktop shortcut, I'm not able to open *any* valid
.leo file except  LeoSettings.leo. This is the shortcut:
C:\Python32\pythonw.exe "C:\Program Files\Leo-4.10-b1\launchLeo.py"

However, if I launch using my Windows batch file, I'm able to open all
files as expected. The code is:
C:\Python32\python "C:\Program Files\Leo-4.10-b1\launchLeo.py" %*

Anyone else experiencing this? Any possible explanation?

EKR: Are you doing this in a console window? 

No, that's why I used the batch file so I can launch w/ a console. When I
do, it works fine. 

I just installed on another box w/ Python 2.6.2 That works fine. The other
box has Python 3.2, not sure why that matters, but it might be a clue.
#@+node:ekr.20120327163022.9742: *5* EKR response
@language rest

There are two differences between the two ways of launching.

1. pythonw.exe vs python.exe

2. The former has no "%*" argument.  It's possible that you have no
workbook.leo file in your home directory, which might cause a failure,
iirc.

I suggest first changing pythonw to python.  This will open a console,
but probably too briefly to read.  To fix this, add a -i argument,
which will drop python into interactive mode, which has the side
effect of leaving the console open.  This should tell you why exactly
nothing happens.

I suspect that adding the "%*" argument will fix the problem,
regardless of whether you use pythonw or python.

If not, please feel free to ask more questions.
#@+node:ekr.20120329072206.9701: ** 4.10.1
#@+node:ekr.20120330040023.9780: *3* Bugs
#@+node:ekr.20120330040023.9779: *4* Restore focus on window activation
# Changed: onActivateEvent (qtGui), onDeactivateEvent (qtGui)
#@+node:ekr.20120409074150.9940: *4* The @auto read code now catches failed asserts in import code.
@nocolor-node

If an assert fails, the entire file is read into a single node.
#@+node:ekr.20120409182030.10028: *4* Fixed several problems with c-to-py command
@nocolor-node

An assert failed during scanning in mungeAllFunctions.

Added defensive code to mungeAllFunctions, dedentBlocks and
replaceComments. The new code simply increments a pointer if a "progress"
assert would fail. (The progress assert still exists, as a double-check.)

Fixed bug: the call to u.afterChangeGroup in the go() method is called only once.

Suppress warning messages given by CPrettyPrinter.indent.
#@+node:ekr.20120401144849.10036: *4* Fixed bug 971171: re .leoRecentFiles
@language python
@language rest

If If $(HOME)/.leo/.leoRecentFiles.txt does not exist,
the only recent file ever is the current file
https://bugs.launchpad.net/leo-editor/+bug/971171

The fix: rf.writeRecentFilesFile creates $(HOME)/.leo/.leoRecentFiles.txt if it does not exist.
#@+node:ekr.20120401144849.10035: *4* Fixed bugs 971166 & 979142 re copy/paste
@language python
@language rest

These bugs are really the same bug

Node body contents displayed is unpredictably incorrect
https://bugs.launchpad.net/leo-editor/+bug/979142

Prints to tabs in the Log Pane are UTF-8 encoded
https://bugs.launchpad.net/leo-editor/+bug/971166

The fix was:

1. Use the "slow" code in leoQTextEditWidget.get.
2. Use w.get/setAllText in leoFrame.pasteText.
#@+node:ekr.20120413152012.10048: *4* Minimize scrolling during paste-text
#@+node:ekr.20120427064024.10062: *4* Fixed bug 711158: Warn if same .leo file open in another Leo instance
@language python
@language rest

Warn if same .leo file open in another Leo instance
https://bugs.launchpad.net/leo-editor/+bug/711158


What I did:

- The PickleShareDB object is created even if caching (of files) is disabled.
  This allows us to used g.app.db even when --no-cache is in effect.
  
- Added the three methods in app.Detecting already-open files.
#@+node:ekr.20120520055508.11872: *4* Clear previous focus-border after alt-tab
@nocolor-node

This was a recent problem.  Normally setInputState should *not* set the border.

Added code to eventFilter to call remove_border on focus out.

set-xxx-state commands call setInputState with set_border = True.
#@+node:ekr.20120520055508.11874: *4* Fixed special cases of auto-completion of commands
@nocolor-node

If the user has not typed anything in the minibuffer, <alt-x><tab> returns *all* completions.

Otherwise, if there are no completions, the "Completions" tab is empty, *not* all completions.

This behavior is much more intuitive than the old behavior.

The fix was a new special case in k.computeCompletionList.
#@+node:ekr.20111025141618.16484: *4* Fixed bug 879338: Global tables in leoApp.py should describe all languages known to the colorizer
@nocolor-node

https://bugs.launchpad.net/leo-editor/+bug/879338

Having the colorizer colorize a language properly gives the false illusion
that Leo "understands" the language.

Supporting the language in the global tables in leoApp.py makes the
illusion a reality.
#@+node:ekr.20120522160137.9908: *5* Notes
@language rest

Rev 5334 is a first draft of a fix of bug 879338:
Global tables in leoApp.py should describe all languages known to the colorizer
https://bugs.launchpad.net/leo-editor/+bug/879338

The essence of the bug fix is that Leo's language-description tables should
contain entries for all .py files in the leo/modes folder. These files
control the colorizer. If Leo's colorizer knows about a language, then Leo
should know as much as possible about the language.

In concept, this is a fairly straightforward process, but there were *many*
details to handle. If you aren't a Leo developer, you might want to stop
reading now...

===== Tables

Fixing this bug required non-trivial changes to the following tables::

    g.app.language_delims_dict
    # Keys are languages, values are 1,2 or 3-tuples of delims.

    g.app.language_extension_dict
    # Keys are language names, values are extensions.

    g.app.self.extension_dict
    # Keys are extensions, values are language names

I used scripts to generate new entries for these tables, but these scripts
can not possibly deal with the all the complications...

Leo uses these tables as follows:

1.  To generate the comment delimiters in sentinels for each language.

Happily, getting the comment delimiters correct was probably the easiest
part, so Leo should continue to write sentinels properly for
previously-know languages. However, I had to take care to preserve the REM,
CWEB, forth and perlpod hacks, so that comment delims would include the
necessary spaces.

2. To associate file extensions with importers.

Knowing about new file extensions doesn't actually allow Leo to import any
new languages. For all languages without an official importer Leo will
simply copy the entire text of the file into a single node, as it always
has.

3. To colorize code.

Leo's colorizer mostly doesn't use these tables: to colorize language x,
the colorizer looks for the file leo/modes/x.py. Thus, these changes
probably do not affect the colorizer at all.

===== Special cases

I did a lot of googling in order to determine the proper file extensions to
use for various language. In the process, I learned that *almost* all
languages described in the leo/modes folder are real, interesting and
useful languages.

However, there at least 5 categories of special cases that affect the
tables:

1. Languages that are really just colorizer modes:

These include embperl, pseudoplain and phpsection. We need entries in
leo/modes for these, but they aren't real languages and thus they should
not appear in the language-description tables.

2. Things that might be colorized but aren't real languages.

Afaik, the following are not real languages, and Leo would never have to
generate files in these languages: cvs_commit,dsssl,relax_ng_compatc and svn_commit.

Notes:

- relax_ng_compact is an xml schema.

- The rtf colorizer is *not* a colorizer for binary .rtf file format, is a
  colorizer for .rtf sources. It probably won't do too much harm to retain
  the colorizer data for these languages, but I wouldn't mind eliminating
  them either.

3.  Unknown languages.

A few languages seem not really to exist: freemarker, hex, jcl, progress, props.

4. Languages without real comment delimiters.

Patch annotations are *not* real comment delimiters, so Leo could not
generate patch (.fix or .patch) files from an outline. Happily, there is no
need to do so.

5. Conflicting file extensions.

There are two separate kinds of problems:

A. Leo contains colorizers for several assembly languages. Typically,
assembly languages have .asm or .a file extensions. However, a particular
extension can only be associated with a single language name. Thus, Leo has
no way of knowing what language to associate with .asm or .a files. So I
just punted and didn't make any association at all.

B. Both the rebol and r languages use the .r file extension. One of Leo's
users previously created an entry for rebol, so that's the language that
takes precedence.
#@+node:ekr.20120523114117.10885: *5* Removed unused files from leo/modes directory
@language rest

Remove all .xml files in the leo/modes directory.

Imo, this should have been done long ago, for at least the following
reasons:

- These files are part of the jEdit project.
- They are used only by the jedit2py script in scripts.leo.
- The colorizer doesn't use them.
- Bug fixes to the colorizer are made to the .py files, not to the .xml files.
- We can always get updated versions of the .xml files from the jEdit
  project in the unlikely event that we ever need them again.

2. Remove the following .py files from the leo/modes directory:
cvs_commit.py, dsssl.py, freemarker.py, hex.py, jcl.py, progress.py,
props.py and svn_commit.py.

Notes:

- embperl.py, phpsection.py and pseudoplain.py will *not* be removed;
they are internal colorizer states.

- relax_ng_compact.py will be removed if it is not used by any other
colorizer.

- patch.py and rtf.py colorizers will be retained, even though Leo can
never generate such files. 
#@+node:ekr.20120522024827.9900: *5* Scripts
#@+node:ekr.20111021035504.9469: *6* Script: get all comments from modes (slow)
@language python

'''Slow script.'''

import glob
import imp

@others

if 0: # The other script is much faster.
    
    keys = ("lineComment","commentStart","commentEnd",)
    d = {}
        # Keys are language names.
        # Values are a list of comment delims, in keys order.
    
    paths,modes_path = get_paths()
    for path in paths:
        module_name = g.shortFileName(path)[:-3]
        module = import_module(module_name,modes_path)
        aList = []
        for key in keys:
            val = module.properties.get(key)
            if val: aList.append(val)
        d[module_name] = aList
    
    print('-'* 20)
    print('language_delims_dict')
    for key in sorted(d):
        print('%16s: "%s"' % ('"%s"' % (key),' '.join(d.get(key))))
#@+node:ekr.20111021035504.9470: *7* get_paths
def get_paths():
    
    modes_path = g.os_path_finalize_join(g.app.loadDir,'..','modes')
    pattern = g.os_path_finalize_join(modes_path,'*.py')
    paths = glob.glob(pattern)
    paths = [z for z in paths if not z.endswith('__init__.py')]
    return paths,modes_path
#@+node:ekr.20111021035504.9471: *7* import_module
def import_module(module_name,modes_path):
    
    data = imp.find_module(module_name,[modes_path])
        # This can open the file.
    theFile,pathname,description = data
    module = imp.load_module(module_name,theFile,pathname,description)
    return module
#@+node:ekr.20110528103005.18319: *6* Script to create global data structures from in modes/*.py files
@language python

'''Script to create global data structures from modes/*.py files.'''

import glob
import imp

g.cls()

theDir = g.os_path_finalize_join(g.app.loadDir,'..','modes','*.py')
aList = glob.glob(theDir)

theDir = g.os_path_finalize_join(g.app.loadDir,'..','modes')

# print('-'*40)
known_keys = list(g.app.language_delims_dict.keys())
new_languages = {}

for z in aList:
    name = g.os_path_basename(z)
    name2 = name[:-3]
    if name2 in known_keys or name2.startswith('__'):
        if 0: print('ignore: %s' % (name2))
    else:
        try:
            theFile, pathname, description = imp.find_module(name2,[theDir])
            m = imp.load_module(name2, theFile, pathname, description)
            if hasattr(m,'properties'):
                # new_languages.append(name2)
                new_languages[name2] = m
            else:
                print('no properties: %s %s' % (name2,m))
        except Exception:
            g.es_exception()
            
print('%s new languages\n' % (len(list(new_languages.keys()))))
    
for key in sorted(new_languages.keys()):
    m = new_languages.get(key)
    aList2 = [m.properties.get(z)
        for z in ('lineComment','commentStart','commentEnd')
            if m.properties.get(z)]
    print('%-20s : "%s",' % (
        '"%s"' % (key),
        ' '.join(aList2)))
    # computed[name2] = ' '.join(aList2)
       
if 0:
    mismatches = 0
    print()
    for z in known_keys:
        val = g.app.language_delims_dict.get(z)
        val2 = computed.get(z)
        if not val:
            print('no val',z)
        elif not val2:
            print('no val2',z)
        elif val != val2:
            mismatches += 1
            print('mismatch for %s. expected %s got %s' % (z,repr(val),repr(val2)))
            print(repr(val))
            print(repr(val2))
    print('%s mismatches' % mismatches)
#@+node:ekr.20120522160137.9914: *5* @test consistency of leoApp tables
@
language_delims_dict 
    # Keys are languages, values are 1,2 or 3-tuples of delims. 
language_extension_dict
    # Keys are languages, values are extensions.
extension_dict = {
    # Keys are extensions, values are languages.
@c

delims_d    = g.app.language_delims_dict
lang_d      = g.app.language_extension_dict
ext_d       = g.app.extension_dict

for lang in lang_d:
    ext = lang_d.get(lang)
    assert lang in delims_d,'fail 1: %s' % lang
    assert ext in ext_d,'fail 2: %s' % ext
for ext in ext_d:
    lang = ext_d.get(ext)
    assert lang in lang_d,'fail 3: %s' % lang
#@+node:ekr.20120524082434.9920: *4* Fixed crash after viewrendered-hide
#@+node:ekr.20120523095917.9905: *4* Fixed bug: elected node was not always restored properly
@nocolor-node

The bug was in chapter.findPositionInChapter.
#@+node:ekr.20120525051746.9982: *4* Fixed bug 998090: save file as doesn't remove entry from open file list
@nocolor-node

https://bugs.launchpad.net/leo-editor/+bug/998090
save file as doesn't remove entry from open file list

Save file as leaves the file's previous path in g.app.db.openFiles, so
that next time the original file's opened you get a "already open"
message.
#@+node:ekr.20120308120323.9864: *4* Wont Fix bug 903640: Import of Python files containing the strings "<<" and ">>" does not work
@nocolor-node

See: https://bugs.launchpad.net/leo-editor/+bug/903640
Import of Python files containing the strings "<<" and ">>" does not work

At present @auto can import .py files containing self.cprint("<<" + ret +
">>\n")

Furthermore, it's possible to write such files properly after changing
them.

Thus, this bug seems to have been completely fixed, as far as @auto goes.

However, *importing* the file with Leo's import-file command does fail (an
@ignore is inserted). This is expected: unlike @auto, the import command
creates an @file node, so the "perfect import" check will complain that the
section called << ret >> is undefined.

I am going to close this item. I see no real need to support other section
delimiters in external files. If there ever becomes a real need to do so, a
separate wishlist item will be appropriate.
#@+node:ekr.20120418065452.10029: *4* Fixed bug 981849: incorrect body content shown
@language python
@language rest

https://bugs.launchpad.net/leo-editor/+bug/981849

The original fix was misguided. It attempted to use more careful code in
setSelectionRangeHelper & lengthHelper.
    
The new fix avoids messing with the viewport in both setEditorColors methods:

leo-editor thread: opening new top level windows
http://groups.google.com/group/leo-editor/browse_thread/thread/8f5f6c72d8716b33

The key is to use a descriptor in LeoQTextBrowser stylesheets.  Example::

'LeoQTextBrowser { << the actual stylesheet >> }

See http://stackoverflow.com/questions/9554435/qtextedit-background-color-change-also-the-color-of-scrollbar


    
#@+node:ekr.20120602154454.10193: *4* Fixed import problems discovered by importing 2to3
@language rest

** Not all import problems can be fixed automatically! **

- Added perfectImportFlag. (There was already an importing flag).

- Fixed bug in Fixed underindent convention:

    undentBy adds a period; parseUnderindentTag removes the period.
    
- @file read code must *regenerate* the \\- convention.

    This is done by readNormalLine.
    
    - Fixed an unrelated bug in g.computeWidth.  All unit tests pass.

    - Created g.computeLeadingWhitespaceWidth.
    
- some docstrings are not imported properly in py2_test_grammar.py

    The must be fixed by hand, using @raw and @end_raw.

- escapeFalseSectionReferences now is a do-nothing:
    
    It never generates @verbatim sentinels during import.
#@+node:ekr.20120605185352.10283: *5*  Notes: do not delete
@language rest

Rev 5378: cleanup-imported-nodes script in scripts.leo & an Aha
http://groups.google.com/group/leo-editor/browse_thread/thread/77b9df4f4ed6dba0

> The third (and I think last) import fail involves not generating
> @verbatim sentinels when importing files.

Fixed in the trunk at rev 5386.

This is (to me) a really interesting dark corner of Leo's import code.

By searching for @verbatim, I discovered a method called
escapeFalseSectionReferences.  This method inserts an @verbatim
"directive" before lines that look like section references.

This is wrong for multiple reasons.  It confuses the importer, there
is no such thing as an @verbatim directive, and worst, it fails to
solve the essential problem, which is that before the imported file is
saved, the **user** must fix the problem!

For example, when importing a line like::

  a = x << y >> z

The user, and *only* the user, should change this to something like::

   a = x << y \
   >> z # EKR

I suppose each importer could figure out a language-specific
workaround, but imo this isn't particularly important, for reasons
which will become clearer below.

So now escapeFalseSectionReferences is a do-nothing.

With this explanation, perhaps the checkin log will make sense::

QQQQQ
Fixed another import fail in an "interesting" way: the import code no
longer inserts @verbatim. This means a later write of the imported
will fail. This is correct!

Indeed, the failed write is the only way to alert the user that the
code must be revised by hand.

Note that another import fail, involving a leading '@' on a line in a
docstring, must also be fixed by hand. In lib2to3/pgen/grammar.py the
*only* possible fix is to enclose the entire docstring at the end of
the file by @raw and @end_raw.

All unit tests pass, but no new tests have been added so far.
QQQQQ

The other import fail mentioned in the checkin log is a truly
fascinating case, one that no amount of AI could possibly discover the
correct fix.

At the very end of lib2to3/pgen/grammar.py the following code
(shortened a bit) appears at the top level::

   opmap_raw = """
   ( LPAR
   ) RPAR
   [snip]
   @ AT
   [snip]
   == EQEQUAL
   != NOTEQUAL
   """
   opmap = {}
   for line in opmap_raw.splitlines():
       if line:
           op, name = line.split()
           opmap[op] = getattr(token, name)

There are several things to notice about this code:

1. It contains a line starting with '@'.  Sooner or later, this is
going to cause problems for either Leo's import code or Leo's write
code.

2. It's overly clever, but it's overly clever for a reason: it's
testing tokenizing logic.

3. The code at the end of the file assumes that all lines of the
docstring are 2-tuples.

For these reasons, the one and *only* possible way to make Leo write
this code correctly is to enclose the *entire* docstring in @raw and
@end_raw directives.  Like this::

   @raw
   opmap_raw = """
   ( LPAR
   ) RPAR
   [snip]
   @ AT
   [snip]
   == EQEQUAL
   != NOTEQUAL
   """
  @end_raw

In particular, surrounding the line "@ AT" with @raw/@end_raw
directives will cause 2to3 to fail on startup:  the Leo sentinel lines
will not be 2-tuples!

===== Important Conclusions

All this picky detail illustrates a crucial fact.  No matter how good
Leo's importers are, (and they are now quite good), there will
*always* be cases where thoughtful human intervention will be
required.

Furthermore, the simplest thing that could possibly work is for the
importers to allow some constructions that are guaranteed to cause
problems later, when the user attempts to write the file.  We hope
that Leo will complain about certain constructions, but Leo may not be
able to complain about all constructions.

Thus, some import mistakes can *only* be found by running tests.  For
complex programs like 2to3, the only truly safe way to check imports
is by running the 2to3 test suite.
#@+node:ekr.20120921145435.10607: *4* Fixed activateMenu
@nocolor-node
The trick is to find the wrapper first: it is *also* a QMenu.
We can then call menuBar.setActiveAction on its action!!
#@+node:ekr.20120925061642.13505: *4* Changes in rev 4163 caused the bug
@nocolor-node

The problem is the call to w.setStyleSheet in g.app.gui.update_style_sheet.
Apparently, this causes a layout-request event that spoils the scroll position.

The fixes:
    
1. update_style_sheet does nothing if the new stylesheet is the same as the old.

2. Added lockout to mouseReleaseEvent. update_style_sheet does nothing if
   the lockout is set.
   
3. mouseReleaseEvent sets c.p.v.insertPoint if appropriate.

Hitting Ctrl-H can still cause a small unwanted scroll, but the insert point remains visible.
#@+node:ekr.20120928142052.13477: *4* Fixed scrolling problem with scrollwheel
@nocolor-node

The maintain_scroll option is *evil*.
#@+node:ekr.20121002023749.10191: *4* Fixed problems with scrolling when saving
@nocolor-node

LeoQTextBrowser.onSliderChanged must set v.scrollBarSpot only if "self" is actually the body pane.

Othewise scrolling the log pane will scroll the body pane!
#@+node:ekr.20121002123916.10203: *4* Fixed crasher in active_path.py
#@+node:ekr.20121017074047.10103: *4* Fixed leoBridge problems
#@+node:ekr.20121002082022.10183: *4* fixed crasher involving g.importFromPath
@nocolor-node

Note: happens only with Python 3.3.0.

Here is a minor traceback when opening quickstart.leo

Leo 4.11 devel, build 5468, 2012-09-30 13:02:59
Python 3.3.0, qt version 4.8.3
Windows 6, 1, 7601, 2, Service Pack 1
reading: C:\Python33\Lib\site-packages\leo-editor\leo\doc\quickstart.leo
unexpected exception in g.importFromPath(rest)
Traceback (most recent call last):
  File "C:\Python33\Lib\site-packages\leo-editor\leo\core\leoGlobals.py", line 5689, in importFromPath
    data = imp.find_module(moduleName,[path]) # This can open the file.
  File "C:\Python33\lib\imp.py", line 203, in find_module
    package_directory = os.path.join(entry, name)
  File "C:\Python33\lib\ntpath.py", line 171, in join
    if b[:1] in seps:
TypeError: Type str doesn't support the buffer API
Can not import rest
#@+node:ekr.20121126103128.10137: *4* Fixed missing search text bug
@nocolor-node

The fix was to always call c.selectPosition in leoFind.showSuccess.
This ensures that leoTree.setBodyTextAfterSelect always does w.setAllText,
which is essential to init the syntax colorer properly.

The happy side effect of this change is that a lot of duplicate selection
code in showSuccess disappears.

Also converted two section references in leoTree.selectHelper to selectNewNode.
#@+node:ekr.20121126194831.10147: *4* Fixed crasher in leoBridge
@nocolor-node

The crash happens only when the new readSettings argument to leoBridge.bridgeController is False.
In that case, the global dicts were not inited properly.

What I did:
    
- Created lm.createDefaultSettingsDicts, called by lm.readGlobalSettingsFiles.
- leoBridge.initLeo calls lm.createDefaultSettingsDicts to set the global dicts.
#@+node:ekr.20121126194831.10148: *5* report
@nocolor-node

Bug description:
  --- Begin Python script to run from a console ------
  import leo.core.leoBridge as b
  bridge = b.controller(gui='nullGui',verbose=False,loadPlugins=False,readSettings=False)
  c = bridge.openLeoFile(r'c:\users\edreamleo\test\minimal.leo')
  --- End Python Script -----

  The above script and minimal.leo are attached to this bug report.  Put
  them in the same directory, open a console, set the current working
  directory to the directory containing the script, and run the script.
  You will see the this exception on the console:

  2012-11-16 11:28:51 /home/ldi/tmp
  $ python readSettingsFalse.py
  Traceback (most recent call last):
    File "readSettingsFalse.py", line 5, in <module>
      cmdrUnl = bridge.openLeoFile('minimal.leo')
    File "/home/ldi/bzr/LeoLatest/leo/core/leoBridge.py", line 330, in openLeoFile
      c = self.createFrame(fileName)
    File "/home/ldi/bzr/LeoLatest/leo/core/leoBridge.py", line 367, in createFrame
      c = g.openWithFileName(fileName)
    File "/home/ldi/bzr/LeoLatest/leo/core/leoGlobals.py", line 1875, in openWithFileName
      return g.app.loadManager.loadLocalFile(fileName,gui,old_c)
    File "/home/ldi/bzr/LeoLatest/leo/core/leoApp.py", line 2539, in loadLocalFile
      previousSettings = lm.getPreviousSettings(fn)
    File "/home/ldi/bzr/LeoLatest/leo/core/leoApp.py", line 1668, in getPreviousSettings
      lm.globalSettingsDict,lm.globalShortcutsDict,localFlag=True)
    File "/home/ldi/bzr/LeoLatest/leo/core/leoApp.py", line 1626, in computeLocalSettings
      settings_d = settings_d.copy()
  AttributeError: 'NoneType' object has no attribute 'copy'
  2012-11-16 11:28:55 /home/ldi/tmp
  $
  ---------
  I confirmed this bug on Leo-Editor revisions 5501 and 5505.
#@+node:ekr.20121205091157.12223: *4* Don't horizontally scroll body pane if word wrapping
The fix was in qtBody.setWrap.
#@+node:ekr.20130111082855.10176: *4* Running unit tests no longer change the selected tab
@nocolor-node


The fix was to set new_c=self.c in the call to c.close in createFileFromOutline.
#@+node:ekr.20130410075228.10310: *4* Increased the width of find/change text
@language rest

dw.createFindTab now creates a third column with a minimum width.
The find/change text widgets span the second and third columns.
#@+node:ekr.20120519193038.9883: *3* Code
#@+node:ekr.20110518103946.18179: *4* Added external/leosax.py to leoPyRef.leo
http://groups.google.com/group/leo-editor/browse_thread/thread/93f2cc88ebbf9893
#@+node:ekr.20120529040553.10140: *4* Added local pylint suppressions
The format of such local suppressions is::

    # pylint: disable=<message-number>
#@+node:ekr.20130111082855.10177: *4* g.pdb now does qtGui stuff
#@+node:ekr.20120529083658.11110: *3* Docs
#@+node:ekr.20120529083658.11109: *4* For the FAQ: create a bzr repository before importing it into Leo
@language rest

When I study a program, I like to import it into Leo.  I have several
scripts that do this:  some create @auto nodes; others create @file
nodes.

Either way, the import process has the potential to change many
files.  Usually, I just change @auto and @file to @@auto or @@file, so
that any changes I make while studying the code won't affect the
originals.

But this "safety first" approach means that I can't actually use Leo
to insert tracing statements (or for any other changes.)  A few days
ago, I found a way to import "live" code into Leo safely:

   Create a bzr repository for the code before importing it

The Aha is to create the repository *wherever the code is*, including,
say, python/Lib/site-packages.

- bzr qdiff ensures that import hasn't significantly altered the code,
- bzr revert undoes any unwise or unwanted changes.

This is exactly what I need:  I can make changes to important tools
*safely* within Leo.
#@+node:ekr.20121126213658.10142: *4* Documented all keyword args to the leoBridge.controller ctor
#@+node:ekr.20120330040023.9781: *3* Features
#@+node:ekr.20120926072255.12062: *4* @color minibuffer-foreground-color setting
#@+node:ekr.20120410175141.10027: *4* Added --no-plugins option
#@+node:ekr.20120928142052.11489: *4* Added @color log_warning_color and g.getActualColor
@nocolor-node

g.es now "redirects" colors using the following settings:

    'black':    @color log_text_foreground_color
    'blue':     @color log_warning_color
    'red':      @color log_error_color
#@+node:ekr.20120622075651.10000: *4* Added c2 keyword arg to c.bringToFront
#@+node:ekr.20120622075651.10001: *4* Added default button to dialog methods
#@+node:ekr.20120321174708.9743: *4* Added docstrings for all commands
@nocolor-node

Help-for command translate !<command-name>! in the docstring to the binding for command-name.
#@+node:ekr.20120912094259.10550: *4* Added support for @testclass
@nocolor-node
@

@testclass nodes should set either the suite or testclass vars.

@suite nodes should set the suite var.
#@+node:ekr.20120912094259.10546: *5* makeTestClass
def makeTestClass (self,p):

    """Create a subclass of unittest.TestCase"""

    c,tm = self.c,self
    fname = 'makeTestClass'
    p = p.copy()
    script = g.getScript(c,p).strip()
    if not script:
        print("nothing in %s" % p.h)
        return None
    try:
        script = script + tm.get_test_class_script()
        script = script + tm.get_suite_script()
        d = {'c':c,'g':g,'p':p,'unittest':unittest}
        if c.write_script_file:
            scriptFile = c.writeScriptFile(script)
            if g.isPython3:
                exec(compile(script,scriptFile,'exec'),d)
            else:
                execfile(scriptFile,d)
        else:
            exec(script + '\n',d)
        testclass = g.app.scriptDict.get('testclass')
        suite = g.app.scriptDict.get('suite')
        if suite and testclass:
            print("\n%s: both 'suite' and 'testclass defined in %s" % (
                fname,p.h)) 
        elif testclass:
            suite = unittest.TestLoader().loadTestsFromTestCase(testclass)
            return suite
        elif suite:
            return suite
        else:
            print("\n%s: neither 'suite' nor 'testclass' defined in %s" % (
                fname,p.h))
            return None
    except Exception:
        print('\n%s: exception creating test class in %s' % (fname,p.h))
        g.es_print_exception()
        return None
#@+node:ekr.20051104075904.4: *5* TM.doTests & helpers (local tests)
def doTests(self,all=None,marked=None,verbosity=1):

    '''Run any kind of local unit test.

    Important: this is also called from dynamicUnitTest.leo
    to run external tests "locally" from dynamicUnitTest.leo
    '''

    trace = False
    c,tm = self.c,self

    # 2013/02/25: clear the screen before running multiple unit tests locally.
    if all or marked: g.cls()
    p1 = c.p.copy() # 2011/10/31: always restore the selected position.

    # This seems a bit risky when run in unitTest.leo.
    if not c.fileName().endswith('unitTest.leo'):
        if c.isChanged():
            c.save() # Eliminate the need for ctrl-s.

    if trace: g.trace('marked',marked,'c',c)
    try:
        g.unitTesting = g.app.unitTesting = True
        g.app.runningAllUnitTests = all and not marked # Bug fix: 2012/12/20
        g.app.unitTestDict["fail"] = False
        g.app.unitTestDict['c'] = c
        g.app.unitTestDict['g'] = g
        g.app.unitTestDict['p'] = c.p.copy()

        # c.undoer.clearUndoState() # New in 4.3.1.
        changed = c.isChanged()
        suite = unittest.makeSuite(unittest.TestCase)
        aList = tm.findAllUnitTestNodes(all,marked)
        setup_script = None
        found = False
        for p in aList:
            if tm.isTestSetupNode(p):
                setup_script = p.b
                test = None
            elif tm.isTestNode(p):
                if trace: g.trace('adding',p.h)
                test = tm.makeTestCase(p,setup_script)
            elif tm.isSuiteNode(p): # @suite
                if trace: g.trace('adding',p.h)
                test = tm.makeTestSuite(p,setup_script)
            elif tm.isTestClassNode(p):
                if trace: g.trace('adding',p.h)
                test = tm.makeTestClass(p) # A suite of tests.
            else:
                test = None
            if test:
                suite.addTest(test)
                found = True
        # Verbosity: 1: print just dots.
        if not found:
            # 2011/10/30: run the body of p as a unit test.
            if trace: g.trace('no found: running raw body')
            test = tm.makeTestCase(c.p)
            if test:
                suite.addTest(test)
                found = True
        if found:
            res = unittest.TextTestRunner(verbosity=verbosity).run(suite)
            # put info to db as well
            if g.enableDB:
                key = 'unittest/cur/fail'
                archive = [(t.p.gnx,trace2) for (t,trace2) in res.errors]
                c.cacher.db[key] = archive
        else:
            g.error('no %s@test or @suite nodes in %s outline' % (
                g.choose(marked,'marked ',''),
                g.choose(all,'entire','selected')))
    finally:
        c.setChanged(changed) # Restore changed state.
        if g.app.unitTestDict.get('restoreSelectedNode',True):
            c.contractAllHeadlines()
            c.redraw(p1)
        g.unitTesting = g.app.unitTesting = False
#@+node:ekr.20120912094259.10549: *6* get_suite_script
def get_suite_script(self):

    s = '''

try:
    g.app.scriptDict['suite'] = suite
except NameError:
    pass

'''
    return g.adjustTripleString(s, self.c.tab_width)
#@+node:ekr.20120912094259.10547: *6* get_test_class_script
def get_test_class_script(self):

    s = '''

try:
    g.app.scriptDict['testclass'] = testclass
except NameError:
    pass

'''
    return g.adjustTripleString(s,self.c.tab_width)
#@+node:ekr.20051104075904.13: *6* makeTestCase
def makeTestCase (self,p,setup_script):

    c = self.c
    p = p.copy()

    if p.b.strip():
        return GeneralTestCase(c,p,setup_script)
    else:
        return None
#@+node:ekr.20120912094259.10546: *6* makeTestClass
def makeTestClass (self,p):

    """Create a subclass of unittest.TestCase"""

    c,tm = self.c,self
    fname = 'makeTestClass'
    p = p.copy()
    script = g.getScript(c,p).strip()
    if not script:
        print("nothing in %s" % p.h)
        return None
    try:
        script = script + tm.get_test_class_script()
        script = script + tm.get_suite_script()
        d = {'c':c,'g':g,'p':p,'unittest':unittest}
        if c.write_script_file:
            scriptFile = c.writeScriptFile(script)
            if g.isPython3:
                exec(compile(script,scriptFile,'exec'),d)
            else:
                execfile(scriptFile,d)
        else:
            exec(script + '\n',d)
        testclass = g.app.scriptDict.get('testclass')
        suite = g.app.scriptDict.get('suite')
        if suite and testclass:
            print("\n%s: both 'suite' and 'testclass defined in %s" % (
                fname,p.h)) 
        elif testclass:
            suite = unittest.TestLoader().loadTestsFromTestCase(testclass)
            return suite
        elif suite:
            return suite
        else:
            print("\n%s: neither 'suite' nor 'testclass' defined in %s" % (
                fname,p.h))
            return None
    except Exception:
        print('\n%s: exception creating test class in %s' % (fname,p.h))
        g.es_print_exception()
        return None
#@+node:ekr.20051104075904.12: *6* makeTestSuite
# This code executes the script in an @suite node.
# This code assumes that the script sets the 'suite' var to the test suite.

def makeTestSuite (self,p,setup_script):

    """Create a suite of test cases by executing the script in an @suite node."""

    c,tm = self.c,self
    fname = 'makeTestSuite'
    p = p.copy()
    script = g.getScript(c,p).strip()
    if not script:
        print("no script in %s" % p.h)
        return None
    if setup_script:
        script = setup_script + script
    try:
        script = script + tm.get_suite_script()
        d = {'c':c,'g':g,'p':p}
        if c.write_script_file:
            scriptFile = c.writeScriptFile(script)
            if g.isPython3:
                exec(compile(script,scriptFile,'exec'),d)
            else:
                execfile(scriptFile,d)
        else:
            exec(script + '\n',d)
        suite = g.app.scriptDict.get("suite")
        if not suite:
            print("\n%s: %s script did not set suite var" % (fname,p.h))
        return suite
    except Exception:
        print('\n%s: exception creating test cases for %s' % (fname,p.h))
        g.es_print_exception()
        return None
#@+node:ekr.20120524082434.9923: *4* All viewrended commands now start with vr
#@+node:ekr.20130117143204.10205: *4* Allow clones anywhere in @file nodes
@language rest

Reposted from http://groups.google.com/group/leo-editor/browse_thread/thread/67a28984616d09c9
About bug 882243: clones sometimes not saved

What I did:

- Added allow_cloned_sibs switch at the start of leoAtFile.py.
  All new code enabled by this switch.

- Refactored at.createNewThinNode:
    - Renamed createThinChild4 to old_createThinChild4.
    - Added new_createThinChild4.
    - Added createV5ThinNode.
    
The key invariant in createV5ThinNode:
    On exit from at.changeLevel, top of at.thinNodeStack is the parent.
#@+node:ekr.20120409074150.9941: *4* baseNativeTree.onHeadChanged now truncates headlines
@nocolor-node

The new code works like leoTree.onHeadChanged.

The code can be called twice, so it is a bit tricky
to only issue warnings once.
#@+node:ekr.20120928081706.11455: *4* body border
- (done) deprecate/reorganize *dynamic* body text background colors.
    - (done) set body pane color only once. (Now done only via stylesheet).
    - (done) use border colors to indicate state.
#@+node:ekr.20121002023749.10190: *4* Cached syntax coloring
#@+node:ekr.20120519114248.9886: *4* Change focus-border color depending on input state
@nocolor-node

Leo now supports two new setting, with the indicated default::

    @color focus_border_command_state_color = blue
    @color focus_border_overwrite_state_color = green

This works in conjunction with the existing border-color setting::

    @color focus_border_color = red

So by default, the color border is red when in insert state, and blue
if in command state.

This allows the focus border to change color depending on whether we
are in input or command mode.  This is a workaround for the (extreme)
difficulty of changing cursors depending on mode.

When used to distinguish insert from command modes, I recommend
changing the existing setting::

    @int focus_border_width = 1

to::

    @int focus_border_width = 2

This makes the focus border much more visible, and makes the color
changes obvious.

I tested this code without changing bindings using the set-command-
state command.

As before, if you don't want any such colored borders, just do::

    @bool use_focus_border = False 
#@+node:ekr.20120529105626.10145: *5* What I did
Changed set-xxx-state & setInputState.

Changed qtGui.add_border and remove_border.
#@+node:ekr.20120522160137.9907: *4* Completed apropos-regular-expressions command
#@+node:ekr.20120420095827.9947: *4* Completed the new support for sessions
@nocolor-node

(Done) Changed es so it always queues messages when g.app.log is None.
(Done) complete the command-line args: --session-save and --session-restore.
(Done) Write session info in leoTabbedTopLevel.closeEvent and g.app.onQuit.

Rev 5324 finishes some session-related work. The existing
session commands are unchanged, but Leo now fully supports
two new command-line arguments::

    --session-restore     restore previously saved session tabs at startup
    --session-save        save session tabs on exit

If you use both arguments, everything is automatic: Leo
saves the tabs when you quit Leo, and restores tabs when you
start Leo. Note that you can still specify file names on the
command line in addition to whatever files --session-restore
will open for you.

If you use only --session-restore, it is up to you to save
sessions "by hand" with one of the session commands, for
instance, session-snapshot-save.
#@+node:ekr.20120429125741.10056: *4* created parse-body command
@nocolor-node

Useful for re-parsing text that was not originally parsed properly.
#@+node:ekr.20111017102409.15875: *4* Created print-buttons command
@nocolor-node

Created print-buttons command, showing source of all @command and @button nodes.

Changed ParserBaseClass.doButtons/doCommands so they return
lists of (p.copy(),script) rather than (p.h,script)

Added g.app.config.atLocalButtonsList & g.app.config.atLocalCommandsList
for use by print-buttons command.
#@+node:ekr.20120525051746.9983: *4* Fixed bug 994985: Wishlist: normalize-whitespace
@nocolor-node

https://bugs.launchpad.net/bugs/994985
Wishlist: normalize-whitespace

When using @auto, the logic often complains about "abnormal" whitespace and
refuses to write/read node normally.

What I did:
    
1. The clean-lines command (and thus the clean-all-lines command)
   now remove trailing whitespace while preserving newlines.
   
2. reportMismatch suggests using the clean-all-lines command.
   Note: a good unit test for reportMismatch already exists.
   
3. Added a unit test for clean-lines.
#@+node:ekr.20121002082022.10182: *4* Fixed dabbrev commands
@nocolor-node

The commands now work, and Alt-slash and Alt-Ctrl-slash are bound as in Emacs.
#@+node:ekr.20120519114248.9887: *4* Fully support :: convention in @mode nodes
@nocolor-node

If the @mode headline contains ::, everything following
the :: is the mode prompt. For example::
    
    @mode abc :: xyz
    
Creates the enter-abc-mode command, but the prompt for the command is xyz.

ParserBaseClass.createModeCommand creates this convention.
Changed k.modeHelpHelper.
#@+node:ekr.20120524082434.12620: *4* help-for-command executes apropos commands
Instead of just printing their docstrings.
#@+node:ekr.20120519114248.9885: *4* Improved incremental search commands
@nocolor-node

- Documented that return ends the search.
- Documented that deleting the entire search pattern aborts the search.
- Removed annoying status messages printed to log.

- (Can't do) If text is already highlighted, Alt-S or Alt-R should use that text.

#@+node:ekr.20120419095424.9924: *4* Integrated free_layout into Leo's core
#@+node:ekr.20070521105645: *4* investigated epydoc
@language rest

http://sourceforge.net/forum/message.php?msg_id=4319363

http://epydoc.sourceforge.net/epydoc.html
By: ktenney

epydoc doesn't seem active.  It produces interesting web sites,
but a find-def function in Leo itself would be more useful.

#@+node:ekr.20120527053550.10118: *5* Fixed epydoc crash
The easy solution, as revealed on StackOverflow is to edit the offending
line to catch the error:

# markup/restructuredtext.py, line 307
<   m = self._SUMMARY_RE.match(child.data)
---
>   try:
>      m = self._SUMMARY_RE.match(child.data)
>   except AttributeError:
>      m = None

#@+node:ekr.20111017132257.15891: *4* IPython now works with all versions of IPython
@nocolor-node

Changed:

- init (qtGui.py top level) (qtPdb)
- runMainLoop (qtGui)
- start_new_api
#@+node:ekr.20120330110655.10023: *5*  Notes
@language rest

Investigate how IPython hijacks event loops
http://groups.google.com/group/leo-editor/browse_thread/thread/e1dc6439bf8b17f9

pyos_inputhook is relevant

IPython lib.inputhook
http://ipython.org/ipython-doc/stable/api/generated/IPython.lib.inputhook.html


* IPython seems to require Python 2.x.
* I can run IPython from either C:\prog\ipython-0.12 or from python\lib\site-packages

From C:\prog\ipython-0.12\IPython\scripts

#!/usr/bin/env python
"""Terminal-based IPython entry point.
"""

from IPython.frontend.terminal.ipapp import launch_new_instance

launch_new_instance()

Here is ipapi.get::

@language python

    def get():
        """Get the global InteractiveShell instance."""
        from IPython.core.interactiveshell import InteractiveShell
        return InteractiveShell.instance()
#@+node:ekr.20120401144849.10152: *5*  What I did
@language rest

- Import logic looks for legacy IPython first (0.11 and prev),
  then looks for new-style IPython (0.12 and above).

- Created GlobalIPythonManager class, assigned to leoIPython.gipm and g.app.gipm.

- Added self.c ivar to LeoNode class.  This is the same as p.v.context.
#@+node:ekr.20120330110655.10025: *5* @url ipython api
http://ipython.org/ipython-doc/rel-0.12/api/index.html
#@+node:ekr.20120330110655.10024: *5* @url ipython-dev archive
http://mail.scipy.org/pipermail/ipython-dev/
#@+node:ekr.20120330110655.10026: *5* @url ipython.core.interactiveshell
http://ipython.org/ipython-doc/rel-0.12/api/generated/IPython.core.interactiveshell.html
#@+node:ekr.20120415133744.10050: *4* Made sure tab completion only happens on explicit tab
@language python

# Don't show full completion list when the minibuffer becomes empty.
#@+node:ekr.20130111120935.10193: *4* SherlockTracer now shows returned values
#@+node:ekr.20120305084218.9915: *4* Show all commands after <alt-x><tab>
@nocolor-node

A simple change to k.computeCompletionList was all that was needed.

#@+node:ekr.20120411095406.10036: *4* Sorted statistics in profile_leo
#@+node:ekr.20121011093316.10103: *4* Support TypeScript
# Added TypeScriptScanner class and related code.
#@+node:ekr.20120523133829.14118: *4* Supported ctrl-clicks in vr panes
#@+node:ekr.20120517124200.9985: *4* Supported vimoutliner imports and @auto-otl
@nocolor-node

Created vimoutlinerScanner.

Created at.writeAtAutoOtlFile.
#@+node:ekr.20130126062633.10184: *4* Allow cloned siblings
@language rest

A major change in Leo's read/write code.  The first "live" rev was 5584.
At present, controlled by the allow_cloned_sibs switch in leoAtFile.py.

Fixes the following bugs:

clones sometimes not saved
https://bugs.launchpad.net/leo-editor/+bug/882243

When all clones of a node are in an @file subtree, they disappear on exit
https://bugs.launchpad.net/leo-editor/+bug/1084661
#@+node:ekr.20130126080923.10186: *4* Improved g.trace
@language rest

A few changes that should have been done long ago:

- Added support for 'before' keyword.  Prints something before the function name.
- Use g.shortFileName(__file__) instead of "<module>"
#@+node:ekr.20130404060056.10454: *4* Added support for @testsetup
@language rest

This is a major step forward.  It is a much cleaner solution than exec(g.findTestScript)
#@+node:ekr.20130329063228.10304: *4* Allow periods before section names in headlines
@nocolor-node

Skip '.' before section names in headlines.
#@+node:ekr.20130412141141.10332: *4* All @button nodes now show call tips
@language rest

These are generated from the @button node's docstring, if it exists.
#@+node:ekr.20130412173637.11777: *4* help-for-python now uses vr window
#@+node:ekr.20130413061407.10359: *4* Added vr-expand/contract commands
#@+node:ekr.20120602062004.13335: *3* Scripts
#@+node:ekr.20111019104425.15893: *4* Added jinja2 templating script to scripts.leo
Should Leo support a standard template tool?
http://groups.google.com/group/leo-editor/browse_thread/thread/dd629473f4b3c4fc

Added a jinja2 templating example.  See:
    
file://../scripts/scripts.leo#Scripts-->@thin%20leoScripts.txt-->Important-->Prototype%20of%20jinja%20@command%20nodes
#@+node:ekr.20120531092617.10141: *4* Added cleanup-imported-nodes script to scripts.leo
@nocolor-node

The script itself is in scripts.leo: cleanup-imported-nodes

- Put docstring in root node.
- Use section reference for declarations.
- Remove leading and trailing blank lines from all nodes, leaving only a trailing newline.
- If a new contains nothing but comments, merge it with the next node.
- If a node contains nothing but a property, merge it with the previous node.

The holy grail would be to do all this in the Python importer, but a post-processing script suffices.

Workflow:
    
1. bzr checkin of all *unchanged* file.
2. Import all files and save.
3. bzr commit all changed files.
4. Run cleanup script.
5. bzr qdiff then shows all cleanups.
6. bzr commit if all goes well.
#@+node:ekr.20130417081749.10492: ** porting docutils
#@+node:ekr.20130421002947.10748: *3* Script: write constants to log pane
# -*- coding: utf8 -*-

# Define the constants for the define_xxx functions in the new punctuations_chars.py.
import unicodedata
s =  ur"\.\,\;\!\?"
assert not g.isPython3
d = {}
for uc in s:
    assert isinstance(uc,(str,unicode)),type(uc)
    comment = unicodedata.name(uc,'Unknown') if isinstance(uc,unicode) else 'ascii'
    d[ord(uc)] = comment
for i in sorted(d.keys()):
    g.es('%5s, # %s' % (i,d.get(i)))
#@+node:ekr.20130421002947.10747: *3* Tests: do not delete
#@+node:ekr.20130418080354.10719: *4* @test special chars script & helpers
# -*- coding: utf8 -*-

from __future__ import print_function

g.cls()

import re
import sys
import unicodedata

openers = ur"""\"\'\(\<\[\{༺༼᚛⁅⁽₍〈❨❪❬❮❰❲❴⟅⟦⟨⟪⟬⟮⦃⦅⦇⦉⦋⦍⦏⦑⦓⦕⦗⧘⧚⧼⸢⸤⸦⸨〈《「『【〔〖〘〚〝〝﴾︗︵︷︹︻︽︿﹁﹃﹇﹙﹛﹝（［｛｟｢«‘“‹⸂⸄⸉⸌⸜⸠‚„»’”›⸃⸅⸊⸍⸝⸡‛‟"""
closers = ur"""\"\'\)\>\]\}༻༽᚜⁆⁾₎〉❩❫❭❯❱❳❵⟆⟧⟩⟫⟭⟯⦄⦆⦈⦊⦌⦎⦐⦒⦔⦖⦘⧙⧛⧽⸣⸥⸧⸩〉》」』】〕〗〙〛〞〟﴿︘︶︸︺︼︾﹀﹂﹄﹈﹚﹜﹞）］｝｠｣»’”›⸃⸅⸊⸍⸝⸡‛‟«‘“‹⸂⸄⸉⸌⸜⸠‚„"""
delimiters = ur"\-\/\:֊־᐀᠆‐‑‒–—―⸗⸚〜〰゠︱︲﹘﹣－¡·¿;·՚՛՜՝՞՟։׀׃׆׳״؉؊،؍؛؞؟٪٫٬٭۔܀܁܂܃܄܅܆܇܈܉܊܋܌܍߷߸߹࠰࠱࠲࠳࠴࠵࠶࠷࠸࠹࠺࠻࠼࠽࠾।॥॰෴๏๚๛༄༅༆༇༈༉༊་༌།༎༏༐༑༒྅࿐࿑࿒࿓࿔၊။၌၍၎၏჻፡።፣፤፥፦፧፨᙭᙮᛫᛬᛭᜵᜶។៕៖៘៙៚᠀᠁᠂᠃᠄᠅᠇᠈᠉᠊᥄᥅᧞᧟᨞᨟᪠᪡᪢᪣᪤᪥᪦᪨᪩᪪᪫᪬᪭᭚᭛᭜᭝᭞᭟᭠᰻᰼᰽᰾᰿᱾᱿᳓‖‗†‡•‣․‥…‧‰‱′″‴‵‶‷‸※‼‽‾⁁⁂⁃⁇⁈⁉⁊⁋⁌⁍⁎⁏⁐⁑⁓⁕⁖⁗⁘⁙⁚⁛⁜⁝⁞⳹⳺⳻⳼⳾⳿⸀⸁⸆⸇⸈⸋⸎⸏⸐⸑⸒⸓⸔⸕⸖⸘⸙⸛⸞⸟⸪⸫⸬⸭⸮⸰⸱、。〃〽・꓾꓿꘍꘎꘏꙳꙾꛲꛳꛴꛵꛶꛷꡴꡵꡶꡷꣎꣏꣸꣹꣺꤮꤯꥟꧁꧂꧃꧄꧅꧆꧇꧈꧉꧊꧋꧌꧍꧞꧟꩜꩝꩞꩟꫞꫟꯫︐︑︒︓︔︕︖︙︰﹅﹆﹉﹊﹋﹌﹐﹑﹒﹔﹕﹖﹗﹟﹠﹡﹨﹪﹫！＂＃％＆＇＊，．／：；？＠＼｡､･𐄀𐄁𐎟𐏐𐡗𐤟𐤿𐩐𐩑𐩒𐩓𐩔𐩕𐩖𐩗𐩘𐩿𐬹𐬺𐬻𐬼𐬽𐬾𐬿𑂻𑂼𑂾𑂿𑃀𑃁𒑰𒑱𒑲𒑳"
closing_delimiters = ur"\.\,\;\!\?"

unicode_punctuation_categories = {
    # 'Pc': 'Connector', # not used in Docutils inline markup recognition
    'Pd': 'Dash',
    'Ps': 'Open',
    'Pe': 'Close',
    'Pi': 'Initial quote', # may behave like Ps or Pe depending on usage
    'Pf': 'Final quote', # may behave like Ps or Pe depending on usage
    'Po': 'Other'
    }
"""Unicode character categories for punctuation"""

@others

test()
#@+node:ekr.20130418080354.10721: *5* punctuation_samples
def punctuation_samples():

    """Docutils punctuation category sample strings.

    Return list of sample strings for the categories "Open", "Close",
    "Delimiters" and "Closing-Delimiters" used in the `inline markup
    recognition rules`_.
    """

    # Lists with characters in Unicode punctuation character categories
    cp_min = 160 # ASCII chars have special rules for backwards compatibility
    ucharlists = unicode_charlists(unicode_punctuation_categories, cp_min)

    # match opening/closing characters
    # --------------------------------
    # Rearange the lists to ensure matching characters at the same
    # index position.

    # low quotation marks are also used as closers (e.g. in Greek)
    # move them to category Pi:
    ucharlists['Ps'].remove(u'‚') # 201A  SINGLE LOW-9 QUOTATION MARK
    ucharlists['Ps'].remove(u'„') # 201E  DOUBLE LOW-9 QUOTATION MARK
    ucharlists['Pi'] += [u'‚', u'„']

    ucharlists['Pi'].remove(u'‛') # 201B  SINGLE HIGH-REVERSED-9 QUOTATION MARK
    ucharlists['Pi'].remove(u'‟') # 201F  DOUBLE HIGH-REVERSED-9 QUOTATION MARK
    ucharlists['Pf'] += [u'‛', u'‟']

    # 301F  LOW DOUBLE PRIME QUOTATION MARK misses the opening pendant:
    ucharlists['Ps'].insert(ucharlists['Pe'].index(u'\u301f'), u'\u301d')

    # print u''.join(ucharlists['Ps']).encode('utf8')
    # print u''.join(ucharlists['Pe']).encode('utf8')
    # print u''.join(ucharlists['Pi']).encode('utf8')
    # print u''.join(ucharlists['Pf']).encode('utf8')

    # The Docutils character categories
    # ---------------------------------
    #
    # The categorization of ASCII chars is non-standard to reduce both
    # false positives and need for escaping. (see `inline markup recognition
    # rules`_)

    # matching, allowed before markup
    openers = [re.escape('"\'(<[{')]
    for cat in ('Ps', 'Pi', 'Pf'):
        openers.extend(ucharlists[cat])

    # matching, allowed after markup
    closers = [re.escape('"\')>]}')]
    for cat in ('Pe', 'Pf', 'Pi'):
        closers.extend(ucharlists[cat])

    # non-matching, allowed on both sides
    delimiters = [re.escape('-/:')]
    for cat in ('Pd', 'Po'):
        delimiters.extend(ucharlists[cat])

    # non-matching, after markup
    closing_delimiters = [re.escape('.,;!?')]

    # # Test open/close matching:
    # for i in range(min(len(openers),len(closers))):
    #     print '%4d    %s    %s' % (i, openers[i].encode('utf8'),
    #                                closers[i].encode('utf8'))
    # dump(delimiters)
    
    return [u''.join(chars)
            for chars in (openers, closers, delimiters, closing_delimiters)]
#@+node:ekr.20130418080354.10720: *5* unicode_charlists
def unicode_charlists(categories, cp_min=0, cp_max=None):
    """Return dictionary of Unicode character lists.

    For each of the `catagories`, an item contains a list with all Unicode
    characters with `cp_min` <= code-point <= `cp_max` that belong to the
    category. (The default values check every code-point supported by Python.)
    """
    # Determine highest code point with one of the given categories
    # (may shorten the search time considerably if there are many
    # categories with not too high characters):
    if cp_max is None:
        cp_max = max(x for x in xrange(sys.maxunicode + 1)
                     if unicodedata.category(unichr(x)) in categories)
        # print cp_max # => 74867 for unicode_punctuation_categories
    charlists = {}
    for cat in categories:
        charlists[cat] = [unichr(x) for x in xrange(cp_min, cp_max+1)
                          if unicodedata.category(unichr(x)) == cat]
    return charlists
#@+node:ekr.20130418164851.10730: *5* compare
def compare(s1,s2):
    
    print(len(s1),len(s2))
    d1,d2 = {},{}
    for uc in s1:
        assert isinstance(uc,(str,unicode)),type(uc)
        n = ord(uc)
        d1[n] = uc
    for uc in s2:
        assert isinstance(uc,(str,unicode)),type(uc)
        n = ord(uc)
        d2[n] = uc
    nset = set()
    for n in d1.keys():
        nset.add(n)
    for n in d2.keys():
        nset.add(n)
    matches = 0
    for n in sorted(nset):
        uc1 = d1.get(n)
        uc2 = d2.get(n)
        if uc1 is None and uc2 is None:
            print('%5s hu??' % (n))
        elif uc1 is None:
            print('%5s' % (n),'missing1',uc2,unicodedata.name(uc2,'Unknown'))
        elif uc2 is None:
            pass # print('%5s' % (n),'missing2',uc1,unicodedata.name(uc1,'Unknown'))
        elif uc1 == uc2:
            # print('%5s' % (n),'match',uc1,unicodedata.name(uc1,'Unknown'))
            # print('%s, # %s' % (n,unicodedata.name(uc1,'Unknown').lower()))
            matches += 1
        else:
            print('%5s' % (n),uc1,unicodedata.name(uc1,'Unknown'),uc2,unicodedata.name(uc2,'Unknown'))
    print('matches: %s' % matches)
#@+node:ekr.20130418164851.10729: *5* dump
def dump(s):
    for uc in s:
        assert isinstance(uc,(str,unicode)),type(uc)
        if isinstance(uc,unicode):
            print('%5s' % (ord(uc)),uc,unicodedata.name(uc,'Unknown'))
#@+node:ekr.20130418080354.10722: *5* test
# The if __name__ == '__main__' part of puntuation_chars.py

def test():
    
    # (re) create and compare the samples:
    (o, c, d, cd) = punctuation_samples()
    if o != openers:
        print('- openers = ur"""%s"""' % openers.encode('utf8'))
        print('+ openers = ur"""%s"""' % o.encode('utf8'))
    if c != closers:
        print('- closers = ur"""%s"""' % closers.encode('utf8'))
        print('+ closers = ur"""%s"""' % c.encode('utf8'))
    if d != delimiters:
        print('- delimiters = ur"%s"' % delimiters.encode('utf8'))
        # dump(delimiters)
        print('+ delimiters = ur"%s"' % d.encode('utf8'))
        # dump(d)
        compare(delimiters,d)
    if cd != closing_delimiters:
        print('- closing_delimiters = ur"%s"' % closing_delimiters.encode('utf8'))
        print('+ closing_delimiters = ur"%s"' % cd.encode('utf8'))
#@+node:ekr.20130417081749.10495: *4* check syntax of all docutils files
g.cls()
import os
path = g.os_path_finalize_join(g.app.loadDir,'..','extensions','docutils')
if g.isPython3:
    exclude = ('punctuation2.py',)
else:
    exclude = ('punctuation3.py',)

for root, dirs, files in os.walk(path):
    for fn in files:
        if fn.endswith('.py'):
            fn = g.os_path_join(root,fn)
            if not g.shortFileName(fn) in exclude:
                s,e = g.readFileIntoString(fn)
                c.testManager.checkFileSyntax(fn,s,reraise=False,suppress=False)
print('done')
#@+node:ekr.20130418190319.10735: *3* delimiter contstants
import sys
import unicodedata
delim_list = [
    45, # hyphen-minus
    47, # solidus
    58, # colon
    92, # reverse solidus
    161, # inverted exclamation mark
    183, # middle dot
    191, # inverted question mark
    894, # greek question mark
    903, # greek ano teleia
    1370, # armenian apostrophe
    1371, # armenian emphasis mark
    1372, # armenian exclamation mark
    1373, # armenian comma
    1374, # armenian question mark
    1375, # armenian abbreviation mark
    1417, # armenian full stop
    1418, # armenian hyphen
    1470, # hebrew punctuation maqaf
    1472, # hebrew punctuation paseq
    1475, # hebrew punctuation sof pasuq
    1478, # hebrew punctuation nun hafukha
    1523, # hebrew punctuation geresh
    1524, # hebrew punctuation gershayim
    1545, # arabic-indic per mille sign
    1546, # arabic-indic per ten thousand sign
    1548, # arabic comma
    1549, # arabic date separator
    1563, # arabic semicolon
    1566, # arabic triple dot punctuation mark
    1567, # arabic question mark
    1642, # arabic percent sign
    1643, # arabic decimal separator
    1644, # arabic thousands separator
    1645, # arabic five pointed star
    1748, # arabic full stop
    1792, # syriac end of paragraph
    1793, # syriac supralinear full stop
    1794, # syriac sublinear full stop
    1795, # syriac supralinear colon
    1796, # syriac sublinear colon
    1797, # syriac horizontal colon
    1798, # syriac colon skewed left
    1799, # syriac colon skewed right
    1800, # syriac supralinear colon skewed left
    1801, # syriac sublinear colon skewed right
    1802, # syriac contraction
    1803, # syriac harklean obelus
    1804, # syriac harklean metobelus
    1805, # syriac harklean asteriscus
    2039, # nko symbol gbakurunen
    2040, # nko comma
    2041, # nko exclamation mark
    2096, # samaritan punctuation nequdaa
    2097, # samaritan punctuation afsaaq
    2098, # samaritan punctuation anged
    2099, # samaritan punctuation bau
    2100, # samaritan punctuation atmaau
    2101, # samaritan punctuation shiyyaalaa
    2102, # samaritan abbreviation mark
    2103, # samaritan punctuation melodic qitsa
    2104, # samaritan punctuation ziqaa
    2105, # samaritan punctuation qitsa
    2106, # samaritan punctuation zaef
    2107, # samaritan punctuation turu
    2108, # samaritan punctuation arkaanu
    2109, # samaritan punctuation sof mashfaat
    2110, # samaritan punctuation annaau
    2404, # devanagari danda
    2405, # devanagari double danda
    2416, # devanagari abbreviation sign
    3572, # sinhala punctuation kunddaliya
    3663, # thai character fongman
    3674, # thai character angkhankhu
    3675, # thai character khomut
    3844, # tibetan mark initial yig mgo mdun ma
    3845, # tibetan mark closing yig mgo sgab ma
    3846, # tibetan mark caret yig mgo phur shad ma
    3847, # tibetan mark yig mgo tsheg shad ma
    3848, # tibetan mark sbrul shad
    3849, # tibetan mark bskur yig mgo
    3850, # tibetan mark bka- shog yig mgo
    3851, # tibetan mark intersyllabic tsheg
    3852, # tibetan mark delimiter tsheg bstar
    3853, # tibetan mark shad
    3854, # tibetan mark nyis shad
    3855, # tibetan mark tsheg shad
    3856, # tibetan mark nyis tsheg shad
    3857, # tibetan mark rin chen spungs shad
    3858, # tibetan mark rgya gram shad
    3973, # tibetan mark paluta
    4048, # tibetan mark bska- shog gi mgo rgyan
    4049, # tibetan mark mnyam yig gi mgo rgyan
    4050, # tibetan mark nyis tsheg
    4051, # tibetan mark initial brda rnying yig mgo mdun ma
    4052, # tibetan mark closing brda rnying yig mgo sgab ma
    4170, # myanmar sign little section
    4171, # myanmar sign section
    4172, # myanmar symbol locative
    4173, # myanmar symbol completed
    4174, # myanmar symbol aforementioned
    4175, # myanmar symbol genitive
    4347, # georgian paragraph separator
    4961, # ethiopic wordspace
    4962, # ethiopic full stop
    4963, # ethiopic comma
    4964, # ethiopic semicolon
    4965, # ethiopic colon
    4966, # ethiopic preface colon
    4967, # ethiopic question mark
    4968, # ethiopic paragraph separator
    5120, # canadian syllabics hyphen
    5741, # canadian syllabics chi sign
    5742, # canadian syllabics full stop
    5867, # runic single punctuation
    5868, # runic multiple punctuation
    5869, # runic cross punctuation
    5941, # philippine single punctuation
    5942, # philippine double punctuation
    6100, # khmer sign khan
    6101, # khmer sign bariyoosan
    6102, # khmer sign camnuc pii kuuh
    6104, # khmer sign beyyal
    6105, # khmer sign phnaek muan
    6106, # khmer sign koomuut
    6144, # mongolian birga
    6145, # mongolian ellipsis
    6146, # mongolian comma
    6147, # mongolian full stop
    6148, # mongolian colon
    6149, # mongolian four dots
    6150, # mongolian todo soft hyphen
    6151, # mongolian sibe syllable boundary marker
    6152, # mongolian manchu comma
    6153, # mongolian manchu full stop
    6154, # mongolian nirugu
    6468, # limbu exclamation mark
    6469, # limbu question mark
    6622, # new tai lue sign lae
    6623, # new tai lue sign laev
    6686, # buginese pallawa
    6687, # buginese end of section
    6816, # tai tham sign wiang
    6817, # tai tham sign wiangwaak
    6818, # tai tham sign sawan
    6819, # tai tham sign keow
    6820, # tai tham sign hoy
    6821, # tai tham sign dokmai
    6822, # tai tham sign reversed rotated rana
    6824, # tai tham sign kaan
    6825, # tai tham sign kaankuu
    6826, # tai tham sign satkaan
    6827, # tai tham sign satkaankuu
    6828, # tai tham sign hang
    6829, # tai tham sign caang
    7002, # balinese panti
    7003, # balinese pamada
    7004, # balinese windu
    7005, # balinese carik pamungkah
    7006, # balinese carik siki
    7007, # balinese carik pareren
    7008, # balinese pameneng
    7227, # lepcha punctuation ta-rol
    7228, # lepcha punctuation nyet thyoom ta-rol
    7229, # lepcha punctuation cer-wa
    7230, # lepcha punctuation tshook cer-wa
    7231, # lepcha punctuation tshook
    7294, # ol chiki punctuation mucaad
    7295, # ol chiki punctuation double mucaad
    7379, # vedic sign nihshvasa
    8208, # hyphen
    8209, # non-breaking hyphen
    8210, # figure dash
    8211, # en dash
    8212, # em dash
    8213, # horizontal bar
    8214, # double vertical line
    8215, # double low line
    8224, # dagger
    8225, # double dagger
    8226, # bullet
    8227, # triangular bullet
    8228, # one dot leader
    8229, # two dot leader
    8230, # horizontal ellipsis
    8231, # hyphenation point
    8240, # per mille sign
    8241, # per ten thousand sign
    8242, # prime
    8243, # double prime
    8244, # triple prime
    8245, # reversed prime
    8246, # reversed double prime
    8247, # reversed triple prime
    8248, # caret
    8251, # reference mark
    8252, # double exclamation mark
    8253, # interrobang
    8254, # overline
    8257, # caret insertion point
    8258, # asterism
    8259, # hyphen bullet
    8263, # double question mark
    8264, # question exclamation mark
    8265, # exclamation question mark
    8266, # tironian sign et
    8267, # reversed pilcrow sign
    8268, # black leftwards bullet
    8269, # black rightwards bullet
    8270, # low asterisk
    8271, # reversed semicolon
    8272, # close up
    8273, # two asterisks aligned vertically
    8275, # swung dash
    8277, # flower punctuation mark
    8278, # three dot punctuation
    8279, # quadruple prime
    8280, # four dot punctuation
    8281, # five dot punctuation
    8282, # two dot punctuation
    8283, # four dot mark
    8284, # dotted cross
    8285, # tricolon
    8286, # vertical four dots
    11513, # coptic old nubian full stop
    11514, # coptic old nubian direct question mark
    11515, # coptic old nubian indirect question mark
    11516, # coptic old nubian verse divider
    11518, # coptic full stop
    11519, # coptic morphological divider
    11776, # right angle substitution marker
    11777, # right angle dotted substitution marker
    11782, # raised interpolation marker
    11783, # raised dotted interpolation marker
    11784, # dotted transposition marker
    11787, # raised square
    11790, # editorial coronis
    11791, # paragraphos
    11792, # forked paragraphos
    11793, # reversed forked paragraphos
    11794, # hypodiastole
    11795, # dotted obelos
    11796, # downwards ancora
    11797, # upwards ancora
    11798, # dotted right-pointing angle
    11799, # double oblique hyphen
    11800, # inverted interrobang
    11801, # palm branch
    11802, # hyphen with diaeresis
    11803, # tilde with ring above
    11806, # tilde with dot above
    11807, # tilde with dot below
    11818, # two dots over one dot punctuation
    11819, # one dot over two dots punctuation
    11820, # squared four dot punctuation
    11821, # five dot mark
    11822, # reversed question mark
    11824, # ring point
    11825, # word separator middle dot
    12289, # ideographic comma
    12290, # ideographic full stop
    12291, # ditto mark
    12316, # wave dash
    12336, # wavy dash
    12349, # part alternation mark
    12448, # katakana-hiragana double hyphen
    12539, # katakana middle dot
    42238, # lisu punctuation comma
    42239, # lisu punctuation full stop
    42509, # vai comma
    42510, # vai full stop
    42511, # vai question mark
    42611, # slavonic asterisk
    42622, # cyrillic kavyka
    42738, # bamum njaemli
    42739, # bamum full stop
    42740, # bamum colon
    42741, # bamum comma
    42742, # bamum semicolon
    42743, # bamum question mark
    43124, # phags-pa single head mark
    43125, # phags-pa double head mark
    43126, # phags-pa mark shad
    43127, # phags-pa mark double shad
    43214, # saurashtra danda
    43215, # saurashtra double danda
    43256, # devanagari sign pushpika
    43257, # devanagari gap filler
    43258, # devanagari caret
    43310, # kayah li sign cwi
    43311, # kayah li sign shya
    43359, # rejang section mark
    43457, # javanese left rerenggan
    43458, # javanese right rerenggan
    43459, # javanese pada andap
    43460, # javanese pada madya
    43461, # javanese pada luhur
    43462, # javanese pada windu
    43463, # javanese pada pangkat
    43464, # javanese pada lingsa
    43465, # javanese pada lungsi
    43466, # javanese pada adeg
    43467, # javanese pada adeg adeg
    43468, # javanese pada piseleh
    43469, # javanese turned pada piseleh
    43486, # javanese pada tirta tumetes
    43487, # javanese pada isen-isen
    43612, # cham punctuation spiral
    43613, # cham punctuation danda
    43614, # cham punctuation double danda
    43615, # cham punctuation triple danda
    43742, # tai viet symbol ho hoi
    43743, # tai viet symbol koi koi
    44011, # meetei mayek cheikhei
    65040, # presentation form for vertical comma
    65041, # presentation form for vertical ideographic comma
    65042, # presentation form for vertical ideographic full stop
    65043, # presentation form for vertical colon
    65044, # presentation form for vertical semicolon
    65045, # presentation form for vertical exclamation mark
    65046, # presentation form for vertical question mark
    65049, # presentation form for vertical horizontal ellipsis
    65072, # presentation form for vertical two dot leader
    65073, # presentation form for vertical em dash
    65074, # presentation form for vertical en dash
    65093, # sesame dot
    65094, # white sesame dot
    65097, # dashed overline
    65098, # centreline overline
    65099, # wavy overline
    65100, # double wavy overline
    65104, # small comma
    65105, # small ideographic comma
    65106, # small full stop
    65108, # small semicolon
    65109, # small colon
    65110, # small question mark
    65111, # small exclamation mark
    65112, # small em dash
    65119, # small number sign
    65120, # small ampersand
    65121, # small asterisk
    65123, # small hyphen-minus
    65128, # small reverse solidus
    65130, # small percent sign
    65131, # small commercial at
    65281, # fullwidth exclamation mark
    65282, # fullwidth quotation mark
    65283, # fullwidth number sign
    65285, # fullwidth percent sign
    65286, # fullwidth ampersand
    65287, # fullwidth apostrophe
    65290, # fullwidth asterisk
    65292, # fullwidth comma
    65293, # fullwidth hyphen-minus
    65294, # fullwidth full stop
    65295, # fullwidth solidus
    65306, # fullwidth colon
    65307, # fullwidth semicolon
    65311, # fullwidth question mark
    65312, # fullwidth commercial at
    65340, # fullwidth reverse solidus
    65377, # halfwidth ideographic full stop
    65380, # halfwidth ideographic comma
    65381, # halfwidth katakana middle dot
]
ch = unichr if sys.version_info < (3,) else chr
delimiters = ''.join([ch(i) for i in delim_list])
comments = '\n'.join(['%s, # %s %s' % (i,ch(i),unicodedata.name(ch(i)).lower()) for i in delim_list])
g.es(comments)

#@+node:ekr.20130418172116.10733: *3* print font test
# -*- coding: utf8 -*-

s = '''
ASCII     abcde xyz
German    äöü ÄÖÜ ß
Polish    ąęźżńł
Russian   абвгдеж эюя
CJK       你好
'''

print(s)
#@+node:ekr.20130417081749.10493: *3* docutils porting notes
@language rest
#@+node:ekr.20130417081749.10494: *4* Syntax
- Created docutils/punctuation2.py and punctuation3.py to handle ur'string' contstants.
  ur'string' is valid only in Python 2.x.
  
- The A-fixers should suffice to fix all other problems.
#@+node:ekr.20130417081749.10496: *3* docutils test imports
docutils = g.importExtension('docutils',pluginName='leoRst.py',verbose=True)
print(docutils)
from docutils import parsers
print(parsers)
from docutils.parsers import rst
print(rst)
import docutils.parsers.rst
print(docutils.parsers.rst)
from docutils.parsers.rst import directives
#@+node:ekr.20130417081749.10497: *3* Old punctuation files
#@+node:ekr.20130417081749.10498: *4* @@edit utils\punctuation2.py
@language python
#!/usr/bin/env python
# -*- coding: utf8 -*-
# punctuation2.py: Python 2.x syntax.
# This file by Edward K. Ream.

# :Copyright: © 2011 Günter Milde.
# :License: Released under the terms of the `2-Clause BSD license`_, in short:
#
#    Copying and distribution of this file, with or without modification,
#    are permitted in any medium without royalty provided the copyright
#    notice and this notice are preserved.
#    This file is offered as-is, without any warranty.
#
# .. _2-Clause BSD license: http://www.spdx.org/licenses/BSD-2-Clause

# fom punctuation_chars.py.

openers = ur"""\"\'\(\<\[\{༺༼᚛⁅⁽₍〈❨❪❬❮❰❲❴⟅⟦⟨⟪⟬⟮⦃⦅⦇⦉⦋⦍⦏⦑⦓⦕⦗⧘⧚⧼⸢⸤⸦⸨〈《「『【〔〖〘〚〝〝﴾︗︵︷︹︻︽︿﹁﹃﹇﹙﹛﹝（［｛｟｢«‘“‹⸂⸄⸉⸌⸜⸠‚„»’”›⸃⸅⸊⸍⸝⸡‛‟"""
closers = ur"""\"\'\)\>\]\}༻༽᚜⁆⁾₎〉❩❫❭❯❱❳❵⟆⟧⟩⟫⟭⟯⦄⦆⦈⦊⦌⦎⦐⦒⦔⦖⦘⧙⧛⧽⸣⸥⸧⸩〉》」』】〕〗〙〛〞〟﴿︘︶︸︺︼︾﹀﹂﹄﹈﹚﹜﹞）］｝｠｣»’”›⸃⸅⸊⸍⸝⸡‛‟«‘“‹⸂⸄⸉⸌⸜⸠‚„"""
delimiters = ur"\-\/\:֊־᐀᠆‐‑‒–—―⸗⸚〜〰゠︱︲﹘﹣－¡·¿;·՚՛՜՝՞՟։׀׃׆׳״؉؊،؍؛؞؟٪٫٬٭۔܀܁܂܃܄܅܆܇܈܉܊܋܌܍߷߸߹࠰࠱࠲࠳࠴࠵࠶࠷࠸࠹࠺࠻࠼࠽࠾।॥॰෴๏๚๛༄༅༆༇༈༉༊་༌།༎༏༐༑༒྅࿐࿑࿒࿓࿔၊။၌၍၎၏჻፡።፣፤፥፦፧፨᙭᙮᛫᛬᛭᜵᜶។៕៖៘៙៚᠀᠁᠂᠃᠄᠅᠇᠈᠉᠊᥄᥅᧞᧟᨞᨟᪠᪡᪢᪣᪤᪥᪦᪨᪩᪪᪫᪬᪭᭚᭛᭜᭝᭞᭟᭠᰻᰼᰽᰾᰿᱾᱿᳓‖‗†‡•‣․‥…‧‰‱′″‴‵‶‷‸※‼‽‾⁁⁂⁃⁇⁈⁉⁊⁋⁌⁍⁎⁏⁐⁑⁓⁕⁖⁗⁘⁙⁚⁛⁜⁝⁞⳹⳺⳻⳼⳾⳿⸀⸁⸆⸇⸈⸋⸎⸏⸐⸑⸒⸓⸔⸕⸖⸘⸙⸛⸞⸟⸪⸫⸬⸭⸮⸰⸱、。〃〽・꓾꓿꘍꘎꘏꙳꙾꛲꛳꛴꛵꛶꛷꡴꡵꡶꡷꣎꣏꣸꣹꣺꤮꤯꥟꧁꧂꧃꧄꧅꧆꧇꧈꧉꧊꧋꧌꧍꧞꧟꩜꩝꩞꩟꫞꫟꯫︐︑︒︓︔︕︖︙︰﹅﹆﹉﹊﹋﹌﹐﹑﹒﹔﹕﹖﹗﹟﹠﹡﹨﹪﹫！＂＃％＆＇＊，．／：；？＠＼｡､･𐄀𐄁𐎟𐏐𐡗𐤟𐤿𐩐𐩑𐩒𐩓𐩔𐩕𐩖𐩗𐩘𐩿𐬹𐬺𐬻𐬼𐬽𐬾𐬿𑂻𑂼𑂾𑂿𑃀𑃁𒑰𒑱𒑲𒑳"
closing_delimiters = ur"\.\,\;\!\?"

# From manpage.py.

replace_pairs = [
    (u'-', ur'\-'),
    (u'\'', ur'\(aq'),
    (u'´', ur'\''),
    (u'`', ur'\(ga'),
]

# From writers/latex2e/__init__.py

literal_double_quote = ur'\dq{}'
italian_literal_double_quote = ur'{\char`\"}'

table_reflectbox = ur'\reflectbox{/}'
table_textbar = ur'\textbar{}'
table_textless = ur'\textless{}'
table_textgreater = ur'\textgreater{}'
table_tilde = ur'~'

special_chars = {
    ord('#'): ur'\#',
    ord('%'): ur'\%',
    ord('\\'): ur'\\',
}

special = {
    ord('#'): ur'\#',
    ord('$'): ur'\$',
    ord('%'): ur'\%',
    ord('&'): ur'\&',
    ord('~'): ur'\textasciitilde{}',
    ord('_'): ur'\_',
    ord('^'): ur'\textasciicircum{}',
    ord('\\'): ur'\textbackslash{}',
    ord('{'): ur'\{',
    ord('}'): ur'\}',
    # Square brackets are ordinary chars and cannot be escaped with '\',
    # so we put them in a group '{[}'. (Alternative: ensure that all
    # macros with optional arguments are terminated with {} and text
    # inside any optional argument is put in a group ``[{text}]``).
    # Commands with optional args inside an optional arg must be put in a
    # group, e.g. ``\item[{\hyperref[label]{text}}]``.
    ord('['): ur'{[}',
    ord(']'): ur'{]}',
    # the soft hyphen is unknown in 8-bit text and not properly handled by XeTeX
    0x00AD: ur'\-', # SOFT HYPHEN
}

# Unicode chars that are not recognized by LaTeX's utf8 encoding
unsupported_unicode = {
    0x00A0: ur'~', # NO-BREAK SPACE
    # TODO: ensure white space also at the beginning of a line?
    # 0x00A0: ur'\leavevmode\nobreak\vadjust{}~'
    0x2008: ur'\,', # PUNCTUATION SPACE   
    0x2011: ur'\hbox{-}', # NON-BREAKING HYPHEN
    0x202F: ur'\,', # NARROW NO-BREAK SPACE
    0x21d4: ur'$\Leftrightarrow$',
    # Docutils footnote symbols:
    0x2660: ur'$\spadesuit$',
    0x2663: ur'$\clubsuit$',
}

# Unicode chars that are recognized by LaTeX's utf8 encoding
utf8_supported_unicode = {
    0x00AB: ur'\guillemotleft', # LEFT-POINTING DOUBLE ANGLE QUOTATION MARK
    0x00bb: ur'\guillemotright', # RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
    0x200C: ur'\textcompwordmark', # ZERO WIDTH NON-JOINER
    0x2013: ur'\textendash{}',
    0x2014: ur'\textemdash{}',
    0x2018: ur'\textquoteleft{}',
    0x2019: ur'\textquoteright{}',
    0x201A: ur'\quotesinglbase{}', # SINGLE LOW-9 QUOTATION MARK
    0x201C: ur'\textquotedblleft{}',
    0x201D: ur'\textquotedblright{}',
    0x201E: ur'\quotedblbase{}', # DOUBLE LOW-9 QUOTATION MARK
    0x2030: ur'\textperthousand{}',   # PER MILLE SIGN
    0x2031: ur'\textpertenthousand{}', # PER TEN THOUSAND SIGN
    0x2039: ur'\guilsinglleft{}',
    0x203A: ur'\guilsinglright{}',
    0x2423: ur'\textvisiblespace{}',  # OPEN BOX
    0x2020: ur'\dag{}',
    0x2021: ur'\ddag{}',
    0x2026: ur'\dots{}',
    0x2122: ur'\texttrademark{}',
}

# recognized with 'utf8', if textcomp is loaded
textcomp = {
    # Latin-1 Supplement
    0x00a2: ur'\textcent{}',          # ¢ CENT SIGN
    0x00a4: ur'\textcurrency{}',      # ¤ CURRENCY SYMBOL
    0x00a5: ur'\textyen{}',           # ¥ YEN SIGN
    0x00a6: ur'\textbrokenbar{}',     # ¦ BROKEN BAR
    0x00a7: ur'\textsection{}',       # § SECTION SIGN
    0x00a8: ur'\textasciidieresis{}', # ¨ DIAERESIS
    0x00a9: ur'\textcopyright{}',     # © COPYRIGHT SIGN
    0x00aa: ur'\textordfeminine{}',   # ª FEMININE ORDINAL INDICATOR
    0x00ac: ur'\textlnot{}',          # ¬ NOT SIGN
    0x00ae: ur'\textregistered{}',    # ® REGISTERED SIGN
    0x00af: ur'\textasciimacron{}',   # ¯ MACRON
    0x00b0: ur'\textdegree{}',        # ° DEGREE SIGN
    0x00b1: ur'\textpm{}',            # ± PLUS-MINUS SIGN
    0x00b2: ur'\texttwosuperior{}',   # ² SUPERSCRIPT TWO
    0x00b3: ur'\textthreesuperior{}', # ³ SUPERSCRIPT THREE
    0x00b4: ur'\textasciiacute{}',    # ´ ACUTE ACCENT
    0x00b5: ur'\textmu{}',            # µ MICRO SIGN
    0x00b6: ur'\textparagraph{}',     # ¶ PILCROW SIGN # not equal to \textpilcrow
    0x00b9: ur'\textonesuperior{}',   # ¹ SUPERSCRIPT ONE
    0x00ba: ur'\textordmasculine{}',  # º MASCULINE ORDINAL INDICATOR
    0x00bc: ur'\textonequarter{}',    # 1/4 FRACTION
    0x00bd: ur'\textonehalf{}',       # 1/2 FRACTION
    0x00be: ur'\textthreequarters{}', # 3/4 FRACTION
    0x00d7: ur'\texttimes{}',         # × MULTIPLICATION SIGN
    0x00f7: ur'\textdiv{}',           # ÷ DIVISION SIGN
    #
    0x0192: ur'\textflorin{}',        # LATIN SMALL LETTER F WITH HOOK
    0x02b9: ur'\textasciiacute{}',    # MODIFIER LETTER PRIME
    0x02ba: ur'\textacutedbl{}',      # MODIFIER LETTER DOUBLE PRIME
    0x2016: ur'\textbardbl{}',        # DOUBLE VERTICAL LINE
    0x2022: ur'\textbullet{}',        # BULLET
    0x2032: ur'\textasciiacute{}',    # PRIME
    0x2033: ur'\textacutedbl{}',      # DOUBLE PRIME
    0x2035: ur'\textasciigrave{}',    # REVERSED PRIME
    0x2036: ur'\textgravedbl{}',      # REVERSED DOUBLE PRIME
    0x203b: ur'\textreferencemark{}', # REFERENCE MARK
    0x203d: ur'\textinterrobang{}',   # INTERROBANG
    0x2044: ur'\textfractionsolidus{}', # FRACTION SLASH
    0x2045: ur'\textlquill{}',        # LEFT SQUARE BRACKET WITH QUILL
    0x2046: ur'\textrquill{}',        # RIGHT SQUARE BRACKET WITH QUILL
    0x2052: ur'\textdiscount{}',      # COMMERCIAL MINUS SIGN
    0x20a1: ur'\textcolonmonetary{}', # COLON SIGN
    0x20a3: ur'\textfrenchfranc{}',   # FRENCH FRANC SIGN
    0x20a4: ur'\textlira{}',          # LIRA SIGN
    0x20a6: ur'\textnaira{}',         # NAIRA SIGN
    0x20a9: ur'\textwon{}',           # WON SIGN
    0x20ab: ur'\textdong{}',          # DONG SIGN
    0x20ac: ur'\texteuro{}',          # EURO SIGN
    0x20b1: ur'\textpeso{}',          # PESO SIGN
    0x20b2: ur'\textguarani{}',       # GUARANI SIGN
    0x2103: ur'\textcelsius{}',       # DEGREE CELSIUS
    0x2116: ur'\textnumero{}',        # NUMERO SIGN
    0x2117: ur'\textcircledP{}',      # SOUND RECORDING COYRIGHT
    0x211e: ur'\textrecipe{}',        # PRESCRIPTION TAKE
    0x2120: ur'\textservicemark{}',   # SERVICE MARK
    0x2122: ur'\texttrademark{}',     # TRADE MARK SIGN
    0x2126: ur'\textohm{}',           # OHM SIGN
    0x2127: ur'\textmho{}',           # INVERTED OHM SIGN
    0x212e: ur'\textestimated{}',     # ESTIMATED SYMBOL
    0x2190: ur'\textleftarrow{}',     # LEFTWARDS ARROW
    0x2191: ur'\textuparrow{}',       # UPWARDS ARROW
    0x2192: ur'\textrightarrow{}',    # RIGHTWARDS ARROW
    0x2193: ur'\textdownarrow{}',     # DOWNWARDS ARROW
    0x2212: ur'\textminus{}',         # MINUS SIGN
    0x2217: ur'\textasteriskcentered{}', # ASTERISK OPERATOR
    0x221a: ur'\textsurd{}',          # SQUARE ROOT
    0x2422: ur'\textblank{}',         # BLANK SYMBOL
    0x25e6: ur'\textopenbullet{}',    # WHITE BULLET
    0x25ef: ur'\textbigcircle{}',     # LARGE CIRCLE
    0x266a: ur'\textmusicalnote{}',   # EIGHTH NOTE
    0x26ad: ur'\textmarried{}',       # MARRIAGE SYMBOL
    0x26ae: ur'\textdivorced{}',      # DIVORCE SYMBOL
    0x27e8: ur'\textlangle{}',        # MATHEMATICAL LEFT ANGLE BRACKET
    0x27e9: ur'\textrangle{}',        # MATHEMATICAL RIGHT ANGLE BRACKET
}

# Unicode chars that require a feature/package to render
pifont = {
    0x2665: ur'\ding{170}',     # black heartsuit
    0x2666: ur'\ding{169}',     # black diamondsuit
    0x2713: ur'\ding{51}',      # check mark
    0x2717: ur'\ding{55}',      # check mark
}
#@+node:ekr.20130417081749.10499: *4* @@edit utils\punctuation3.py
@language python
#!/usr/bin/env python
# -*- coding: utf8 -*-
# punctuation3.py: Python 3.x syntax.
# This file by Edward K. Ream

# :Copyright: © 2011 Günter Milde.
# :License: Released under the terms of the `2-Clause BSD license`_, in short:
#
#    Copying and distribution of this file, with or without modification,
#    are permitted in any medium without royalty provided the copyright
#    notice and this notice are preserved.
#    This file is offered as-is, without any warranty.
#
# .. _2-Clause BSD license: http://www.spdx.org/licenses/BSD-2-Clause

# Extracted from punctuation_chars.py by Edward K. Ream.

openers = r"""\"\'\(\<\[\{༺༼᚛⁅⁽₍〈❨❪❬❮❰❲❴⟅⟦⟨⟪⟬⟮⦃⦅⦇⦉⦋⦍⦏⦑⦓⦕⦗⧘⧚⧼⸢⸤⸦⸨〈《「『【〔〖〘〚〝〝﴾︗︵︷︹︻︽︿﹁﹃﹇﹙﹛﹝（［｛｟｢«‘“‹⸂⸄⸉⸌⸜⸠‚„»’”›⸃⸅⸊⸍⸝⸡‛‟"""
closers = r"""\"\'\)\>\]\}༻༽᚜⁆⁾₎〉❩❫❭❯❱❳❵⟆⟧⟩⟫⟭⟯⦄⦆⦈⦊⦌⦎⦐⦒⦔⦖⦘⧙⧛⧽⸣⸥⸧⸩〉》」』】〕〗〙〛〞〟﴿︘︶︸︺︼︾﹀﹂﹄﹈﹚﹜﹞）］｝｠｣»’”›⸃⸅⸊⸍⸝⸡‛‟«‘“‹⸂⸄⸉⸌⸜⸠‚„"""
delimiters = r"\-\/\:֊־᐀᠆‐‑‒–—―⸗⸚〜〰゠︱︲﹘﹣－¡·¿;·՚՛՜՝՞՟։׀׃׆׳״؉؊،؍؛؞؟٪٫٬٭۔܀܁܂܃܄܅܆܇܈܉܊܋܌܍߷߸߹࠰࠱࠲࠳࠴࠵࠶࠷࠸࠹࠺࠻࠼࠽࠾।॥॰෴๏๚๛༄༅༆༇༈༉༊་༌།༎༏༐༑༒྅࿐࿑࿒࿓࿔၊။၌၍၎၏჻፡።፣፤፥፦፧፨᙭᙮᛫᛬᛭᜵᜶។៕៖៘៙៚᠀᠁᠂᠃᠄᠅᠇᠈᠉᠊᥄᥅᧞᧟᨞᨟᪠᪡᪢᪣᪤᪥᪦᪨᪩᪪᪫᪬᪭᭚᭛᭜᭝᭞᭟᭠᰻᰼᰽᰾᰿᱾᱿᳓‖‗†‡•‣․‥…‧‰‱′″‴‵‶‷‸※‼‽‾⁁⁂⁃⁇⁈⁉⁊⁋⁌⁍⁎⁏⁐⁑⁓⁕⁖⁗⁘⁙⁚⁛⁜⁝⁞⳹⳺⳻⳼⳾⳿⸀⸁⸆⸇⸈⸋⸎⸏⸐⸑⸒⸓⸔⸕⸖⸘⸙⸛⸞⸟⸪⸫⸬⸭⸮⸰⸱、。〃〽・꓾꓿꘍꘎꘏꙳꙾꛲꛳꛴꛵꛶꛷꡴꡵꡶꡷꣎꣏꣸꣹꣺꤮꤯꥟꧁꧂꧃꧄꧅꧆꧇꧈꧉꧊꧋꧌꧍꧞꧟꩜꩝꩞꩟꫞꫟꯫︐︑︒︓︔︕︖︙︰﹅﹆﹉﹊﹋﹌﹐﹑﹒﹔﹕﹖﹗﹟﹠﹡﹨﹪﹫！＂＃％＆＇＊，．／：；？＠＼｡､･𐄀𐄁𐎟𐏐𐡗𐤟𐤿𐩐𐩑𐩒𐩓𐩔𐩕𐩖𐩗𐩘𐩿𐬹𐬺𐬻𐬼𐬽𐬾𐬿𑂻𑂼𑂾𑂿𑃀𑃁𒑰𒑱𒑲𒑳"
closing_delimiters = r"\.\,\;\!\?"

# From manpage.py.

replace_pairs = [
    (u'-', r'\-'),
    (u'\'', r'\(aq'),
    (u'´', r'\''),
    (u'`', r'\(ga'),
]

# From writers/latex2e/__init__.py

literal_double_quote = r'\dq{}'
italian_literal_double_quote = r'{\char`\"}'

table_reflectbox = r'\reflectbox{/}'
table_textbar = r'\textbar{}'
table_textless = r'\textless{}'
table_textgreater = r'\textgreater{}'
table_tilde = r'~'

special_chars = {
    ord('#'): r'\#',
    ord('%'): r'\%',
    ord('\\'): r'\\',
}

special = {
    ord('#'): r'\#',
    ord('$'): r'\$',
    ord('%'): r'\%',
    ord('&'): r'\&',
    ord('~'): r'\textasciitilde{}',
    ord('_'): r'\_',
    ord('^'): r'\textasciicircum{}',
    ord('\\'): r'\textbackslash{}',
    ord('{'): r'\{',
    ord('}'): r'\}',
    # Square brackets are ordinary chars and cannot be escaped with '\',
    # so we put them in a group '{[}'. (Alternative: ensure that all
    # macros with optional arguments are terminated with {} and text
    # inside any optional argument is put in a group ``[{text}]``).
    # Commands with optional args inside an optional arg must be put in a
    # group, e.g. ``\item[{\hyperref[label]{text}}]``.
    ord('['): r'{[}',
    ord(']'): r'{]}',
    # the soft hyphen is unknown in 8-bit text and not properly handled by XeTeX
    0x00AD: r'\-', # SOFT HYPHEN
}

# Unicode chars that are not recognized by LaTeX's utf8 encoding
unsupported_unicode = {
    0x00A0: r'~', # NO-BREAK SPACE
    # TODO: ensure white space also at the beginning of a line?
    # 0x00A0: r'\leavevmode\nobreak\vadjust{}~'
    0x2008: r'\,', # PUNCTUATION SPACE   
    0x2011: r'\hbox{-}', # NON-BREAKING HYPHEN
    0x202F: r'\,', # NARROW NO-BREAK SPACE
    0x21d4: r'$\Leftrightarrow$',
    # Docutils footnote symbols:
    0x2660: r'$\spadesuit$',
    0x2663: r'$\clubsuit$',
}

# Unicode chars that are recognized by LaTeX's utf8 encoding
utf8_supported_unicode = {
    0x00AB: r'\guillemotleft', # LEFT-POINTING DOUBLE ANGLE QUOTATION MARK
    0x00bb: r'\guillemotright', # RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
    0x200C: r'\textcompwordmark', # ZERO WIDTH NON-JOINER
    0x2013: r'\textendash{}',
    0x2014: r'\textemdash{}',
    0x2018: r'\textquoteleft{}',
    0x2019: r'\textquoteright{}',
    0x201A: r'\quotesinglbase{}', # SINGLE LOW-9 QUOTATION MARK
    0x201C: r'\textquotedblleft{}',
    0x201D: r'\textquotedblright{}',
    0x201E: r'\quotedblbase{}', # DOUBLE LOW-9 QUOTATION MARK
    0x2030: r'\textperthousand{}',   # PER MILLE SIGN
    0x2031: r'\textpertenthousand{}', # PER TEN THOUSAND SIGN
    0x2039: r'\guilsinglleft{}',
    0x203A: r'\guilsinglright{}',
    0x2423: r'\textvisiblespace{}',  # OPEN BOX
    0x2020: r'\dag{}',
    0x2021: r'\ddag{}',
    0x2026: r'\dots{}',
    0x2122: r'\texttrademark{}',
}

# recognized with 'utf8', if textcomp is loaded
textcomp = {
    # Latin-1 Supplement
    0x00a2: r'\textcent{}',          # ¢ CENT SIGN
    0x00a4: r'\textcurrency{}',      # ¤ CURRENCY SYMBOL
    0x00a5: r'\textyen{}',           # ¥ YEN SIGN
    0x00a6: r'\textbrokenbar{}',     # ¦ BROKEN BAR
    0x00a7: r'\textsection{}',       # § SECTION SIGN
    0x00a8: r'\textasciidieresis{}', # ¨ DIAERESIS
    0x00a9: r'\textcopyright{}',     # © COPYRIGHT SIGN
    0x00aa: r'\textordfeminine{}',   # ª FEMININE ORDINAL INDICATOR
    0x00ac: r'\textlnot{}',          # ¬ NOT SIGN
    0x00ae: r'\textregistered{}',    # ® REGISTERED SIGN
    0x00af: r'\textasciimacron{}',   # ¯ MACRON
    0x00b0: r'\textdegree{}',        # ° DEGREE SIGN
    0x00b1: r'\textpm{}',            # ± PLUS-MINUS SIGN
    0x00b2: r'\texttwosuperior{}',   # ² SUPERSCRIPT TWO
    0x00b3: r'\textthreesuperior{}', # ³ SUPERSCRIPT THREE
    0x00b4: r'\textasciiacute{}',    # ´ ACUTE ACCENT
    0x00b5: r'\textmu{}',            # µ MICRO SIGN
    0x00b6: r'\textparagraph{}',     # ¶ PILCROW SIGN # not equal to \textpilcrow
    0x00b9: r'\textonesuperior{}',   # ¹ SUPERSCRIPT ONE
    0x00ba: r'\textordmasculine{}',  # º MASCULINE ORDINAL INDICATOR
    0x00bc: r'\textonequarter{}',    # 1/4 FRACTION
    0x00bd: r'\textonehalf{}',       # 1/2 FRACTION
    0x00be: r'\textthreequarters{}', # 3/4 FRACTION
    0x00d7: r'\texttimes{}',         # × MULTIPLICATION SIGN
    0x00f7: r'\textdiv{}',           # ÷ DIVISION SIGN
    #
    0x0192: r'\textflorin{}',        # LATIN SMALL LETTER F WITH HOOK
    0x02b9: r'\textasciiacute{}',    # MODIFIER LETTER PRIME
    0x02ba: r'\textacutedbl{}',      # MODIFIER LETTER DOUBLE PRIME
    0x2016: r'\textbardbl{}',        # DOUBLE VERTICAL LINE
    0x2022: r'\textbullet{}',        # BULLET
    0x2032: r'\textasciiacute{}',    # PRIME
    0x2033: r'\textacutedbl{}',      # DOUBLE PRIME
    0x2035: r'\textasciigrave{}',    # REVERSED PRIME
    0x2036: r'\textgravedbl{}',      # REVERSED DOUBLE PRIME
    0x203b: r'\textreferencemark{}', # REFERENCE MARK
    0x203d: r'\textinterrobang{}',   # INTERROBANG
    0x2044: r'\textfractionsolidus{}', # FRACTION SLASH
    0x2045: r'\textlquill{}',        # LEFT SQUARE BRACKET WITH QUILL
    0x2046: r'\textrquill{}',        # RIGHT SQUARE BRACKET WITH QUILL
    0x2052: r'\textdiscount{}',      # COMMERCIAL MINUS SIGN
    0x20a1: r'\textcolonmonetary{}', # COLON SIGN
    0x20a3: r'\textfrenchfranc{}',   # FRENCH FRANC SIGN
    0x20a4: r'\textlira{}',          # LIRA SIGN
    0x20a6: r'\textnaira{}',         # NAIRA SIGN
    0x20a9: r'\textwon{}',           # WON SIGN
    0x20ab: r'\textdong{}',          # DONG SIGN
    0x20ac: r'\texteuro{}',          # EURO SIGN
    0x20b1: r'\textpeso{}',          # PESO SIGN
    0x20b2: r'\textguarani{}',       # GUARANI SIGN
    0x2103: r'\textcelsius{}',       # DEGREE CELSIUS
    0x2116: r'\textnumero{}',        # NUMERO SIGN
    0x2117: r'\textcircledP{}',      # SOUND RECORDING COYRIGHT
    0x211e: r'\textrecipe{}',        # PRESCRIPTION TAKE
    0x2120: r'\textservicemark{}',   # SERVICE MARK
    0x2122: r'\texttrademark{}',     # TRADE MARK SIGN
    0x2126: r'\textohm{}',           # OHM SIGN
    0x2127: r'\textmho{}',           # INVERTED OHM SIGN
    0x212e: r'\textestimated{}',     # ESTIMATED SYMBOL
    0x2190: r'\textleftarrow{}',     # LEFTWARDS ARROW
    0x2191: r'\textuparrow{}',       # UPWARDS ARROW
    0x2192: r'\textrightarrow{}',    # RIGHTWARDS ARROW
    0x2193: r'\textdownarrow{}',     # DOWNWARDS ARROW
    0x2212: r'\textminus{}',         # MINUS SIGN
    0x2217: r'\textasteriskcentered{}', # ASTERISK OPERATOR
    0x221a: r'\textsurd{}',          # SQUARE ROOT
    0x2422: r'\textblank{}',         # BLANK SYMBOL
    0x25e6: r'\textopenbullet{}',    # WHITE BULLET
    0x25ef: r'\textbigcircle{}',     # LARGE CIRCLE
    0x266a: r'\textmusicalnote{}',   # EIGHTH NOTE
    0x26ad: r'\textmarried{}',       # MARRIAGE SYMBOL
    0x26ae: r'\textdivorced{}',      # DIVORCE SYMBOL
    0x27e8: r'\textlangle{}',        # MATHEMATICAL LEFT ANGLE BRACKET
    0x27e9: r'\textrangle{}',        # MATHEMATICAL RIGHT ANGLE BRACKET
}

# Unicode chars that require a feature/package to render
pifont = {
    0x2665: r'\ding{170}',     # black heartsuit
    0x2666: r'\ding{169}',     # black diamondsuit
    0x2713: r'\ding{51}',      # check mark
    0x2717: r'\ding{55}',      # check mark
}
#@+node:ekr.20130417081749.10500: *4* @@edit utils\punctuation_chars.py
@language python
#!/usr/bin/env python
# -*- coding: utf8 -*-
# :Copyright: © 2011 Günter Milde.
# :License: Released under the terms of the `2-Clause BSD license`_, in short:
#
#    Copying and distribution of this file, with or without modification,
#    are permitted in any medium without royalty provided the copyright
#    notice and this notice are preserved.
#    This file is offered as-is, without any warranty.
#
# .. _2-Clause BSD license: http://www.spdx.org/licenses/BSD-2-Clause

# :Id: $Id: punctuation_chars.py 7463 2012-06-22 19:49:51Z milde $

import sys, re
import unicodedata

# punctuation characters around inline markup
# ===========================================
#
# This module provides the lists of characters for the implementation of
# the `inline markup recognition rules`_ in the reStructuredText parser
# (states.py)
#
# .. _inline markup recognition rules:
#     ../../../docs/ref/rst/restructuredtext.html#inline-markup

# Docutils punctuation category sample strings
# --------------------------------------------
#
# The sample strings are generated by punctuation_samples() and put here
# literal to avoid the time-consuming generation with every Docutils
# run. Running this file as a standalone module checks the definitions below
# against a re-calculation.

### 2to3: import helper files to handle ur'string' syntax which is only valid in Python 2.x.
if sys.version_info < (3,0):
    from punctuation2 import openers,closers,delimiters,closing_delimiters
else:
    from .punctuation3 import openers,closers,delimiters,closing_delimiters

### Original code.
# openers = ur"""\"\'\(\<\[\{༺༼᚛⁅⁽₍〈❨❪❬❮❰❲❴⟅⟦⟨⟪⟬⟮⦃⦅⦇⦉⦋⦍⦏⦑⦓⦕⦗⧘⧚⧼⸢⸤⸦⸨〈《「『【〔〖〘〚〝〝﴾︗︵︷︹︻︽︿﹁﹃﹇﹙﹛﹝（［｛｟｢«‘“‹⸂⸄⸉⸌⸜⸠‚„»’”›⸃⸅⸊⸍⸝⸡‛‟"""
# closers = ur"""\"\'\)\>\]\}༻༽᚜⁆⁾₎〉❩❫❭❯❱❳❵⟆⟧⟩⟫⟭⟯⦄⦆⦈⦊⦌⦎⦐⦒⦔⦖⦘⧙⧛⧽⸣⸥⸧⸩〉》」』】〕〗〙〛〞〟﴿︘︶︸︺︼︾﹀﹂﹄﹈﹚﹜﹞）］｝｠｣»’”›⸃⸅⸊⸍⸝⸡‛‟«‘“‹⸂⸄⸉⸌⸜⸠‚„"""
# delimiters = ur"\-\/\:֊־᐀᠆‐‑‒–—―⸗⸚〜〰゠︱︲﹘﹣－¡·¿;·՚՛՜՝՞՟։׀׃׆׳״؉؊،؍؛؞؟٪٫٬٭۔܀܁܂܃܄܅܆܇܈܉܊܋܌܍߷߸߹࠰࠱࠲࠳࠴࠵࠶࠷࠸࠹࠺࠻࠼࠽࠾।॥॰෴๏๚๛༄༅༆༇༈༉༊་༌།༎༏༐༑༒྅࿐࿑࿒࿓࿔၊။၌၍၎၏჻፡።፣፤፥፦፧፨᙭᙮᛫᛬᛭᜵᜶។៕៖៘៙៚᠀᠁᠂᠃᠄᠅᠇᠈᠉᠊᥄᥅᧞᧟᨞᨟᪠᪡᪢᪣᪤᪥᪦᪨᪩᪪᪫᪬᪭᭚᭛᭜᭝᭞᭟᭠᰻᰼᰽᰾᰿᱾᱿᳓‖‗†‡•‣․‥…‧‰‱′″‴‵‶‷‸※‼‽‾⁁⁂⁃⁇⁈⁉⁊⁋⁌⁍⁎⁏⁐⁑⁓⁕⁖⁗⁘⁙⁚⁛⁜⁝⁞⳹⳺⳻⳼⳾⳿⸀⸁⸆⸇⸈⸋⸎⸏⸐⸑⸒⸓⸔⸕⸖⸘⸙⸛⸞⸟⸪⸫⸬⸭⸮⸰⸱、。〃〽・꓾꓿꘍꘎꘏꙳꙾꛲꛳꛴꛵꛶꛷꡴꡵꡶꡷꣎꣏꣸꣹꣺꤮꤯꥟꧁꧂꧃꧄꧅꧆꧇꧈꧉꧊꧋꧌꧍꧞꧟꩜꩝꩞꩟꫞꫟꯫︐︑︒︓︔︕︖︙︰﹅﹆﹉﹊﹋﹌﹐﹑﹒﹔﹕﹖﹗﹟﹠﹡﹨﹪﹫！＂＃％＆＇＊，．／：；？＠＼｡､･𐄀𐄁𐎟𐏐𐡗𐤟𐤿𐩐𐩑𐩒𐩓𐩔𐩕𐩖𐩗𐩘𐩿𐬹𐬺𐬻𐬼𐬽𐬾𐬿𑂻𑂼𑂾𑂿𑃀𑃁𒑰𒑱𒑲𒑳"
# closing_delimiters = ur"\.\,\;\!\?"


# Unicode punctuation character categories
# ----------------------------------------

unicode_punctuation_categories = {
    # 'Pc': 'Connector', # not used in Docutils inline markup recognition
    'Pd': 'Dash',
    'Ps': 'Open',
    'Pe': 'Close',
    'Pi': 'Initial quote', # may behave like Ps or Pe depending on usage
    'Pf': 'Final quote', # may behave like Ps or Pe depending on usage
    'Po': 'Other'
    }
"""Unicode character categories for punctuation"""


# generate character pattern strings
# ==================================

def unicode_charlists(categories, cp_min=0, cp_max=None):
    """Return dictionary of Unicode character lists.

    For each of the `catagories`, an item contains a list with all Unicode
    characters with `cp_min` <= code-point <= `cp_max` that belong to the
    category. (The default values check every code-point supported by Python.)
    """
    # Determine highest code point with one of the given categories
    # (may shorten the search time considerably if there are many
    # categories with not too high characters):
    if cp_max is None:
        cp_max = max(x for x in xrange(sys.maxunicode + 1)
                     if unicodedata.category(unichr(x)) in categories)
        # print cp_max # => 74867 for unicode_punctuation_categories
    charlists = {}
    for cat in categories:
        charlists[cat] = [unichr(x) for x in xrange(cp_min, cp_max+1)
                          if unicodedata.category(unichr(x)) == cat]
    return charlists


# Character categories in Docutils
# --------------------------------

def punctuation_samples():

    """Docutils punctuation category sample strings.

    Return list of sample strings for the categories "Open", "Close",
    "Delimiters" and "Closing-Delimiters" used in the `inline markup
    recognition rules`_.
    """

    # Lists with characters in Unicode punctuation character categories
    cp_min = 160 # ASCII chars have special rules for backwards compatibility
    ucharlists = unicode_charlists(unicode_punctuation_categories, cp_min)

    # match opening/closing characters
    # --------------------------------
    # Rearange the lists to ensure matching characters at the same
    # index position.

    # low quotation marks are also used as closers (e.g. in Greek)
    # move them to category Pi:
    ucharlists['Ps'].remove(u'‚') # 201A  SINGLE LOW-9 QUOTATION MARK
    ucharlists['Ps'].remove(u'„') # 201E  DOUBLE LOW-9 QUOTATION MARK
    ucharlists['Pi'] += [u'‚', u'„']

    ucharlists['Pi'].remove(u'‛') # 201B  SINGLE HIGH-REVERSED-9 QUOTATION MARK
    ucharlists['Pi'].remove(u'‟') # 201F  DOUBLE HIGH-REVERSED-9 QUOTATION MARK
    ucharlists['Pf'] += [u'‛', u'‟']

    # 301F  LOW DOUBLE PRIME QUOTATION MARK misses the opening pendant:
    ucharlists['Ps'].insert(ucharlists['Pe'].index(u'\u301f'), u'\u301d')

    # print u''.join(ucharlists['Ps']).encode('utf8')
    # print u''.join(ucharlists['Pe']).encode('utf8')
    # print u''.join(ucharlists['Pi']).encode('utf8')
    # print u''.join(ucharlists['Pf']).encode('utf8')

    # The Docutils character categories
    # ---------------------------------
    #
    # The categorization of ASCII chars is non-standard to reduce both
    # false positives and need for escaping. (see `inline markup recognition
    # rules`_)

    # matching, allowed before markup
    openers = [re.escape('"\'(<[{')]
    for cat in ('Ps', 'Pi', 'Pf'):
        openers.extend(ucharlists[cat])

    # matching, allowed after markup
    closers = [re.escape('"\')>]}')]
    for cat in ('Pe', 'Pf', 'Pi'):
        closers.extend(ucharlists[cat])

    # non-matching, allowed on both sides
    delimiters = [re.escape('-/:')]
    for cat in ('Pd', 'Po'):
        delimiters.extend(ucharlists[cat])

    # non-matching, after markup
    closing_delimiters = [re.escape('.,;!?')]

    # # Test open/close matching:
    # for i in range(min(len(openers),len(closers))):
    #     print '%4d    %s    %s' % (i, openers[i].encode('utf8'),
    #                                closers[i].encode('utf8'))

    return [u''.join(chars)
            for chars in (openers, closers, delimiters, closing_delimiters)]


# Matching open/close quotes
# --------------------------

# Rule (5) requires determination of matching open/close pairs. However,
# the pairing of open/close quotes is ambigue due to  different typographic
# conventions in different languages.

quote_pairs = {u'\xbb': u'\xbb', # Swedish
               u'\u2018': u'\u201a', # Greek
               u'\u2019': u'\u2019', # Swedish
               u'\u201a': u'\u2018\u2019', # German, Polish
               u'\u201c': u'\u201e', # German
               u'\u201e': u'\u201c\u201d',
               u'\u201d': u'\u201d', # Swedish
               u'\u203a': u'\u203a', # Swedish
              }

def match_chars(c1, c2):
    try:
        i = openers.index(c1)
    except ValueError:  # c1 not in openers
        return False
    return c2 == closers[i] or c2 in quote_pairs.get(c1, '')





# print results
# =============

if __name__ == '__main__':

    # (re) create and compare the samples:
    (o, c, d, cd) = punctuation_samples()
    ### 2to3: ur -> r
    if o != openers:
        print(('- openers = r"""%s"""' % openers.encode('utf8')))
        print(('+ openers = r"""%s"""' % o.encode('utf8')))
    if c != closers:
        print(('- closers = r"""%s"""' % closers.encode('utf8')))
        print(('+ closers = r"""%s"""' % c.encode('utf8')))
    if d != delimiters:
        print(('- delimiters = r"%s"' % delimiters.encode('utf8')))
        print(('+ delimiters = r"%s"' % d.encode('utf8')))
    if cd != closing_delimiters:
        print(('- closing_delimiters = r"%s"' % closing_delimiters.encode('utf8')))
        print(('+ closing_delimiters = r"%s"' % cd.encode('utf8')))

    # # test prints
    # print 'openers = ', repr(openers)
    # print 'closers = ', repr(closers)
    # print 'delimiters = ', repr(delimiters)
    # print 'closing_delimiters = ', repr(closing_delimiters)

    # ucharlists = unicode_charlists(unicode_punctuation_categories)
    # for cat, chars in ucharlists.items():
    #     # print cat, chars
    #     # compact output (visible with a comprehensive font):
    #     print (u":%s: %s" % (cat, u''.join(chars))).encode('utf8')
#@+node:ekr.20130417081749.10501: *4* @@edit parsers\rst\states.py
@killcolor

@language python
# $Id: states.py 7495 2012-08-16 14:50:57Z milde $
# Author: David Goodger <goodger@python.org>
# Copyright: This module has been placed in the public domain.

"""
This is the ``docutils.parsers.rst.states`` module, the core of
the reStructuredText parser.  It defines the following:

:Classes:
    - `RSTStateMachine`: reStructuredText parser's entry point.
    - `NestedStateMachine`: recursive StateMachine.
    - `RSTState`: reStructuredText State superclass.
    - `Inliner`: For parsing inline markup.
    - `Body`: Generic classifier of the first line of a block.
    - `SpecializedBody`: Superclass for compound element members.
    - `BulletList`: Second and subsequent bullet_list list_items
    - `DefinitionList`: Second+ definition_list_items.
    - `EnumeratedList`: Second+ enumerated_list list_items.
    - `FieldList`: Second+ fields.
    - `OptionList`: Second+ option_list_items.
    - `RFC2822List`: Second+ RFC2822-style fields.
    - `ExtensionOptions`: Parses directive option fields.
    - `Explicit`: Second+ explicit markup constructs.
    - `SubstitutionDef`: For embedded directives in substitution definitions.
    - `Text`: Classifier of second line of a text block.
    - `SpecializedText`: Superclass for continuation lines of Text-variants.
    - `Definition`: Second line of potential definition_list_item.
    - `Line`: Second line of overlined section title or transition marker.
    - `Struct`: An auxiliary collection class.

:Exception classes:
    - `MarkupError`
    - `ParserError`
    - `MarkupMismatch`

:Functions:
    - `escape2null()`: Return a string, escape-backslashes converted to nulls.
    - `unescape()`: Return a string, nulls removed or restored to backslashes.

:Attributes:
    - `state_classes`: set of State classes used with `RSTStateMachine`.

Parser Overview
===============

The reStructuredText parser is implemented as a recursive state machine,
examining its input one line at a time.  To understand how the parser works,
please first become familiar with the `docutils.statemachine` module.  In the
description below, references are made to classes defined in this module;
please see the individual classes for details.

Parsing proceeds as follows:

1. The state machine examines each line of input, checking each of the
   transition patterns of the state `Body`, in order, looking for a match.
   The implicit transitions (blank lines and indentation) are checked before
   any others.  The 'text' transition is a catch-all (matches anything).

2. The method associated with the matched transition pattern is called.

   A. Some transition methods are self-contained, appending elements to the
      document tree (`Body.doctest` parses a doctest block).  The parser's
      current line index is advanced to the end of the element, and parsing
      continues with step 1.

   B. Other transition methods trigger the creation of a nested state machine,
      whose job is to parse a compound construct ('indent' does a block quote,
      'bullet' does a bullet list, 'overline' does a section [first checking
      for a valid section header], etc.).

      - In the case of lists and explicit markup, a one-off state machine is
        created and run to parse contents of the first item.

      - A new state machine is created and its initial state is set to the
        appropriate specialized state (`BulletList` in the case of the
        'bullet' transition; see `SpecializedBody` for more detail).  This
        state machine is run to parse the compound element (or series of
        explicit markup elements), and returns as soon as a non-member element
        is encountered.  For example, the `BulletList` state machine ends as
        soon as it encounters an element which is not a list item of that
        bullet list.  The optional omission of inter-element blank lines is
        enabled by this nested state machine.

      - The current line index is advanced to the end of the elements parsed,
        and parsing continues with step 1.

   C. The result of the 'text' transition depends on the next line of text.
      The current state is changed to `Text`, under which the second line is
      examined.  If the second line is:

      - Indented: The element is a definition list item, and parsing proceeds
        similarly to step 2.B, using the `DefinitionList` state.

      - A line of uniform punctuation characters: The element is a section
        header; again, parsing proceeds as in step 2.B, and `Body` is still
        used.

      - Anything else: The element is a paragraph, which is examined for
        inline markup and appended to the parent element.  Processing
        continues with step 1.
"""

__docformat__ = 'reStructuredText'


import sys
import re
try:
    import roman
except ImportError:
    import docutils.utils.roman as roman
from types import FunctionType, MethodType

from docutils import nodes, statemachine, utils
from docutils import ApplicationError, DataError
from docutils.statemachine import StateMachineWS, StateWS
from docutils.nodes import fully_normalize_name as normalize_name
from docutils.nodes import whitespace_normalize_name
import docutils.parsers.rst
from docutils.parsers.rst import directives, languages, tableparser, roles
from docutils.parsers.rst.languages import en as _fallback_language_module
from docutils.utils import escape2null, unescape, column_width
from docutils.utils import punctuation_chars, urischemes

class MarkupError(DataError): pass
class UnknownInterpretedRoleError(DataError): pass
class InterpretedRoleNotImplementedError(DataError): pass
class ParserError(ApplicationError): pass
class MarkupMismatch(Exception): pass


class Struct:

    """Stores data attributes for dotted-attribute access."""

    def __init__(self, **keywordargs):
        self.__dict__.update(keywordargs)


class RSTStateMachine(StateMachineWS):

    """
    reStructuredText's master StateMachine.

    The entry point to reStructuredText parsing is the `run()` method.
    """

    def run(self, input_lines, document, input_offset=0, match_titles=True,
            inliner=None):
        """
        Parse `input_lines` and modify the `document` node in place.

        Extend `StateMachineWS.run()`: set up parse-global data and
        run the StateMachine.
        """
        self.language = languages.get_language(
            document.settings.language_code)
        self.match_titles = match_titles
        if inliner is None:
            inliner = Inliner()
        inliner.init_customizations(document.settings)
        self.memo = Struct(document=document,
                           reporter=document.reporter,
                           language=self.language,
                           title_styles=[],
                           section_level=0,
                           section_bubble_up_kludge=False,
                           inliner=inliner)
        self.document = document
        self.attach_observer(document.note_source)
        self.reporter = self.memo.reporter
        self.node = document
        results = StateMachineWS.run(self, input_lines, input_offset,
                                     input_source=document['source'])
        assert results == [], 'RSTStateMachine.run() results should be empty!'
        self.node = self.memo = None    # remove unneeded references


class NestedStateMachine(StateMachineWS):

    """
    StateMachine run from within other StateMachine runs, to parse nested
    document structures.
    """

    def run(self, input_lines, input_offset, memo, node, match_titles=True):
        """
        Parse `input_lines` and populate a `docutils.nodes.document` instance.

        Extend `StateMachineWS.run()`: set up document-wide data.
        """
        self.match_titles = match_titles
        self.memo = memo
        self.document = memo.document
        self.attach_observer(self.document.note_source)
        self.reporter = memo.reporter
        self.language = memo.language
        self.node = node
        results = StateMachineWS.run(self, input_lines, input_offset)
        assert results == [], ('NestedStateMachine.run() results should be '
                               'empty!')
        return results


class RSTState(StateWS):

    """
    reStructuredText State superclass.

    Contains methods used by all State subclasses.
    """

    nested_sm = NestedStateMachine
    nested_sm_cache = []

    def __init__(self, state_machine, debug=False):
        self.nested_sm_kwargs = {'state_classes': state_classes,
                                 'initial_state': 'Body'}
        StateWS.__init__(self, state_machine, debug)

    def runtime_init(self):
        StateWS.runtime_init(self)
        memo = self.state_machine.memo
        self.memo = memo
        self.reporter = memo.reporter
        self.inliner = memo.inliner
        self.document = memo.document
        self.parent = self.state_machine.node
        # enable the reporter to determine source and source-line
        if not hasattr(self.reporter, 'get_source_and_line'):
            self.reporter.get_source_and_line = self.state_machine.get_source_and_line
            # print "adding get_source_and_line to reporter", self.state_machine.input_offset


    def goto_line(self, abs_line_offset):
        """
        Jump to input line `abs_line_offset`, ignoring jumps past the end.
        """
        try:
            self.state_machine.goto_line(abs_line_offset)
        except EOFError:
            pass

    def no_match(self, context, transitions):
        """
        Override `StateWS.no_match` to generate a system message.

        This code should never be run.
        """
        self.reporter.severe(
            'Internal error: no transition pattern match.  State: "%s"; '
            'transitions: %s; context: %s; current line: %r.'
            % (self.__class__.__name__, transitions, context,
               self.state_machine.line))
        return context, None, []

    def bof(self, context):
        """Called at beginning of file."""
        return [], []

    def nested_parse(self, block, input_offset, node, match_titles=False,
                     state_machine_class=None, state_machine_kwargs=None):
        """
        Create a new StateMachine rooted at `node` and run it over the input
        `block`.
        """
        use_default = 0
        if state_machine_class is None:
            state_machine_class = self.nested_sm
            use_default += 1
        if state_machine_kwargs is None:
            state_machine_kwargs = self.nested_sm_kwargs
            use_default += 1
        block_length = len(block)

        state_machine = None
        if use_default == 2:
            try:
                state_machine = self.nested_sm_cache.pop()
            except IndexError:
                pass
        if not state_machine:
            state_machine = state_machine_class(debug=self.debug,
                                                **state_machine_kwargs)
        state_machine.run(block, input_offset, memo=self.memo,
                          node=node, match_titles=match_titles)
        if use_default == 2:
            self.nested_sm_cache.append(state_machine)
        else:
            state_machine.unlink()
        new_offset = state_machine.abs_line_offset()
        # No `block.parent` implies disconnected -- lines aren't in sync:
        if block.parent and (len(block) - block_length) != 0:
            # Adjustment for block if modified in nested parse:
            self.state_machine.next_line(len(block) - block_length)
        return new_offset

    def nested_list_parse(self, block, input_offset, node, initial_state,
                          blank_finish,
                          blank_finish_state=None,
                          extra_settings={},
                          match_titles=False,
                          state_machine_class=None,
                          state_machine_kwargs=None):
        """
        Create a new StateMachine rooted at `node` and run it over the input
        `block`. Also keep track of optional intermediate blank lines and the
        required final one.
        """
        if state_machine_class is None:
            state_machine_class = self.nested_sm
        if state_machine_kwargs is None:
            state_machine_kwargs = self.nested_sm_kwargs.copy()
        state_machine_kwargs['initial_state'] = initial_state
        state_machine = state_machine_class(debug=self.debug,
                                            **state_machine_kwargs)
        if blank_finish_state is None:
            blank_finish_state = initial_state
        state_machine.states[blank_finish_state].blank_finish = blank_finish
        for key, value in extra_settings.items():
            setattr(state_machine.states[initial_state], key, value)
        state_machine.run(block, input_offset, memo=self.memo,
                          node=node, match_titles=match_titles)
        blank_finish = state_machine.states[blank_finish_state].blank_finish
        state_machine.unlink()
        return state_machine.abs_line_offset(), blank_finish

    def section(self, title, source, style, lineno, messages):
        """Check for a valid subsection and create one if it checks out."""
        if self.check_subsection(source, style, lineno):
            self.new_subsection(title, lineno, messages)

    def check_subsection(self, source, style, lineno):
        """
        Check for a valid subsection header.  Return 1 (true) or None (false).

        When a new section is reached that isn't a subsection of the current
        section, back up the line count (use ``previous_line(-x)``), then
        ``raise EOFError``.  The current StateMachine will finish, then the
        calling StateMachine can re-examine the title.  This will work its way
        back up the calling chain until the correct section level isreached.

        @@@ Alternative: Evaluate the title, store the title info & level, and
        back up the chain until that level is reached.  Store in memo? Or
        return in results?

        :Exception: `EOFError` when a sibling or supersection encountered.
        """
        memo = self.memo
        title_styles = memo.title_styles
        mylevel = memo.section_level
        try:                            # check for existing title style
            level = title_styles.index(style) + 1
        except ValueError:              # new title style
            if len(title_styles) == memo.section_level: # new subsection
                title_styles.append(style)
                return 1
            else:                       # not at lowest level
                self.parent += self.title_inconsistent(source, lineno)
                return None
        if level <= mylevel:            # sibling or supersection
            memo.section_level = level   # bubble up to parent section
            if len(style) == 2:
                memo.section_bubble_up_kludge = True
            # back up 2 lines for underline title, 3 for overline title
            self.state_machine.previous_line(len(style) + 1)
            raise EOFError              # let parent section re-evaluate
        if level == mylevel + 1:        # immediate subsection
            return 1
        else:                           # invalid subsection
            self.parent += self.title_inconsistent(source, lineno)
            return None

    def title_inconsistent(self, sourcetext, lineno):
        error = self.reporter.severe(
            'Title level inconsistent:', nodes.literal_block('', sourcetext),
            line=lineno)
        return error

    def new_subsection(self, title, lineno, messages):
        """Append new subsection to document tree. On return, check level."""
        memo = self.memo
        mylevel = memo.section_level
        memo.section_level += 1
        section_node = nodes.section()
        self.parent += section_node
        textnodes, title_messages = self.inline_text(title, lineno)
        titlenode = nodes.title(title, '', *textnodes)
        name = normalize_name(titlenode.astext())
        section_node['names'].append(name)
        section_node += titlenode
        section_node += messages
        section_node += title_messages
        self.document.note_implicit_target(section_node, section_node)
        offset = self.state_machine.line_offset + 1
        absoffset = self.state_machine.abs_line_offset() + 1
        newabsoffset = self.nested_parse(
              self.state_machine.input_lines[offset:], input_offset=absoffset,
              node=section_node, match_titles=True)
        self.goto_line(newabsoffset)
        if memo.section_level <= mylevel: # can't handle next section?
            raise EOFError              # bubble up to supersection
        # reset section_level; next pass will detect it properly
        memo.section_level = mylevel

    def paragraph(self, lines, lineno):
        """
        Return a list (paragraph & messages) & a boolean: literal_block next?
        """
        data = '\n'.join(lines).rstrip()
        if re.search(r'(?<!\\)(\\\\)*::$', data):
            if len(data) == 2:
                return [], 1
            elif data[-3] in ' \n':
                text = data[:-3].rstrip()
            else:
                text = data[:-1]
            literalnext = 1
        else:
            text = data
            literalnext = 0
        textnodes, messages = self.inline_text(text, lineno)
        p = nodes.paragraph(data, '', *textnodes)
        p.source, p.line = self.state_machine.get_source_and_line(lineno)
        return [p] + messages, literalnext

    def inline_text(self, text, lineno):
        """
        Return 2 lists: nodes (text and inline elements), and system_messages.
        """
        return self.inliner.parse(text, lineno, self.memo, self.parent)

    def unindent_warning(self, node_name):
        # the actual problem is one line below the current line
        lineno = self.state_machine.abs_line_number()+1
        return self.reporter.warning('%s ends without a blank line; '
                                     'unexpected unindent.' % node_name,
                                     line=lineno)


def build_regexp(definition, compile=True):
    """
    Build, compile and return a regular expression based on `definition`.

    :Parameter: `definition`: a 4-tuple (group name, prefix, suffix, parts),
        where "parts" is a list of regular expressions and/or regular
        expression definitions to be joined into an or-group.
    """
    name, prefix, suffix, parts = definition
    part_strings = []
    for part in parts:
        ### if type(part) is tuple:
        if isinstance(part,tuple):
            part_strings.append(build_regexp(part, None))
        else:
            part_strings.append(part)
    or_group = '|'.join(part_strings)
    regexp = '%(prefix)s(?P<%(name)s>%(or_group)s)%(suffix)s' % locals()
    if compile:
        return re.compile(regexp, re.UNICODE)
    else:
        return regexp


class Inliner:

    """
    Parse inline markup; call the `parse()` method.
    """

    def __init__(self):
        self.implicit_dispatch = [(self.patterns.uri, self.standalone_uri),]
        """List of (pattern, bound method) tuples, used by
        `self.implicit_inline`."""

    def init_customizations(self, settings):
        """Setting-based customizations; run when parsing begins."""
        if settings.pep_references:
            self.implicit_dispatch.append((self.patterns.pep,
                                           self.pep_reference))
        if settings.rfc_references:
            self.implicit_dispatch.append((self.patterns.rfc,
                                           self.rfc_reference))

    def parse(self, text, lineno, memo, parent):
        # Needs to be refactored for nested inline markup.
        # Add nested_parse() method?
        """
        Return 2 lists: nodes (text and inline elements), and system_messages.

        Using `self.patterns.initial`, a pattern which matches start-strings
        (emphasis, strong, interpreted, phrase reference, literal,
        substitution reference, and inline target) and complete constructs
        (simple reference, footnote reference), search for a candidate.  When
        one is found, check for validity (e.g., not a quoted '*' character).
        If valid, search for the corresponding end string if applicable, and
        check it for validity.  If not found or invalid, generate a warning
        and ignore the start-string.  Implicit inline markup (e.g. standalone
        URIs) is found last.
        """
        self.reporter = memo.reporter
        self.document = memo.document
        self.language = memo.language
        self.parent = parent
        pattern_search = self.patterns.initial.search
        dispatch = self.dispatch
        remaining = escape2null(text)
        processed = []
        unprocessed = []
        messages = []
        while remaining:
            match = pattern_search(remaining)
            if match:
                groups = match.groupdict()
                method = dispatch[groups['start'] or groups['backquote']
                                  or groups['refend'] or groups['fnend']]
                before, inlines, remaining, sysmessages = method(self, match,
                                                                 lineno)
                unprocessed.append(before)
                messages += sysmessages
                if inlines:
                    processed += self.implicit_inline(''.join(unprocessed),
                                                      lineno)
                    processed += inlines
                    unprocessed = []
            else:
                break
        remaining = ''.join(unprocessed) + remaining
        if remaining:
            processed += self.implicit_inline(remaining, lineno)
        return processed, messages

    # Inline object recognition
    # -------------------------
    # lookahead and look-behind expressions for inline markup rules
    start_string_prefix = (u'(^|(?<=\\s|[%s%s]))' %
                           (punctuation_chars.openers,
                            punctuation_chars.delimiters))
    end_string_suffix = (u'($|(?=\\s|[\x00%s%s%s]))' %
                         (punctuation_chars.closing_delimiters,
                          punctuation_chars.delimiters,
                          punctuation_chars.closers))
    # print start_string_prefix.encode('utf8')
    # TODO: support non-ASCII whitespace in the following 4 patterns?
    non_whitespace_before = r'(?<![ \n])'
    non_whitespace_escape_before = r'(?<![ \n\x00])'
    non_unescaped_whitespace_escape_before = r'(?<!(?<!\x00)[ \n\x00])'
    non_whitespace_after = r'(?![ \n])'
    # Alphanumerics with isolated internal [-._+:] chars (i.e. not 2 together):
    simplename = r'(?:(?!_)\w)+(?:[-._+:](?:(?!_)\w)+)*'
    # Valid URI characters (see RFC 2396 & RFC 2732);
    # final \x00 allows backslash escapes in URIs:
    uric = r"""[-_.!~*'()[\];/:@&=+$,%a-zA-Z0-9\x00]"""
    # Delimiter indicating the end of a URI (not part of the URI):
    uri_end_delim = r"""[>]"""
    # Last URI character; same as uric but no punctuation:
    urilast = r"""[_~*/=+a-zA-Z0-9]"""
    # End of a URI (either 'urilast' or 'uric followed by a
    # uri_end_delim'):
    uri_end = r"""(?:%(urilast)s|%(uric)s(?=%(uri_end_delim)s))""" % locals()
    emailc = r"""[-_!~*'{|}/#?^`&=+$%a-zA-Z0-9\x00]"""
    email_pattern = r"""
          %(emailc)s+(?:\.%(emailc)s+)*   # name
          (?<!\x00)@                      # at
          %(emailc)s+(?:\.%(emailc)s*)*   # host
          %(uri_end)s                     # final URI char
          """
    parts = ('initial_inline', start_string_prefix, '',
             [('start', '', non_whitespace_after,  # simple start-strings
               [r'\*\*',                # strong
                r'\*(?!\*)',            # emphasis but not strong
                r'``',                  # literal
                r'_`',                  # inline internal target
                r'\|(?!\|)']            # substitution reference
               ),
              ('whole', '', end_string_suffix, # whole constructs
               [# reference name & end-string
                r'(?P<refname>%s)(?P<refend>__?)' % simplename,
                ('footnotelabel', r'\[', r'(?P<fnend>\]_)',
                 [r'[0-9]+',               # manually numbered
                  r'\#(%s)?' % simplename, # auto-numbered (w/ label?)
                  r'\*',                   # auto-symbol
                  r'(?P<citationlabel>%s)' % simplename] # citation reference
                 )
                ]
               ),
              ('backquote',             # interpreted text or phrase reference
               '(?P<role>(:%s:)?)' % simplename, # optional role
               non_whitespace_after,
               ['`(?!`)']               # but not literal
               )
              ]
             )
    patterns = Struct(
          initial=build_regexp(parts),
          emphasis=re.compile(non_whitespace_escape_before
                              + r'(\*)' + end_string_suffix, re.UNICODE),
          strong=re.compile(non_whitespace_escape_before
                            + r'(\*\*)' + end_string_suffix, re.UNICODE),
          interpreted_or_phrase_ref=re.compile(
              r"""
              %(non_unescaped_whitespace_escape_before)s
              (
                `
                (?P<suffix>
                  (?P<role>:%(simplename)s:)?
                  (?P<refend>__?)?
                )
              )
              %(end_string_suffix)s
              """ % locals(), re.VERBOSE | re.UNICODE),
          embedded_uri=re.compile(
              r"""
              (
                (?:[ \n]+|^)            # spaces or beginning of line/string
                <                       # open bracket
                %(non_whitespace_after)s
                ([^<>\x00]+)            # anything but angle brackets & nulls
                %(non_whitespace_before)s
                >                       # close bracket w/o whitespace before
              )
              $                         # end of string
              """ % locals(), re.VERBOSE | re.UNICODE),
          literal=re.compile(non_whitespace_before + '(``)'
                             + end_string_suffix),
          target=re.compile(non_whitespace_escape_before
                            + r'(`)' + end_string_suffix),
          substitution_ref=re.compile(non_whitespace_escape_before
                                      + r'(\|_{0,2})'
                                      + end_string_suffix),
          email=re.compile(email_pattern % locals() + '$',
                           re.VERBOSE | re.UNICODE),
          uri=re.compile(
                (r"""
                %(start_string_prefix)s
                (?P<whole>
                  (?P<absolute>           # absolute URI
                    (?P<scheme>             # scheme (http, ftp, mailto)
                      [a-zA-Z][a-zA-Z0-9.+-]*
                    )
                    :
                    (
                      (                       # either:
                        (//?)?                  # hierarchical URI
                        %(uric)s*               # URI characters
                        %(uri_end)s             # final URI char
                      )
                      (                       # optional query
                        \?%(uric)s*
                        %(uri_end)s
                      )?
                      (                       # optional fragment
                        \#%(uric)s*
                        %(uri_end)s
                      )?
                    )
                  )
                |                       # *OR*
                  (?P<email>              # email address
                    """ + email_pattern + r"""
                  )
                )
                %(end_string_suffix)s
                """) % locals(), re.VERBOSE | re.UNICODE),
          pep=re.compile(
                r"""
                %(start_string_prefix)s
                (
                  (pep-(?P<pepnum1>\d+)(.txt)?) # reference to source file
                |
                  (PEP\s+(?P<pepnum2>\d+))      # reference by name
                )
                %(end_string_suffix)s""" % locals(), re.VERBOSE | re.UNICODE),
          rfc=re.compile(
                r"""
                %(start_string_prefix)s
                (RFC(-|\s+)?(?P<rfcnum>\d+))
                %(end_string_suffix)s""" % locals(), re.VERBOSE | re.UNICODE))

    def quoted_start(self, match):
        """Test if inline markup start-string is 'quoted'.

        'Quoted' in this context means the start-string is enclosed in a pair
        of matching opening/closing delimiters (not necessarily quotes)
        or at the end of the match.
        """
        string = match.string
        start = match.start()
        if start == 0:                  # start-string at beginning of text
            return False
        prestart = string[start - 1]
        try:
            poststart = string[match.end()]
        except IndexError:          # start-string at end of text
            return True  # not "quoted" but no markup start-string either
        return punctuation_chars.match_chars(prestart, poststart)

    def inline_obj(self, match, lineno, end_pattern, nodeclass,
                   restore_backslashes=False):
        string = match.string
        matchstart = match.start('start')
        matchend = match.end('start')
        if self.quoted_start(match):
            return (string[:matchend], [], string[matchend:], [], '')
        endmatch = end_pattern.search(string[matchend:])
        if endmatch and endmatch.start(1):  # 1 or more chars
            text = unescape(endmatch.string[:endmatch.start(1)],
                            restore_backslashes)
            textend = matchend + endmatch.end(1)
            rawsource = unescape(string[matchstart:textend], 1)
            return (string[:matchstart], [nodeclass(rawsource, text)],
                    string[textend:], [], endmatch.group(1))
        msg = self.reporter.warning(
              'Inline %s start-string without end-string.'
              % nodeclass.__name__, line=lineno)
        text = unescape(string[matchstart:matchend], 1)
        rawsource = unescape(string[matchstart:matchend], 1)
        prb = self.problematic(text, rawsource, msg)
        return string[:matchstart], [prb], string[matchend:], [msg], ''

    def problematic(self, text, rawsource, message):
        msgid = self.document.set_id(message, self.parent)
        problematic = nodes.problematic(rawsource, text, refid=msgid)
        prbid = self.document.set_id(problematic)
        message.add_backref(prbid)
        return problematic

    def emphasis(self, match, lineno):
        before, inlines, remaining, sysmessages, endstring = self.inline_obj(
              match, lineno, self.patterns.emphasis, nodes.emphasis)
        return before, inlines, remaining, sysmessages

    def strong(self, match, lineno):
        before, inlines, remaining, sysmessages, endstring = self.inline_obj(
              match, lineno, self.patterns.strong, nodes.strong)
        return before, inlines, remaining, sysmessages

    def interpreted_or_phrase_ref(self, match, lineno):
        end_pattern = self.patterns.interpreted_or_phrase_ref
        string = match.string
        matchstart = match.start('backquote')
        matchend = match.end('backquote')
        rolestart = match.start('role')
        role = match.group('role')
        position = ''
        if role:
            role = role[1:-1]
            position = 'prefix'
        elif self.quoted_start(match):
            return (string[:matchend], [], string[matchend:], [])
        endmatch = end_pattern.search(string[matchend:])
        if endmatch and endmatch.start(1):  # 1 or more chars
            textend = matchend + endmatch.end()
            if endmatch.group('role'):
                if role:
                    msg = self.reporter.warning(
                        'Multiple roles in interpreted text (both '
                        'prefix and suffix present; only one allowed).',
                        line=lineno)
                    text = unescape(string[rolestart:textend], 1)
                    prb = self.problematic(text, text, msg)
                    return string[:rolestart], [prb], string[textend:], [msg]
                role = endmatch.group('suffix')[1:-1]
                position = 'suffix'
            escaped = endmatch.string[:endmatch.start(1)]
            rawsource = unescape(string[matchstart:textend], 1)
            if rawsource[-1:] == '_':
                if role:
                    msg = self.reporter.warning(
                          'Mismatch: both interpreted text role %s and '
                          'reference suffix.' % position, line=lineno)
                    text = unescape(string[rolestart:textend], 1)
                    prb = self.problematic(text, text, msg)
                    return string[:rolestart], [prb], string[textend:], [msg]
                return self.phrase_ref(string[:matchstart], string[textend:],
                                       rawsource, escaped, unescape(escaped))
            else:
                rawsource = unescape(string[rolestart:textend], 1)
                nodelist, messages = self.interpreted(rawsource, escaped, role,
                                                      lineno)
                return (string[:rolestart], nodelist,
                        string[textend:], messages)
        msg = self.reporter.warning(
              'Inline interpreted text or phrase reference start-string '
              'without end-string.', line=lineno)
        text = unescape(string[matchstart:matchend], 1)
        prb = self.problematic(text, text, msg)
        return string[:matchstart], [prb], string[matchend:], [msg]

    def phrase_ref(self, before, after, rawsource, escaped, text):
        match = self.patterns.embedded_uri.search(escaped)
        if match:
            text = unescape(escaped[:match.start(0)])
            uri_text = match.group(2)
            uri = ''.join(uri_text.split())
            uri = self.adjust_uri(uri)
            if uri:
                target = nodes.target(match.group(1), refuri=uri)
                target.referenced = 1
            else:
                raise ApplicationError('problem with URI: %r' % uri_text)
            if not text:
                text = uri
        else:
            target = None
        refname = normalize_name(text)
        reference = nodes.reference(rawsource, text,
                                    name=whitespace_normalize_name(text))
        node_list = [reference]
        if rawsource[-2:] == '__':
            if target:
                reference['refuri'] = uri
            else:
                reference['anonymous'] = 1
        else:
            if target:
                reference['refuri'] = uri
                target['names'].append(refname)
                self.document.note_explicit_target(target, self.parent)
                node_list.append(target)
            else:
                reference['refname'] = refname
                self.document.note_refname(reference)
        return before, node_list, after, []

    def adjust_uri(self, uri):
        match = self.patterns.email.match(uri)
        if match:
            return 'mailto:' + uri
        else:
            return uri

    def interpreted(self, rawsource, text, role, lineno):
        role_fn, messages = roles.role(role, self.language, lineno,
                                       self.reporter)
        if role_fn:
            nodes, messages2 = role_fn(role, rawsource, text, lineno, self)
            return nodes, messages + messages2
        else:
            msg = self.reporter.error(
                'Unknown interpreted text role "%s".' % role,
                line=lineno)
            return ([self.problematic(rawsource, rawsource, msg)],
                    messages + [msg])

    def literal(self, match, lineno):
        before, inlines, remaining, sysmessages, endstring = self.inline_obj(
              match, lineno, self.patterns.literal, nodes.literal,
              restore_backslashes=True)
        return before, inlines, remaining, sysmessages

    def inline_internal_target(self, match, lineno):
        before, inlines, remaining, sysmessages, endstring = self.inline_obj(
              match, lineno, self.patterns.target, nodes.target)
        if inlines and isinstance(inlines[0], nodes.target):
            assert len(inlines) == 1
            target = inlines[0]
            name = normalize_name(target.astext())
            target['names'].append(name)
            self.document.note_explicit_target(target, self.parent)
        return before, inlines, remaining, sysmessages

    def substitution_reference(self, match, lineno):
        before, inlines, remaining, sysmessages, endstring = self.inline_obj(
              match, lineno, self.patterns.substitution_ref,
              nodes.substitution_reference)
        if len(inlines) == 1:
            subref_node = inlines[0]
            if isinstance(subref_node, nodes.substitution_reference):
                subref_text = subref_node.astext()
                self.document.note_substitution_ref(subref_node, subref_text)
                if endstring[-1:] == '_':
                    reference_node = nodes.reference(
                        '|%s%s' % (subref_text, endstring), '')
                    if endstring[-2:] == '__':
                        reference_node['anonymous'] = 1
                    else:
                        reference_node['refname'] = normalize_name(subref_text)
                        self.document.note_refname(reference_node)
                    reference_node += subref_node
                    inlines = [reference_node]
        return before, inlines, remaining, sysmessages

    def footnote_reference(self, match, lineno):
        """
        Handles `nodes.footnote_reference` and `nodes.citation_reference`
        elements.
        """
        label = match.group('footnotelabel')
        refname = normalize_name(label)
        string = match.string
        before = string[:match.start('whole')]
        remaining = string[match.end('whole'):]
        if match.group('citationlabel'):
            refnode = nodes.citation_reference('[%s]_' % label,
                                               refname=refname)
            refnode += nodes.Text(label)
            self.document.note_citation_ref(refnode)
        else:
            refnode = nodes.footnote_reference('[%s]_' % label)
            if refname[0] == '#':
                refname = refname[1:]
                refnode['auto'] = 1
                self.document.note_autofootnote_ref(refnode)
            elif refname == '*':
                refname = ''
                refnode['auto'] = '*'
                self.document.note_symbol_footnote_ref(
                      refnode)
            else:
                refnode += nodes.Text(label)
            if refname:
                refnode['refname'] = refname
                self.document.note_footnote_ref(refnode)
            if utils.get_trim_footnote_ref_space(self.document.settings):
                before = before.rstrip()
        return (before, [refnode], remaining, [])

    def reference(self, match, lineno, anonymous=False):
        referencename = match.group('refname')
        refname = normalize_name(referencename)
        referencenode = nodes.reference(
            referencename + match.group('refend'), referencename,
            name=whitespace_normalize_name(referencename))
        if anonymous:
            referencenode['anonymous'] = 1
        else:
            referencenode['refname'] = refname
            self.document.note_refname(referencenode)
        string = match.string
        matchstart = match.start('whole')
        matchend = match.end('whole')
        return (string[:matchstart], [referencenode], string[matchend:], [])

    def anonymous_reference(self, match, lineno):
        return self.reference(match, lineno, anonymous=1)

    def standalone_uri(self, match, lineno):
        if (not match.group('scheme')
                or match.group('scheme').lower() in urischemes.schemes):
            if match.group('email'):
                addscheme = 'mailto:'
            else:
                addscheme = ''
            text = match.group('whole')
            unescaped = unescape(text, 0)
            return [nodes.reference(unescape(text, 1), unescaped,
                                    refuri=addscheme + unescaped)]
        else:                   # not a valid scheme
            raise MarkupMismatch

    def pep_reference(self, match, lineno):
        text = match.group(0)
        if text.startswith('pep-'):
            pepnum = int(match.group('pepnum1'))
        elif text.startswith('PEP'):
            pepnum = int(match.group('pepnum2'))
        else:
            raise MarkupMismatch
        ref = (self.document.settings.pep_base_url
               + self.document.settings.pep_file_url_template % pepnum)
        unescaped = unescape(text, 0)
        return [nodes.reference(unescape(text, 1), unescaped, refuri=ref)]

    rfc_url = 'rfc%d.html'

    def rfc_reference(self, match, lineno):
        text = match.group(0)
        if text.startswith('RFC'):
            rfcnum = int(match.group('rfcnum'))
            ref = self.document.settings.rfc_base_url + self.rfc_url % rfcnum
        else:
            raise MarkupMismatch
        unescaped = unescape(text, 0)
        return [nodes.reference(unescape(text, 1), unescaped, refuri=ref)]

    def implicit_inline(self, text, lineno):
        """
        Check each of the patterns in `self.implicit_dispatch` for a match,
        and dispatch to the stored method for the pattern.  Recursively check
        the text before and after the match.  Return a list of `nodes.Text`
        and inline element nodes.
        """
        if not text:
            return []
        for pattern, method in self.implicit_dispatch:
            match = pattern.search(text)
            if match:
                try:
                    # Must recurse on strings before *and* after the match;
                    # there may be multiple patterns.
                    return (self.implicit_inline(text[:match.start()], lineno)
                            + method(match, lineno) +
                            self.implicit_inline(text[match.end():], lineno))
                except MarkupMismatch:
                    pass
        return [nodes.Text(unescape(text), rawsource=unescape(text, 1))]

    dispatch = {'*': emphasis,
                '**': strong,
                '`': interpreted_or_phrase_ref,
                '``': literal,
                '_`': inline_internal_target,
                ']_': footnote_reference,
                '|': substitution_reference,
                '_': reference,
                '__': anonymous_reference}


def _loweralpha_to_int(s, _zero=(ord('a')-1)):
    return ord(s) - _zero

def _upperalpha_to_int(s, _zero=(ord('A')-1)):
    return ord(s) - _zero

def _lowerroman_to_int(s):
    return roman.fromRoman(s.upper())


class Body(RSTState):

    """
    Generic classifier of the first line of a block.
    """

    double_width_pad_char = tableparser.TableParser.double_width_pad_char
    """Padding character for East Asian double-width text."""

    enum = Struct()
    """Enumerated list parsing information."""

    enum.formatinfo = {
          'parens': Struct(prefix='(', suffix=')', start=1, end=-1),
          'rparen': Struct(prefix='', suffix=')', start=0, end=-1),
          'period': Struct(prefix='', suffix='.', start=0, end=-1)}
    enum.formats = enum.formatinfo.keys()
    enum.sequences = ['arabic', 'loweralpha', 'upperalpha',
                      'lowerroman', 'upperroman'] # ORDERED!
    enum.sequencepats = {'arabic': '[0-9]+',
                         'loweralpha': '[a-z]',
                         'upperalpha': '[A-Z]',
                         'lowerroman': '[ivxlcdm]+',
                         'upperroman': '[IVXLCDM]+',}
    enum.converters = {'arabic': int,
                       'loweralpha': _loweralpha_to_int,
                       'upperalpha': _upperalpha_to_int,
                       'lowerroman': _lowerroman_to_int,
                       'upperroman': roman.fromRoman}

    enum.sequenceregexps = {}
    for sequence in enum.sequences:
        enum.sequenceregexps[sequence] = re.compile(
              enum.sequencepats[sequence] + '$', re.UNICODE)

    grid_table_top_pat = re.compile(r'\+-[-+]+-\+ *$')
    """Matches the top (& bottom) of a full table)."""

    simple_table_top_pat = re.compile('=+( +=+)+ *$')
    """Matches the top of a simple table."""

    simple_table_border_pat = re.compile('=+[ =]*$')
    """Matches the bottom & header bottom of a simple table."""

    pats = {}
    """Fragments of patterns used by transitions."""

    pats['nonalphanum7bit'] = '[!-/:-@[-`{-~]'
    pats['alpha'] = '[a-zA-Z]'
    pats['alphanum'] = '[a-zA-Z0-9]'
    pats['alphanumplus'] = '[a-zA-Z0-9_-]'
    pats['enum'] = ('(%(arabic)s|%(loweralpha)s|%(upperalpha)s|%(lowerroman)s'
                    '|%(upperroman)s|#)' % enum.sequencepats)
    pats['optname'] = '%(alphanum)s%(alphanumplus)s*' % pats
    # @@@ Loosen up the pattern?  Allow Unicode?
    pats['optarg'] = '(%(alpha)s%(alphanumplus)s*|<[^<>]+>)' % pats
    pats['shortopt'] = r'(-|\+)%(alphanum)s( ?%(optarg)s)?' % pats
    pats['longopt'] = r'(--|/)%(optname)s([ =]%(optarg)s)?' % pats
    pats['option'] = r'(%(shortopt)s|%(longopt)s)' % pats

    for format in enum.formats:
        pats[format] = '(?P<%s>%s%s%s)' % (
              format, re.escape(enum.formatinfo[format].prefix),
              pats['enum'], re.escape(enum.formatinfo[format].suffix))

    patterns = {
          'bullet': u'[-+*\u2022\u2023\u2043]( +|$)',
          'enumerator': r'(%(parens)s|%(rparen)s|%(period)s)( +|$)' % pats,
          'field_marker': r':(?![: ])([^:\\]|\\.)*(?<! ):( +|$)',
          'option_marker': r'%(option)s(, %(option)s)*(  +| ?$)' % pats,
          'doctest': r'>>>( +|$)',
          'line_block': r'\|( +|$)',
          'grid_table_top': grid_table_top_pat,
          'simple_table_top': simple_table_top_pat,
          'explicit_markup': r'\.\.( +|$)',
          'anonymous': r'__( +|$)',
          'line': r'(%(nonalphanum7bit)s)\1* *$' % pats,
          'text': r''}
    initial_transitions = (
          'bullet',
          'enumerator',
          'field_marker',
          'option_marker',
          'doctest',
          'line_block',
          'grid_table_top',
          'simple_table_top',
          'explicit_markup',
          'anonymous',
          'line',
          'text')

    def indent(self, match, context, next_state):
        """Block quote."""
        indented, indent, line_offset, blank_finish = \
              self.state_machine.get_indented()
        elements = self.block_quote(indented, line_offset)
        self.parent += elements
        if not blank_finish:
            self.parent += self.unindent_warning('Block quote')
        return context, next_state, []

    def block_quote(self, indented, line_offset):
        elements = []
        while indented:
            (blockquote_lines,
             attribution_lines,
             attribution_offset,
             indented,
             new_line_offset) = self.split_attribution(indented, line_offset)
            blockquote = nodes.block_quote()
            self.nested_parse(blockquote_lines, line_offset, blockquote)
            elements.append(blockquote)
            if attribution_lines:
                attribution, messages = self.parse_attribution(
                    attribution_lines, attribution_offset)
                blockquote += attribution
                elements += messages
            line_offset = new_line_offset
            while indented and not indented[0]:
                indented = indented[1:]
                line_offset += 1
        return elements

    # U+2014 is an em-dash:
    attribution_pattern = re.compile(u'(---?(?!-)|\u2014) *(?=[^ \\n])',
                                     re.UNICODE)

    def split_attribution(self, indented, line_offset):
        """
        Check for a block quote attribution and split it off:

        * First line after a blank line must begin with a dash ("--", "---",
          em-dash; matches `self.attribution_pattern`).
        * Every line after that must have consistent indentation.
        * Attributions must be preceded by block quote content.

        Return a tuple of: (block quote content lines, content offset,
        attribution lines, attribution offset, remaining indented lines).
        """
        blank = None
        nonblank_seen = False
        for i in range(len(indented)):
            line = indented[i].rstrip()
            if line:
                if nonblank_seen and blank == i - 1: # last line blank
                    match = self.attribution_pattern.match(line)
                    if match:
                        attribution_end, indent = self.check_attribution(
                            indented, i)
                        if attribution_end:
                            a_lines = indented[i:attribution_end]
                            a_lines.trim_left(match.end(), end=1)
                            a_lines.trim_left(indent, start=1)
                            return (indented[:i], a_lines,
                                    i, indented[attribution_end:],
                                    line_offset + attribution_end)
                nonblank_seen = True
            else:
                blank = i
        else:
            return (indented, None, None, None, None)

    def check_attribution(self, indented, attribution_start):
        """
        Check attribution shape.
        Return the index past the end of the attribution, and the indent.
        """
        indent = None
        i = attribution_start + 1
        for i in range(attribution_start + 1, len(indented)):
            line = indented[i].rstrip()
            if not line:
                break
            if indent is None:
                indent = len(line) - len(line.lstrip())
            elif len(line) - len(line.lstrip()) != indent:
                return None, None       # bad shape; not an attribution
        else:
            # return index of line after last attribution line:
            i += 1
        return i, (indent or 0)

    def parse_attribution(self, indented, line_offset):
        text = '\n'.join(indented).rstrip()
        lineno = self.state_machine.abs_line_number() + line_offset
        textnodes, messages = self.inline_text(text, lineno)
        node = nodes.attribution(text, '', *textnodes)
        node.source, node.line = self.state_machine.get_source_and_line(lineno)
        return node, messages

    def bullet(self, match, context, next_state):
        """Bullet list item."""
        bulletlist = nodes.bullet_list()
        self.parent += bulletlist
        bulletlist['bullet'] = match.string[0]
        i, blank_finish = self.list_item(match.end())
        bulletlist += i
        offset = self.state_machine.line_offset + 1   # next line
        new_line_offset, blank_finish = self.nested_list_parse(
              self.state_machine.input_lines[offset:],
              input_offset=self.state_machine.abs_line_offset() + 1,
              node=bulletlist, initial_state='BulletList',
              blank_finish=blank_finish)
        self.goto_line(new_line_offset)
        if not blank_finish:
            self.parent += self.unindent_warning('Bullet list')
        return [], next_state, []

    def list_item(self, indent):
        if self.state_machine.line[indent:]:
            indented, line_offset, blank_finish = (
                self.state_machine.get_known_indented(indent))
        else:
            indented, indent, line_offset, blank_finish = (
                self.state_machine.get_first_known_indented(indent))
        listitem = nodes.list_item('\n'.join(indented))
        if indented:
            self.nested_parse(indented, input_offset=line_offset,
                              node=listitem)
        return listitem, blank_finish

    def enumerator(self, match, context, next_state):
        """Enumerated List Item"""
        format, sequence, text, ordinal = self.parse_enumerator(match)
        if not self.is_enumerated_list_item(ordinal, sequence, format):
            raise statemachine.TransitionCorrection('text')
        enumlist = nodes.enumerated_list()
        self.parent += enumlist
        if sequence == '#':
            enumlist['enumtype'] = 'arabic'
        else:
            enumlist['enumtype'] = sequence
        enumlist['prefix'] = self.enum.formatinfo[format].prefix
        enumlist['suffix'] = self.enum.formatinfo[format].suffix
        if ordinal != 1:
            enumlist['start'] = ordinal
            msg = self.reporter.info(
                'Enumerated list start value not ordinal-1: "%s" (ordinal %s)'
                % (text, ordinal))
            self.parent += msg
        listitem, blank_finish = self.list_item(match.end())
        enumlist += listitem
        offset = self.state_machine.line_offset + 1   # next line
        newline_offset, blank_finish = self.nested_list_parse(
              self.state_machine.input_lines[offset:],
              input_offset=self.state_machine.abs_line_offset() + 1,
              node=enumlist, initial_state='EnumeratedList',
              blank_finish=blank_finish,
              extra_settings={'lastordinal': ordinal,
                              'format': format,
                              'auto': sequence == '#'})
        self.goto_line(newline_offset)
        if not blank_finish:
            self.parent += self.unindent_warning('Enumerated list')
        return [], next_state, []

    def parse_enumerator(self, match, expected_sequence=None):
        """
        Analyze an enumerator and return the results.

        :Return:
            - the enumerator format ('period', 'parens', or 'rparen'),
            - the sequence used ('arabic', 'loweralpha', 'upperroman', etc.),
            - the text of the enumerator, stripped of formatting, and
            - the ordinal value of the enumerator ('a' -> 1, 'ii' -> 2, etc.;
              ``None`` is returned for invalid enumerator text).

        The enumerator format has already been determined by the regular
        expression match. If `expected_sequence` is given, that sequence is
        tried first. If not, we check for Roman numeral 1. This way,
        single-character Roman numerals (which are also alphabetical) can be
        matched. If no sequence has been matched, all sequences are checked in
        order.
        """
        groupdict = match.groupdict()
        sequence = ''
        for format in self.enum.formats:
            if groupdict[format]:       # was this the format matched?
                break                   # yes; keep `format`
        else:                           # shouldn't happen
            raise ParserError('enumerator format not matched')
        text = groupdict[format][self.enum.formatinfo[format].start
                                 :self.enum.formatinfo[format].end]
        if text == '#':
            sequence = '#'
        elif expected_sequence:
            try:
                if self.enum.sequenceregexps[expected_sequence].match(text):
                    sequence = expected_sequence
            except KeyError:            # shouldn't happen
                raise ParserError('unknown enumerator sequence: %s'
                                  % sequence)
        elif text == 'i':
            sequence = 'lowerroman'
        elif text == 'I':
            sequence = 'upperroman'
        if not sequence:
            for sequence in self.enum.sequences:
                if self.enum.sequenceregexps[sequence].match(text):
                    break
            else:                       # shouldn't happen
                raise ParserError('enumerator sequence not matched')
        if sequence == '#':
            ordinal = 1
        else:
            try:
                ordinal = self.enum.converters[sequence](text)
            except roman.InvalidRomanNumeralError:
                ordinal = None
        return format, sequence, text, ordinal

    def is_enumerated_list_item(self, ordinal, sequence, format):
        """
        Check validity based on the ordinal value and the second line.

        Return true if the ordinal is valid and the second line is blank,
        indented, or starts with the next enumerator or an auto-enumerator.
        """
        if ordinal is None:
            return None
        try:
            next_line = self.state_machine.next_line()
        except EOFError:              # end of input lines
            self.state_machine.previous_line()
            return 1
        else:
            self.state_machine.previous_line()
        if not next_line[:1].strip():   # blank or indented
            return 1
        result = self.make_enumerator(ordinal + 1, sequence, format)
        if result:
            next_enumerator, auto_enumerator = result
            try:
                if ( next_line.startswith(next_enumerator) or
                     next_line.startswith(auto_enumerator) ):
                    return 1
            except TypeError:
                pass
        return None

    def make_enumerator(self, ordinal, sequence, format):
        """
        Construct and return the next enumerated list item marker, and an
        auto-enumerator ("#" instead of the regular enumerator).

        Return ``None`` for invalid (out of range) ordinals.
        """ #"
        if sequence == '#':
            enumerator = '#'
        elif sequence == 'arabic':
            enumerator = str(ordinal)
        else:
            if sequence.endswith('alpha'):
                if ordinal > 26:
                    return None
                enumerator = chr(ordinal + ord('a') - 1)
            elif sequence.endswith('roman'):
                try:
                    enumerator = roman.toRoman(ordinal)
                except roman.RomanError:
                    return None
            else:                       # shouldn't happen
                raise ParserError('unknown enumerator sequence: "%s"'
                                  % sequence)
            if sequence.startswith('lower'):
                enumerator = enumerator.lower()
            elif sequence.startswith('upper'):
                enumerator = enumerator.upper()
            else:                       # shouldn't happen
                raise ParserError('unknown enumerator sequence: "%s"'
                                  % sequence)
        formatinfo = self.enum.formatinfo[format]
        next_enumerator = (formatinfo.prefix + enumerator + formatinfo.suffix
                           + ' ')
        auto_enumerator = formatinfo.prefix + '#' + formatinfo.suffix + ' '
        return next_enumerator, auto_enumerator

    def field_marker(self, match, context, next_state):
        """Field list item."""
        field_list = nodes.field_list()
        self.parent += field_list
        field, blank_finish = self.field(match)
        field_list += field
        offset = self.state_machine.line_offset + 1   # next line
        newline_offset, blank_finish = self.nested_list_parse(
              self.state_machine.input_lines[offset:],
              input_offset=self.state_machine.abs_line_offset() + 1,
              node=field_list, initial_state='FieldList',
              blank_finish=blank_finish)
        self.goto_line(newline_offset)
        if not blank_finish:
            self.parent += self.unindent_warning('Field list')
        return [], next_state, []

    def field(self, match):
        name = self.parse_field_marker(match)
        src, srcline = self.state_machine.get_source_and_line()
        lineno = self.state_machine.abs_line_number()
        indented, indent, line_offset, blank_finish = \
              self.state_machine.get_first_known_indented(match.end())
        field_node = nodes.field()
        field_node.source = src
        field_node.line = srcline
        name_nodes, name_messages = self.inline_text(name, lineno)
        field_node += nodes.field_name(name, '', *name_nodes)
        field_body = nodes.field_body('\n'.join(indented), *name_messages)
        field_node += field_body
        if indented:
            self.parse_field_body(indented, line_offset, field_body)
        return field_node, blank_finish

    def parse_field_marker(self, match):
        """Extract & return field name from a field marker match."""
        field = match.group()[1:]        # strip off leading ':'
        field = field[:field.rfind(':')] # strip off trailing ':' etc.
        return field

    def parse_field_body(self, indented, offset, node):
        self.nested_parse(indented, input_offset=offset, node=node)

    def option_marker(self, match, context, next_state):
        """Option list item."""
        optionlist = nodes.option_list()
        try:
            listitem, blank_finish = self.option_list_item(match)
        except MarkupError as error:
            # This shouldn't happen; pattern won't match.
            msg = self.reporter.error(u'Invalid option list marker: %s' %
                                      error)
            self.parent += msg
            indented, indent, line_offset, blank_finish = \
                  self.state_machine.get_first_known_indented(match.end())
            elements = self.block_quote(indented, line_offset)
            self.parent += elements
            if not blank_finish:
                self.parent += self.unindent_warning('Option list')
            return [], next_state, []
        self.parent += optionlist
        optionlist += listitem
        offset = self.state_machine.line_offset + 1   # next line
        newline_offset, blank_finish = self.nested_list_parse(
              self.state_machine.input_lines[offset:],
              input_offset=self.state_machine.abs_line_offset() + 1,
              node=optionlist, initial_state='OptionList',
              blank_finish=blank_finish)
        self.goto_line(newline_offset)
        if not blank_finish:
            self.parent += self.unindent_warning('Option list')
        return [], next_state, []

    def option_list_item(self, match):
        offset = self.state_machine.abs_line_offset()
        options = self.parse_option_marker(match)
        indented, indent, line_offset, blank_finish = \
              self.state_machine.get_first_known_indented(match.end())
        if not indented:                # not an option list item
            self.goto_line(offset)
            raise statemachine.TransitionCorrection('text')
        option_group = nodes.option_group('', *options)
        description = nodes.description('\n'.join(indented))
        option_list_item = nodes.option_list_item('', option_group,
                                                  description)
        if indented:
            self.nested_parse(indented, input_offset=line_offset,
                              node=description)
        return option_list_item, blank_finish

    def parse_option_marker(self, match):
        """
        Return a list of `node.option` and `node.option_argument` objects,
        parsed from an option marker match.

        :Exception: `MarkupError` for invalid option markers.
        """
        optlist = []
        optionstrings = match.group().rstrip().split(', ')
        for optionstring in optionstrings:
            tokens = optionstring.split()
            delimiter = ' '
            firstopt = tokens[0].split('=', 1)
            if len(firstopt) > 1:
                # "--opt=value" form
                tokens[:1] = firstopt
                delimiter = '='
            elif (len(tokens[0]) > 2
                  and ((tokens[0].startswith('-')
                        and not tokens[0].startswith('--'))
                       or tokens[0].startswith('+'))):
                # "-ovalue" form
                tokens[:1] = [tokens[0][:2], tokens[0][2:]]
                delimiter = ''
            if len(tokens) > 1 and (tokens[1].startswith('<')
                                    and tokens[-1].endswith('>')):
                # "-o <value1 value2>" form; join all values into one token
                tokens[1:] = [' '.join(tokens[1:])]
            if 0 < len(tokens) <= 2:
                option = nodes.option(optionstring)
                option += nodes.option_string(tokens[0], tokens[0])
                if len(tokens) > 1:
                    option += nodes.option_argument(tokens[1], tokens[1],
                                                    delimiter=delimiter)
                optlist.append(option)
            else:
                raise MarkupError(
                    'wrong number of option tokens (=%s), should be 1 or 2: '
                    '"%s"' % (len(tokens), optionstring))
        return optlist

    def doctest(self, match, context, next_state):
        data = '\n'.join(self.state_machine.get_text_block())
        self.parent += nodes.doctest_block(data, data)
        return [], next_state, []

    def line_block(self, match, context, next_state):
        """First line of a line block."""
        block = nodes.line_block()
        self.parent += block
        lineno = self.state_machine.abs_line_number()
        line, messages, blank_finish = self.line_block_line(match, lineno)
        block += line
        self.parent += messages
        if not blank_finish:
            offset = self.state_machine.line_offset + 1   # next line
            new_line_offset, blank_finish = self.nested_list_parse(
                  self.state_machine.input_lines[offset:],
                  input_offset=self.state_machine.abs_line_offset() + 1,
                  node=block, initial_state='LineBlock',
                  blank_finish=0)
            self.goto_line(new_line_offset)
        if not blank_finish:
            self.parent += self.reporter.warning(
                'Line block ends without a blank line.',
                line=lineno+1)
        if len(block):
            if block[0].indent is None:
                block[0].indent = 0
            self.nest_line_block_lines(block)
        return [], next_state, []

    def line_block_line(self, match, lineno):
        """Return one line element of a line_block."""
        indented, indent, line_offset, blank_finish = \
            self.state_machine.get_first_known_indented(match.end(),
                                                        until_blank=True)
        text = u'\n'.join(indented)
        text_nodes, messages = self.inline_text(text, lineno)
        line = nodes.line(text, '', *text_nodes)
        if match.string.rstrip() != '|': # not empty
            line.indent = len(match.group(1)) - 1
        return line, messages, blank_finish

    def nest_line_block_lines(self, block):
        for index in range(1, len(block)):
            if block[index].indent is None:
                block[index].indent = block[index - 1].indent
        self.nest_line_block_segment(block)

    def nest_line_block_segment(self, block):
        indents = [item.indent for item in block]
        least = min(indents)
        new_items = []
        new_block = nodes.line_block()
        for item in block:
            if item.indent > least:
                new_block.append(item)
            else:
                if len(new_block):
                    self.nest_line_block_segment(new_block)
                    new_items.append(new_block)
                    new_block = nodes.line_block()
                new_items.append(item)
        if len(new_block):
            self.nest_line_block_segment(new_block)
            new_items.append(new_block)
        block[:] = new_items

    def grid_table_top(self, match, context, next_state):
        """Top border of a full table."""
        return self.table_top(match, context, next_state,
                              self.isolate_grid_table,
                              tableparser.GridTableParser)

    def simple_table_top(self, match, context, next_state):
        """Top border of a simple table."""
        return self.table_top(match, context, next_state,
                              self.isolate_simple_table,
                              tableparser.SimpleTableParser)

    def table_top(self, match, context, next_state,
                  isolate_function, parser_class):
        """Top border of a generic table."""
        nodelist, blank_finish = self.table(isolate_function, parser_class)
        self.parent += nodelist
        if not blank_finish:
            msg = self.reporter.warning(
                'Blank line required after table.',
                line=self.state_machine.abs_line_number()+1)
            self.parent += msg
        return [], next_state, []

    def table(self, isolate_function, parser_class):
        """Parse a table."""
        block, messages, blank_finish = isolate_function()
        if block:
            try:
                parser = parser_class()
                tabledata = parser.parse(block)
                tableline = (self.state_machine.abs_line_number() - len(block)
                             + 1)
                table = self.build_table(tabledata, tableline)
                nodelist = [table] + messages
            except tableparser.TableMarkupError as err:
                nodelist = self.malformed_table(block, ' '.join(err.args),
                                                offset=err.offset) + messages
        else:
            nodelist = messages
        return nodelist, blank_finish

    def isolate_grid_table(self):
        messages = []
        blank_finish = 1
        try:
            block = self.state_machine.get_text_block(flush_left=True)
        except statemachine.UnexpectedIndentationError as err:
            block, src, srcline = err.args
            messages.append(self.reporter.error('Unexpected indentation.',
                                                source=src, line=srcline))
            blank_finish = 0
        block.disconnect()
        # for East Asian chars:
        block.pad_double_width(self.double_width_pad_char)
        width = len(block[0].strip())
        for i in range(len(block)):
            block[i] = block[i].strip()
            if block[i][0] not in '+|': # check left edge
                blank_finish = 0
                self.state_machine.previous_line(len(block) - i)
                del block[i:]
                break
        if not self.grid_table_top_pat.match(block[-1]): # find bottom
            blank_finish = 0
            # from second-last to third line of table:
            for i in range(len(block) - 2, 1, -1):
                if self.grid_table_top_pat.match(block[i]):
                    self.state_machine.previous_line(len(block) - i + 1)
                    del block[i+1:]
                    break
            else:
                messages.extend(self.malformed_table(block))
                return [], messages, blank_finish
        for i in range(len(block)):     # check right edge
            if len(block[i]) != width or block[i][-1] not in '+|':
                messages.extend(self.malformed_table(block))
                return [], messages, blank_finish
        return block, messages, blank_finish

    def isolate_simple_table(self):
        start = self.state_machine.line_offset
        lines = self.state_machine.input_lines
        limit = len(lines) - 1
        toplen = len(lines[start].strip())
        pattern_match = self.simple_table_border_pat.match
        found = 0
        found_at = None
        i = start + 1
        while i <= limit:
            line = lines[i]
            match = pattern_match(line)
            if match:
                if len(line.strip()) != toplen:
                    self.state_machine.next_line(i - start)
                    messages = self.malformed_table(
                        lines[start:i+1], 'Bottom/header table border does '
                        'not match top border.')
                    return [], messages, i == limit or not lines[i+1].strip()
                found += 1
                found_at = i
                if found == 2 or i == limit or not lines[i+1].strip():
                    end = i
                    break
            i += 1
        else:                           # reached end of input_lines
            if found:
                extra = ' or no blank line after table bottom'
                self.state_machine.next_line(found_at - start)
                block = lines[start:found_at+1]
            else:
                extra = ''
                self.state_machine.next_line(i - start - 1)
                block = lines[start:]
            messages = self.malformed_table(
                block, 'No bottom table border found%s.' % extra)
            return [], messages, not extra
        self.state_machine.next_line(end - start)
        block = lines[start:end+1]
        # for East Asian chars:
        block.pad_double_width(self.double_width_pad_char)
        return block, [], end == limit or not lines[end+1].strip()

    def malformed_table(self, block, detail='', offset=0):
        block.replace(self.double_width_pad_char, '')
        data = '\n'.join(block)
        message = 'Malformed table.'
        startline = self.state_machine.abs_line_number() - len(block) + 1
        if detail:
            message += '\n' + detail
        error = self.reporter.error(message, nodes.literal_block(data, data),
                                    line=startline+offset)
        return [error]

    def build_table(self, tabledata, tableline, stub_columns=0):
        colwidths, headrows, bodyrows = tabledata
        table = nodes.table()
        tgroup = nodes.tgroup(cols=len(colwidths))
        table += tgroup
        for colwidth in colwidths:
            colspec = nodes.colspec(colwidth=colwidth)
            if stub_columns:
                colspec.attributes['stub'] = 1
                stub_columns -= 1
            tgroup += colspec
        if headrows:
            thead = nodes.thead()
            tgroup += thead
            for row in headrows:
                thead += self.build_table_row(row, tableline)
        tbody = nodes.tbody()
        tgroup += tbody
        for row in bodyrows:
            tbody += self.build_table_row(row, tableline)
        return table

    def build_table_row(self, rowdata, tableline):
        row = nodes.row()
        for cell in rowdata:
            if cell is None:
                continue
            morerows, morecols, offset, cellblock = cell
            attributes = {}
            if morerows:
                attributes['morerows'] = morerows
            if morecols:
                attributes['morecols'] = morecols
            entry = nodes.entry(**attributes)
            row += entry
            if ''.join(cellblock):
                self.nested_parse(cellblock, input_offset=tableline+offset,
                                  node=entry)
        return row


    explicit = Struct()
    """Patterns and constants used for explicit markup recognition."""

    explicit.patterns = Struct(
          target=re.compile(r"""
                            (
                              _               # anonymous target
                            |               # *OR*
                              (?!_)           # no underscore at the beginning
                              (?P<quote>`?)   # optional open quote
                              (?![ `])        # first char. not space or
                                              # backquote
                              (?P<name>       # reference name
                                .+?
                              )
                              %(non_whitespace_escape_before)s
                              (?P=quote)      # close quote if open quote used
                            )
                            (?<!(?<!\x00):) # no unescaped colon at end
                            %(non_whitespace_escape_before)s
                            [ ]?            # optional space
                            :               # end of reference name
                            ([ ]+|$)        # followed by whitespace
                            """ % vars(Inliner), re.VERBOSE | re.UNICODE),
          reference=re.compile(r"""
                               (
                                 (?P<simple>%(simplename)s)_
                               |                  # *OR*
                                 `                  # open backquote
                                 (?![ ])            # not space
                                 (?P<phrase>.+?)    # hyperlink phrase
                                 %(non_whitespace_escape_before)s
                                 `_                 # close backquote,
                                                    # reference mark
                               )
                               $                  # end of string
                               """ % vars(Inliner), re.VERBOSE | re.UNICODE),
          substitution=re.compile(r"""
                                  (
                                    (?![ ])          # first char. not space
                                    (?P<name>.+?)    # substitution text
                                    %(non_whitespace_escape_before)s
                                    \|               # close delimiter
                                  )
                                  ([ ]+|$)           # followed by whitespace
                                  """ % vars(Inliner),
                                  re.VERBOSE | re.UNICODE),)

    def footnote(self, match):
        src, srcline = self.state_machine.get_source_and_line()
        indented, indent, offset, blank_finish = \
              self.state_machine.get_first_known_indented(match.end())
        label = match.group(1)
        name = normalize_name(label)
        footnote = nodes.footnote('\n'.join(indented))
        footnote.source = src
        footnote.line = srcline
        if name[0] == '#':              # auto-numbered
            name = name[1:]             # autonumber label
            footnote['auto'] = 1
            if name:
                footnote['names'].append(name)
            self.document.note_autofootnote(footnote)
        elif name == '*':               # auto-symbol
            name = ''
            footnote['auto'] = '*'
            self.document.note_symbol_footnote(footnote)
        else:                           # manually numbered
            footnote += nodes.label('', label)
            footnote['names'].append(name)
            self.document.note_footnote(footnote)
        if name:
            self.document.note_explicit_target(footnote, footnote)
        else:
            self.document.set_id(footnote, footnote)
        if indented:
            self.nested_parse(indented, input_offset=offset, node=footnote)
        return [footnote], blank_finish

    def citation(self, match):
        src, srcline = self.state_machine.get_source_and_line()
        indented, indent, offset, blank_finish = \
              self.state_machine.get_first_known_indented(match.end())
        label = match.group(1)
        name = normalize_name(label)
        citation = nodes.citation('\n'.join(indented))
        citation.source = src
        citation.line = srcline
        citation += nodes.label('', label)
        citation['names'].append(name)
        self.document.note_citation(citation)
        self.document.note_explicit_target(citation, citation)
        if indented:
            self.nested_parse(indented, input_offset=offset, node=citation)
        return [citation], blank_finish

    def hyperlink_target(self, match):
        pattern = self.explicit.patterns.target
        lineno = self.state_machine.abs_line_number()
        block, indent, offset, blank_finish = \
              self.state_machine.get_first_known_indented(
              match.end(), until_blank=True, strip_indent=False)
        blocktext = match.string[:match.end()] + '\n'.join(block)
        block = [escape2null(line) for line in block]
        escaped = block[0]
        blockindex = 0
        while True:
            targetmatch = pattern.match(escaped)
            if targetmatch:
                break
            blockindex += 1
            try:
                escaped += block[blockindex]
            except IndexError:
                raise MarkupError('malformed hyperlink target.')
        del block[:blockindex]
        block[0] = (block[0] + ' ')[targetmatch.end()-len(escaped)-1:].strip()
        target = self.make_target(block, blocktext, lineno,
                                  targetmatch.group('name'))
        return [target], blank_finish

    def make_target(self, block, block_text, lineno, target_name):
        target_type, data = self.parse_target(block, block_text, lineno)
        if target_type == 'refname':
            target = nodes.target(block_text, '', refname=normalize_name(data))
            target.indirect_reference_name = data
            self.add_target(target_name, '', target, lineno)
            self.document.note_indirect_target(target)
            return target
        elif target_type == 'refuri':
            target = nodes.target(block_text, '')
            self.add_target(target_name, data, target, lineno)
            return target
        else:
            return data

    def parse_target(self, block, block_text, lineno):
        """
        Determine the type of reference of a target.

        :Return: A 2-tuple, one of:

            - 'refname' and the indirect reference name
            - 'refuri' and the URI
            - 'malformed' and a system_message node
        """
        if block and block[-1].strip()[-1:] == '_': # possible indirect target
            reference = ' '.join([line.strip() for line in block])
            refname = self.is_reference(reference)
            if refname:
                return 'refname', refname
        reference = ''.join([''.join(line.split()) for line in block])
        return 'refuri', unescape(reference)

    def is_reference(self, reference):
        match = self.explicit.patterns.reference.match(
            whitespace_normalize_name(reference))
        if not match:
            return None
        return unescape(match.group('simple') or match.group('phrase'))

    def add_target(self, targetname, refuri, target, lineno):
        target.line = lineno
        if targetname:
            name = normalize_name(unescape(targetname))
            target['names'].append(name)
            if refuri:
                uri = self.inliner.adjust_uri(refuri)
                if uri:
                    target['refuri'] = uri
                else:
                    raise ApplicationError('problem with URI: %r' % refuri)
            self.document.note_explicit_target(target, self.parent)
        else:                       # anonymous target
            if refuri:
                target['refuri'] = refuri
            target['anonymous'] = 1
            self.document.note_anonymous_target(target)

    def substitution_def(self, match):
        pattern = self.explicit.patterns.substitution
        src, srcline = self.state_machine.get_source_and_line()
        block, indent, offset, blank_finish = \
              self.state_machine.get_first_known_indented(match.end(),
                                                          strip_indent=False)
        blocktext = (match.string[:match.end()] + '\n'.join(block))
        block.disconnect()
        escaped = escape2null(block[0].rstrip())
        blockindex = 0
        while True:
            subdefmatch = pattern.match(escaped)
            if subdefmatch:
                break
            blockindex += 1
            try:
                escaped = escaped + ' ' + escape2null(block[blockindex].strip())
            except IndexError:
                raise MarkupError('malformed substitution definition.')
        del block[:blockindex]          # strip out the substitution marker
        block[0] = (block[0].strip() + ' ')[subdefmatch.end()-len(escaped)-1:-1]
        if not block[0]:
            del block[0]
            offset += 1
        while block and not block[-1].strip():
            block.pop()
        subname = subdefmatch.group('name')
        substitution_node = nodes.substitution_definition(blocktext)
        substitution_node.source = src
        substitution_node.line = srcline
        if not block:
            msg = self.reporter.warning(
                'Substitution definition "%s" missing contents.' % subname,
                nodes.literal_block(blocktext, blocktext),
                source=src, line=srcline)
            return [msg], blank_finish
        block[0] = block[0].strip()
        substitution_node['names'].append(
            nodes.whitespace_normalize_name(subname))
        new_abs_offset, blank_finish = self.nested_list_parse(
              block, input_offset=offset, node=substitution_node,
              initial_state='SubstitutionDef', blank_finish=blank_finish)
        i = 0
        for node in substitution_node[:]:
            if not (isinstance(node, nodes.Inline) or
                    isinstance(node, nodes.Text)):
                self.parent += substitution_node[i]
                del substitution_node[i]
            else:
                i += 1
        for node in substitution_node.traverse(nodes.Element):
            if self.disallowed_inside_substitution_definitions(node):
                pformat = nodes.literal_block('', node.pformat().rstrip())
                msg = self.reporter.error(
                    'Substitution definition contains illegal element:',
                    pformat, nodes.literal_block(blocktext, blocktext),
                    source=src, line=srcline)
                return [msg], blank_finish
        if len(substitution_node) == 0:
            msg = self.reporter.warning(
                  'Substitution definition "%s" empty or invalid.' % subname,
                  nodes.literal_block(blocktext, blocktext),
                  source=src, line=srcline)
            return [msg], blank_finish
        self.document.note_substitution_def(
            substitution_node, subname, self.parent)
        return [substitution_node], blank_finish

    def disallowed_inside_substitution_definitions(self, node):
        if (node['ids'] or
            isinstance(node, nodes.reference) and node.get('anonymous') or
            isinstance(node, nodes.footnote_reference) and node.get('auto')):
            return 1
        else:
            return 0

    def directive(self, match, **option_presets):
        """Returns a 2-tuple: list of nodes, and a "blank finish" boolean."""
        type_name = match.group(1)
        directive_class, messages = directives.directive(
            type_name, self.memo.language, self.document)
        self.parent += messages
        if directive_class:
            return self.run_directive(
                directive_class, match, type_name, option_presets)
        else:
            return self.unknown_directive(type_name)

    def run_directive(self, directive, match, type_name, option_presets):
        """
        Parse a directive then run its directive function.

        Parameters:

        - `directive`: The class implementing the directive.  Must be
          a subclass of `rst.Directive`.

        - `match`: A regular expression match object which matched the first
          line of the directive.

        - `type_name`: The directive name, as used in the source text.

        - `option_presets`: A dictionary of preset options, defaults for the
          directive options.  Currently, only an "alt" option is passed by
          substitution definitions (value: the substitution name), which may
          be used by an embedded image directive.

        Returns a 2-tuple: list of nodes, and a "blank finish" boolean.
        """
        if isinstance(directive, (FunctionType, MethodType)):
            from docutils.parsers.rst import convert_directive_function
            directive = convert_directive_function(directive)
        lineno = self.state_machine.abs_line_number()
        initial_line_offset = self.state_machine.line_offset
        indented, indent, line_offset, blank_finish \
                  = self.state_machine.get_first_known_indented(match.end(),
                                                                strip_top=0)
        block_text = '\n'.join(self.state_machine.input_lines[
            initial_line_offset : self.state_machine.line_offset + 1])
        try:
            arguments, options, content, content_offset = (
                self.parse_directive_block(indented, line_offset,
                                           directive, option_presets))
        except MarkupError as detail:
            error = self.reporter.error(
                'Error in "%s" directive:\n%s.' % (type_name,
                                                   ' '.join(detail.args)),
                nodes.literal_block(block_text, block_text), line=lineno)
            return [error], blank_finish
        directive_instance = directive(
            type_name, arguments, options, content, lineno,
            content_offset, block_text, self, self.state_machine)
        try:
            result = directive_instance.run()
        except docutils.parsers.rst.DirectiveError as error:
            msg_node = self.reporter.system_message(error.level, error.msg,
                                                    line=lineno)
            msg_node += nodes.literal_block(block_text, block_text)
            result = [msg_node]
        assert isinstance(result, list), \
               'Directive "%s" must return a list of nodes.' % type_name
        for i in range(len(result)):
            assert isinstance(result[i], nodes.Node), \
                   ('Directive "%s" returned non-Node object (index %s): %r'
                    % (type_name, i, result[i]))
        return (result,
                blank_finish or self.state_machine.is_next_line_blank())

    def parse_directive_block(self, indented, line_offset, directive,
                              option_presets):
        option_spec = directive.option_spec
        has_content = directive.has_content
        if indented and not indented[0].strip():
            indented.trim_start()
            line_offset += 1
        while indented and not indented[-1].strip():
            indented.trim_end()
        if indented and (directive.required_arguments
                         or directive.optional_arguments
                         or option_spec):
            for i, line in enumerate(indented):
                if not line.strip():
                    break
            else:
                i += 1
            arg_block = indented[:i]
            content = indented[i+1:]
            content_offset = line_offset + i + 1
        else:
            content = indented
            content_offset = line_offset
            arg_block = []
        if option_spec:
            options, arg_block = self.parse_directive_options(
                option_presets, option_spec, arg_block)
        else:
            options = {}
        if arg_block and not (directive.required_arguments
                              or directive.optional_arguments):
            content = arg_block + indented[i:]
            content_offset = line_offset
            arg_block = []
        while content and not content[0].strip():
            content.trim_start()
            content_offset += 1
        if directive.required_arguments or directive.optional_arguments:
            arguments = self.parse_directive_arguments(
                directive, arg_block)
        else:
            arguments = []
        if content and not has_content:
            raise MarkupError('no content permitted')
        return (arguments, options, content, content_offset)

    def parse_directive_options(self, option_presets, option_spec, arg_block):
        options = option_presets.copy()
        for i, line in enumerate(arg_block):
            if re.match(Body.patterns['field_marker'], line):
                opt_block = arg_block[i:]
                arg_block = arg_block[:i]
                break
        else:
            opt_block = []
        if opt_block:
            success, data = self.parse_extension_options(option_spec,
                                                         opt_block)
            if success:                 # data is a dict of options
                options.update(data)
            else:                       # data is an error string
                raise MarkupError(data)
        return options, arg_block

    def parse_directive_arguments(self, directive, arg_block):
        required = directive.required_arguments
        optional = directive.optional_arguments
        arg_text = '\n'.join(arg_block)
        arguments = arg_text.split()
        if len(arguments) < required:
            raise MarkupError('%s argument(s) required, %s supplied'
                              % (required, len(arguments)))
        elif len(arguments) > required + optional:
            if directive.final_argument_whitespace:
                arguments = arg_text.split(None, required + optional - 1)
            else:
                raise MarkupError(
                    'maximum %s argument(s) allowed, %s supplied'
                    % (required + optional, len(arguments)))
        return arguments

    def parse_extension_options(self, option_spec, datalines):
        """
        Parse `datalines` for a field list containing extension options
        matching `option_spec`.

        :Parameters:
            - `option_spec`: a mapping of option name to conversion
              function, which should raise an exception on bad input.
            - `datalines`: a list of input strings.

        :Return:
            - Success value, 1 or 0.
            - An option dictionary on success, an error string on failure.
        """
        node = nodes.field_list()
        newline_offset, blank_finish = self.nested_list_parse(
              datalines, 0, node, initial_state='ExtensionOptions',
              blank_finish=True)
        if newline_offset != len(datalines): # incomplete parse of block
            return 0, 'invalid option block'
        try:
            options = utils.extract_extension_options(node, option_spec)
        except KeyError as detail:
            return 0, ('unknown option: "%s"' % detail.args[0])
        except (ValueError, TypeError) as detail:
            return 0, ('invalid option value: %s' % ' '.join(detail.args))
        except utils.ExtensionOptionError as detail:
            return 0, ('invalid option data: %s' % ' '.join(detail.args))
        if blank_finish:
            return 1, options
        else:
            return 0, 'option data incompletely parsed'

    def unknown_directive(self, type_name):
        lineno = self.state_machine.abs_line_number()
        indented, indent, offset, blank_finish = \
            self.state_machine.get_first_known_indented(0, strip_indent=False)
        text = '\n'.join(indented)
        error = self.reporter.error(
              'Unknown directive type "%s".' % type_name,
              nodes.literal_block(text, text), line=lineno)
        return [error], blank_finish

    def comment(self, match):
        if not match.string[match.end():].strip() \
              and self.state_machine.is_next_line_blank(): # an empty comment?
            return [nodes.comment()], 1 # "A tiny but practical wart."
        indented, indent, offset, blank_finish = \
              self.state_machine.get_first_known_indented(match.end())
        while indented and not indented[-1].strip():
            indented.trim_end()
        text = '\n'.join(indented)
        return [nodes.comment(text, text)], blank_finish

    explicit.constructs = [
          (footnote,
           re.compile(r"""
                      \.\.[ ]+          # explicit markup start
                      \[
                      (                 # footnote label:
                          [0-9]+          # manually numbered footnote
                        |               # *OR*
                          \#              # anonymous auto-numbered footnote
                        |               # *OR*
                          \#%s            # auto-number ed?) footnote label
                        |               # *OR*
                          \*              # auto-symbol footnote
                      )
                      \]
                      ([ ]+|$)          # whitespace or end of line
                      """ % Inliner.simplename, re.VERBOSE | re.UNICODE)),
          (citation,
           re.compile(r"""
                      \.\.[ ]+          # explicit markup start
                      \[(%s)\]          # citation label
                      ([ ]+|$)          # whitespace or end of line
                      """ % Inliner.simplename, re.VERBOSE | re.UNICODE)),
          (hyperlink_target,
           re.compile(r"""
                      \.\.[ ]+          # explicit markup start
                      _                 # target indicator
                      (?![ ]|$)         # first char. not space or EOL
                      """, re.VERBOSE | re.UNICODE)),
          (substitution_def,
           re.compile(r"""
                      \.\.[ ]+          # explicit markup start
                      \|                # substitution indicator
                      (?![ ]|$)         # first char. not space or EOL
                      """, re.VERBOSE | re.UNICODE)),
          (directive,
           re.compile(r"""
                      \.\.[ ]+          # explicit markup start
                      (%s)              # directive name
                      [ ]?              # optional space
                      ::                # directive delimiter
                      ([ ]+|$)          # whitespace or end of line
                      """ % Inliner.simplename, re.VERBOSE | re.UNICODE))]

    def explicit_markup(self, match, context, next_state):
        """Footnotes, hyperlink targets, directives, comments."""
        nodelist, blank_finish = self.explicit_construct(match)
        self.parent += nodelist
        self.explicit_list(blank_finish)
        return [], next_state, []

    def explicit_construct(self, match):
        """Determine which explicit construct this is, parse & return it."""
        errors = []
        for method, pattern in self.explicit.constructs:
            expmatch = pattern.match(match.string)
            if expmatch:
                try:
                    return method(self, expmatch)
                except MarkupError as error:
                    lineno = self.state_machine.abs_line_number()
                    message = ' '.join(error.args)
                    errors.append(self.reporter.warning(message, line=lineno))
                    break
        nodelist, blank_finish = self.comment(match)
        return nodelist + errors, blank_finish

    def explicit_list(self, blank_finish):
        """
        Create a nested state machine for a series of explicit markup
        constructs (including anonymous hyperlink targets).
        """
        offset = self.state_machine.line_offset + 1   # next line
        newline_offset, blank_finish = self.nested_list_parse(
              self.state_machine.input_lines[offset:],
              input_offset=self.state_machine.abs_line_offset() + 1,
              node=self.parent, initial_state='Explicit',
              blank_finish=blank_finish,
              match_titles=self.state_machine.match_titles)
        self.goto_line(newline_offset)
        if not blank_finish:
            self.parent += self.unindent_warning('Explicit markup')

    def anonymous(self, match, context, next_state):
        """Anonymous hyperlink targets."""
        nodelist, blank_finish = self.anonymous_target(match)
        self.parent += nodelist
        self.explicit_list(blank_finish)
        return [], next_state, []

    def anonymous_target(self, match):
        lineno = self.state_machine.abs_line_number()
        block, indent, offset, blank_finish \
            = self.state_machine.get_first_known_indented(match.end(),
                                                        until_blank=True)
        blocktext = match.string[:match.end()] + '\n'.join(block)
        block = [escape2null(line) for line in block]
        target = self.make_target(block, blocktext, lineno, '')
        return [target], blank_finish

    def line(self, match, context, next_state):
        """Section title overline or transition marker."""
        if self.state_machine.match_titles:
            return [match.string], 'Line', []
        elif match.string.strip() == '::':
            raise statemachine.TransitionCorrection('text')
        elif len(match.string.strip()) < 4:
            msg = self.reporter.info(
                'Unexpected possible title overline or transition.\n'
                "Treating it as ordinary text because it's so short.",
                line=self.state_machine.abs_line_number())
            self.parent += msg
            raise statemachine.TransitionCorrection('text')
        else:
            blocktext = self.state_machine.line
            msg = self.reporter.severe(
                  'Unexpected section title or transition.',
                  nodes.literal_block(blocktext, blocktext),
                  line=self.state_machine.abs_line_number())
            self.parent += msg
            return [], next_state, []

    def text(self, match, context, next_state):
        """Titles, definition lists, paragraphs."""
        return [match.string], 'Text', []


class RFC2822Body(Body):

    """
    RFC2822 headers are only valid as the first constructs in documents.  As
    soon as anything else appears, the `Body` state should take over.
    """

    patterns = Body.patterns.copy()     # can't modify the original
    patterns['rfc2822'] = r'[!-9;-~]+:( +|$)'
    initial_transitions = [(name, 'Body')
                           for name in Body.initial_transitions]
    initial_transitions.insert(-1, ('rfc2822', 'Body')) # just before 'text'

    def rfc2822(self, match, context, next_state):
        """RFC2822-style field list item."""
        fieldlist = nodes.field_list(classes=['rfc2822'])
        self.parent += fieldlist
        field, blank_finish = self.rfc2822_field(match)
        fieldlist += field
        offset = self.state_machine.line_offset + 1   # next line
        newline_offset, blank_finish = self.nested_list_parse(
              self.state_machine.input_lines[offset:],
              input_offset=self.state_machine.abs_line_offset() + 1,
              node=fieldlist, initial_state='RFC2822List',
              blank_finish=blank_finish)
        self.goto_line(newline_offset)
        if not blank_finish:
            self.parent += self.unindent_warning(
                  'RFC2822-style field list')
        return [], next_state, []

    def rfc2822_field(self, match):
        name = match.string[:match.string.find(':')]
        indented, indent, line_offset, blank_finish = \
              self.state_machine.get_first_known_indented(match.end(),
                                                          until_blank=True)
        fieldnode = nodes.field()
        fieldnode += nodes.field_name(name, name)
        fieldbody = nodes.field_body('\n'.join(indented))
        fieldnode += fieldbody
        if indented:
            self.nested_parse(indented, input_offset=line_offset,
                              node=fieldbody)
        return fieldnode, blank_finish


class SpecializedBody(Body):

    """
    Superclass for second and subsequent compound element members.  Compound
    elements are lists and list-like constructs.

    All transition methods are disabled (redefined as `invalid_input`).
    Override individual methods in subclasses to re-enable.

    For example, once an initial bullet list item, say, is recognized, the
    `BulletList` subclass takes over, with a "bullet_list" node as its
    container.  Upon encountering the initial bullet list item, `Body.bullet`
    calls its ``self.nested_list_parse`` (`RSTState.nested_list_parse`), which
    starts up a nested parsing session with `BulletList` as the initial state.
    Only the ``bullet`` transition method is enabled in `BulletList`; as long
    as only bullet list items are encountered, they are parsed and inserted
    into the container.  The first construct which is *not* a bullet list item
    triggers the `invalid_input` method, which ends the nested parse and
    closes the container.  `BulletList` needs to recognize input that is
    invalid in the context of a bullet list, which means everything *other
    than* bullet list items, so it inherits the transition list created in
    `Body`.
    """

    def invalid_input(self, match=None, context=None, next_state=None):
        """Not a compound element member. Abort this state machine."""
        self.state_machine.previous_line() # back up so parent SM can reassess
        raise EOFError

    indent = invalid_input
    bullet = invalid_input
    enumerator = invalid_input
    field_marker = invalid_input
    option_marker = invalid_input
    doctest = invalid_input
    line_block = invalid_input
    grid_table_top = invalid_input
    simple_table_top = invalid_input
    explicit_markup = invalid_input
    anonymous = invalid_input
    line = invalid_input
    text = invalid_input


class BulletList(SpecializedBody):

    """Second and subsequent bullet_list list_items."""

    def bullet(self, match, context, next_state):
        """Bullet list item."""
        if match.string[0] != self.parent['bullet']:
            # different bullet: new list
            self.invalid_input()
        listitem, blank_finish = self.list_item(match.end())
        self.parent += listitem
        self.blank_finish = blank_finish
        return [], next_state, []


class DefinitionList(SpecializedBody):

    """Second and subsequent definition_list_items."""

    def text(self, match, context, next_state):
        """Definition lists."""
        return [match.string], 'Definition', []


class EnumeratedList(SpecializedBody):

    """Second and subsequent enumerated_list list_items."""

    def enumerator(self, match, context, next_state):
        """Enumerated list item."""
        format, sequence, text, ordinal = self.parse_enumerator(
              match, self.parent['enumtype'])
        if ( format != self.format
             or (sequence != '#' and (sequence != self.parent['enumtype']
                                      or self.auto
                                      or ordinal != (self.lastordinal + 1)))
             or not self.is_enumerated_list_item(ordinal, sequence, format)):
            # different enumeration: new list
            self.invalid_input()
        if sequence == '#':
            self.auto = 1
        listitem, blank_finish = self.list_item(match.end())
        self.parent += listitem
        self.blank_finish = blank_finish
        self.lastordinal = ordinal
        return [], next_state, []


class FieldList(SpecializedBody):

    """Second and subsequent field_list fields."""

    def field_marker(self, match, context, next_state):
        """Field list field."""
        field, blank_finish = self.field(match)
        self.parent += field
        self.blank_finish = blank_finish
        return [], next_state, []


class OptionList(SpecializedBody):

    """Second and subsequent option_list option_list_items."""

    def option_marker(self, match, context, next_state):
        """Option list item."""
        try:
            option_list_item, blank_finish = self.option_list_item(match)
        except MarkupError:
            self.invalid_input()
        self.parent += option_list_item
        self.blank_finish = blank_finish
        return [], next_state, []


class RFC2822List(SpecializedBody, RFC2822Body):

    """Second and subsequent RFC2822-style field_list fields."""

    patterns = RFC2822Body.patterns
    initial_transitions = RFC2822Body.initial_transitions

    def rfc2822(self, match, context, next_state):
        """RFC2822-style field list item."""
        field, blank_finish = self.rfc2822_field(match)
        self.parent += field
        self.blank_finish = blank_finish
        return [], 'RFC2822List', []

    blank = SpecializedBody.invalid_input


class ExtensionOptions(FieldList):

    """
    Parse field_list fields for extension options.

    No nested parsing is done (including inline markup parsing).
    """

    def parse_field_body(self, indented, offset, node):
        """Override `Body.parse_field_body` for simpler parsing."""
        lines = []
        for line in list(indented) + ['']:
            if line.strip():
                lines.append(line)
            elif lines:
                text = '\n'.join(lines)
                node += nodes.paragraph(text, text)
                lines = []


class LineBlock(SpecializedBody):

    """Second and subsequent lines of a line_block."""

    blank = SpecializedBody.invalid_input

    def line_block(self, match, context, next_state):
        """New line of line block."""
        lineno = self.state_machine.abs_line_number()
        line, messages, blank_finish = self.line_block_line(match, lineno)
        self.parent += line
        self.parent.parent += messages
        self.blank_finish = blank_finish
        return [], next_state, []


class Explicit(SpecializedBody):

    """Second and subsequent explicit markup construct."""

    def explicit_markup(self, match, context, next_state):
        """Footnotes, hyperlink targets, directives, comments."""
        nodelist, blank_finish = self.explicit_construct(match)
        self.parent += nodelist
        self.blank_finish = blank_finish
        return [], next_state, []

    def anonymous(self, match, context, next_state):
        """Anonymous hyperlink targets."""
        nodelist, blank_finish = self.anonymous_target(match)
        self.parent += nodelist
        self.blank_finish = blank_finish
        return [], next_state, []

    blank = SpecializedBody.invalid_input


class SubstitutionDef(Body):

    """
    Parser for the contents of a substitution_definition element.
    """

    patterns = {
          'embedded_directive': re.compile(r'(%s)::( +|$)'
                                           % Inliner.simplename, re.UNICODE),
          'text': r''}
    initial_transitions = ['embedded_directive', 'text']

    def embedded_directive(self, match, context, next_state):
        nodelist, blank_finish = self.directive(match,
                                                alt=self.parent['names'][0])
        self.parent += nodelist
        if not self.state_machine.at_eof():
            self.blank_finish = blank_finish
        raise EOFError

    def text(self, match, context, next_state):
        if not self.state_machine.at_eof():
            self.blank_finish = self.state_machine.is_next_line_blank()
        raise EOFError


class Text(RSTState):

    """
    Classifier of second line of a text block.

    Could be a paragraph, a definition list item, or a title.
    """

    patterns = {'underline': Body.patterns['line'],
                'text': r''}
    initial_transitions = [('underline', 'Body'), ('text', 'Body')]

    def blank(self, match, context, next_state):
        """End of paragraph."""
        # NOTE: self.paragraph returns [ node, system_message(s) ], literalnext
        paragraph, literalnext = self.paragraph(
              context, self.state_machine.abs_line_number() - 1)
        self.parent += paragraph
        if literalnext:
            self.parent += self.literal_block()
        return [], 'Body', []

    def eof(self, context):
        if context:
            self.blank(None, context, None)
        return []

    def indent(self, match, context, next_state):
        """Definition list item."""
        definitionlist = nodes.definition_list()
        definitionlistitem, blank_finish = self.definition_list_item(context)
        definitionlist += definitionlistitem
        self.parent += definitionlist
        offset = self.state_machine.line_offset + 1   # next line
        newline_offset, blank_finish = self.nested_list_parse(
              self.state_machine.input_lines[offset:],
              input_offset=self.state_machine.abs_line_offset() + 1,
              node=definitionlist, initial_state='DefinitionList',
              blank_finish=blank_finish, blank_finish_state='Definition')
        self.goto_line(newline_offset)
        if not blank_finish:
            self.parent += self.unindent_warning('Definition list')
        return [], 'Body', []

    def underline(self, match, context, next_state):
        """Section title."""
        lineno = self.state_machine.abs_line_number()
        title = context[0].rstrip()
        underline = match.string.rstrip()
        source = title + '\n' + underline
        messages = []
        if column_width(title) > len(underline):
            if len(underline) < 4:
                if self.state_machine.match_titles:
                    msg = self.reporter.info(
                        'Possible title underline, too short for the title.\n'
                        "Treating it as ordinary text because it's so short.",
                        line=lineno)
                    self.parent += msg
                raise statemachine.TransitionCorrection('text')
            else:
                blocktext = context[0] + '\n' + self.state_machine.line
                msg = self.reporter.warning('Title underline too short.',
                    nodes.literal_block(blocktext, blocktext), line=lineno)
                messages.append(msg)
        if not self.state_machine.match_titles:
            blocktext = context[0] + '\n' + self.state_machine.line
            # We need get_source_and_line() here to report correctly
            src, srcline = self.state_machine.get_source_and_line()
            # TODO: why is abs_line_number() == srcline+1
            # if the error is in a table (try with test_tables.py)?
            # print "get_source_and_line", srcline
            # print "abs_line_number", self.state_machine.abs_line_number()
            msg = self.reporter.severe('Unexpected section title.',
                nodes.literal_block(blocktext, blocktext),
                source=src, line=srcline)
            self.parent += messages
            self.parent += msg
            return [], next_state, []
        style = underline[0]
        context[:] = []
        self.section(title, source, style, lineno - 1, messages)
        return [], next_state, []

    def text(self, match, context, next_state):
        """Paragraph."""
        startline = self.state_machine.abs_line_number() - 1
        msg = None
        try:
            block = self.state_machine.get_text_block(flush_left=True)
        except statemachine.UnexpectedIndentationError as err:
            block, src, srcline = err.args
            msg = self.reporter.error('Unexpected indentation.',
                                      source=src, line=srcline)
        lines = context + list(block)
        paragraph, literalnext = self.paragraph(lines, startline)
        self.parent += paragraph
        self.parent += msg
        if literalnext:
            try:
                self.state_machine.next_line()
            except EOFError:
                pass
            self.parent += self.literal_block()
        return [], next_state, []

    def literal_block(self):
        """Return a list of nodes."""
        indented, indent, offset, blank_finish = \
              self.state_machine.get_indented()
        while indented and not indented[-1].strip():
            indented.trim_end()
        if not indented:
            return self.quoted_literal_block()
        data = '\n'.join(indented)
        literal_block = nodes.literal_block(data, data)
        literal_block.line = offset + 1
        nodelist = [literal_block]
        if not blank_finish:
            nodelist.append(self.unindent_warning('Literal block'))
        return nodelist

    def quoted_literal_block(self):
        abs_line_offset = self.state_machine.abs_line_offset()
        offset = self.state_machine.line_offset
        parent_node = nodes.Element()
        new_abs_offset = self.nested_parse(
            self.state_machine.input_lines[offset:],
            input_offset=abs_line_offset, node=parent_node, match_titles=False,
            state_machine_kwargs={'state_classes': (QuotedLiteralBlock,),
                                  'initial_state': 'QuotedLiteralBlock'})
        self.goto_line(new_abs_offset)
        return parent_node.children

    def definition_list_item(self, termline):
        indented, indent, line_offset, blank_finish = \
              self.state_machine.get_indented()
        itemnode = nodes.definition_list_item(
            '\n'.join(termline + list(indented)))
        lineno = self.state_machine.abs_line_number() - 1
        (itemnode.source,
         itemnode.line) = self.state_machine.get_source_and_line(lineno)
        termlist, messages = self.term(termline, lineno)
        itemnode += termlist
        definition = nodes.definition('', *messages)
        itemnode += definition
        if termline[0][-2:] == '::':
            definition += self.reporter.info(
                  'Blank line missing before literal block (after the "::")? '
                  'Interpreted as a definition list item.',
                  line=lineno+1)
        self.nested_parse(indented, input_offset=line_offset, node=definition)
        return itemnode, blank_finish

    classifier_delimiter = re.compile(' +: +')

    def term(self, lines, lineno):
        """Return a definition_list's term and optional classifiers."""
        assert len(lines) == 1
        text_nodes, messages = self.inline_text(lines[0], lineno)
        term_node = nodes.term()
        (term_node.source,
         term_node.line) = self.state_machine.get_source_and_line(lineno)
        term_node.rawsource = unescape(lines[0])
        node_list = [term_node]
        for i in range(len(text_nodes)):
            node = text_nodes[i]
            if isinstance(node, nodes.Text):
                parts = self.classifier_delimiter.split(node.rawsource)
                if len(parts) == 1:
                    node_list[-1] += node
                else:

                    node_list[-1] += nodes.Text(parts[0].rstrip())
                    for part in parts[1:]:
                        classifier_node = nodes.classifier('', part)
                        node_list.append(classifier_node)
            else:
                node_list[-1] += node
        return node_list, messages


class SpecializedText(Text):

    """
    Superclass for second and subsequent lines of Text-variants.

    All transition methods are disabled. Override individual methods in
    subclasses to re-enable.
    """

    def eof(self, context):
        """Incomplete construct."""
        return []

    def invalid_input(self, match=None, context=None, next_state=None):
        """Not a compound element member. Abort this state machine."""
        raise EOFError

    blank = invalid_input
    indent = invalid_input
    underline = invalid_input
    text = invalid_input


class Definition(SpecializedText):

    """Second line of potential definition_list_item."""

    def eof(self, context):
        """Not a definition."""
        self.state_machine.previous_line(2) # so parent SM can reassess
        return []

    def indent(self, match, context, next_state):
        """Definition list item."""
        itemnode, blank_finish = self.definition_list_item(context)
        self.parent += itemnode
        self.blank_finish = blank_finish
        return [], 'DefinitionList', []


class Line(SpecializedText):

    """
    Second line of over- & underlined section title or transition marker.
    """

    eofcheck = 1                        # @@@ ???
    """Set to 0 while parsing sections, so that we don't catch the EOF."""

    def eof(self, context):
        """Transition marker at end of section or document."""
        marker = context[0].strip()
        if self.memo.section_bubble_up_kludge:
            self.memo.section_bubble_up_kludge = False
        elif len(marker) < 4:
            self.state_correction(context)
        if self.eofcheck:               # ignore EOFError with sections
            lineno = self.state_machine.abs_line_number() - 1
            transition = nodes.transition(rawsource=context[0])
            transition.line = lineno
            self.parent += transition
        self.eofcheck = 1
        return []

    def blank(self, match, context, next_state):
        """Transition marker."""
        src, srcline = self.state_machine.get_source_and_line()
        marker = context[0].strip()
        if len(marker) < 4:
            self.state_correction(context)
        transition = nodes.transition(rawsource=marker)
        transition.source = src
        transition.line = srcline - 1
        self.parent += transition
        return [], 'Body', []

    def text(self, match, context, next_state):
        """Potential over- & underlined title."""
        lineno = self.state_machine.abs_line_number() - 1
        overline = context[0]
        title = match.string
        underline = ''
        try:
            underline = self.state_machine.next_line()
        except EOFError:
            blocktext = overline + '\n' + title
            if len(overline.rstrip()) < 4:
                self.short_overline(context, blocktext, lineno, 2)
            else:
                msg = self.reporter.severe(
                    'Incomplete section title.',
                    nodes.literal_block(blocktext, blocktext),
                    line=lineno)
                self.parent += msg
                return [], 'Body', []
        source = '%s\n%s\n%s' % (overline, title, underline)
        overline = overline.rstrip()
        underline = underline.rstrip()
        if not self.transitions['underline'][0].match(underline):
            blocktext = overline + '\n' + title + '\n' + underline
            if len(overline.rstrip()) < 4:
                self.short_overline(context, blocktext, lineno, 2)
            else:
                msg = self.reporter.severe(
                    'Missing matching underline for section title overline.',
                    nodes.literal_block(source, source),
                    line=lineno)
                self.parent += msg
                return [], 'Body', []
        elif overline != underline:
            blocktext = overline + '\n' + title + '\n' + underline
            if len(overline.rstrip()) < 4:
                self.short_overline(context, blocktext, lineno, 2)
            else:
                msg = self.reporter.severe(
                      'Title overline & underline mismatch.',
                      nodes.literal_block(source, source),
                      line=lineno)
                self.parent += msg
                return [], 'Body', []
        title = title.rstrip()
        messages = []
        if column_width(title) > len(overline):
            blocktext = overline + '\n' + title + '\n' + underline
            if len(overline.rstrip()) < 4:
                self.short_overline(context, blocktext, lineno, 2)
            else:
                msg = self.reporter.warning(
                      'Title overline too short.',
                      nodes.literal_block(source, source),
                      line=lineno)
                messages.append(msg)
        style = (overline[0], underline[0])
        self.eofcheck = 0               # @@@ not sure this is correct
        self.section(title.lstrip(), source, style, lineno + 1, messages)
        self.eofcheck = 1
        return [], 'Body', []

    indent = text                       # indented title

    def underline(self, match, context, next_state):
        overline = context[0]
        blocktext = overline + '\n' + self.state_machine.line
        lineno = self.state_machine.abs_line_number() - 1
        if len(overline.rstrip()) < 4:
            self.short_overline(context, blocktext, lineno, 1)
        msg = self.reporter.error(
              'Invalid section title or transition marker.',
              nodes.literal_block(blocktext, blocktext),
              line=lineno)
        self.parent += msg
        return [], 'Body', []

    def short_overline(self, context, blocktext, lineno, lines=1):
        msg = self.reporter.info(
            'Possible incomplete section title.\nTreating the overline as '
            "ordinary text because it's so short.",
            line=lineno)
        self.parent += msg
        self.state_correction(context, lines)

    def state_correction(self, context, lines=1):
        self.state_machine.previous_line(lines)
        context[:] = []
        raise statemachine.StateCorrection('Body', 'text')


class QuotedLiteralBlock(RSTState):

    """
    Nested parse handler for quoted (unindented) literal blocks.

    Special-purpose.  Not for inclusion in `state_classes`.
    """

    patterns = {'initial_quoted': r'(%(nonalphanum7bit)s)' % Body.pats,
                'text': r''}
    initial_transitions = ('initial_quoted', 'text')

    def __init__(self, state_machine, debug=False):
        RSTState.__init__(self, state_machine, debug)
        self.messages = []
        self.initial_lineno = None

    def blank(self, match, context, next_state):
        if context:
            raise EOFError
        else:
            return context, next_state, []

    def eof(self, context):
        if context:
            src, srcline = self.state_machine.get_source_and_line(
                                                        self.initial_lineno)
            text = '\n'.join(context)
            literal_block = nodes.literal_block(text, text)
            literal_block.source = src
            literal_block.line = srcline
            self.parent += literal_block
        else:
            self.parent += self.reporter.warning(
                'Literal block expected; none found.',
                line=self.state_machine.abs_line_number())
                # src not available, because statemachine.input_lines is empty
            self.state_machine.previous_line()
        self.parent += self.messages
        return []

    def indent(self, match, context, next_state):
        assert context, ('QuotedLiteralBlock.indent: context should not '
                         'be empty!')
        self.messages.append(
            self.reporter.error('Unexpected indentation.',
                                line=self.state_machine.abs_line_number()))
        self.state_machine.previous_line()
        raise EOFError

    def initial_quoted(self, match, context, next_state):
        """Match arbitrary quote character on the first line only."""
        self.remove_transition('initial_quoted')
        quote = match.string[0]
        pattern = re.compile(re.escape(quote), re.UNICODE)
        # New transition matches consistent quotes only:
        self.add_transition('quoted',
                            (pattern, self.quoted, self.__class__.__name__))
        self.initial_lineno = self.state_machine.abs_line_number()
        return [match.string], next_state, []

    def quoted(self, match, context, next_state):
        """Match consistent quotes on subsequent lines."""
        context.append(match.string)
        return context, next_state, []

    def text(self, match, context, next_state):
        if context:
            self.messages.append(
                self.reporter.error('Inconsistent literal block quoting.',
                                   line=self.state_machine.abs_line_number()))
            self.state_machine.previous_line()
        raise EOFError


state_classes = (Body, BulletList, DefinitionList, EnumeratedList, FieldList,
                 OptionList, LineBlock, ExtensionOptions, Explicit, Text,
                 Definition, Line, SubstitutionDef, RFC2822Body, RFC2822List)
"""Standard set of State classes used to start `RSTStateMachine`."""
#@+node:ekr.20130417081749.10502: *4* @@edit parsers\rst\directives\__init__.py
@language python
# $Id: __init__.py 7486 2012-07-11 12:25:14Z milde $
# Author: David Goodger <goodger@python.org>
# Copyright: This module has been placed in the public domain.

"""
This package contains directive implementation modules.
"""

__docformat__ = 'reStructuredText'

import re
import codecs
import sys

from docutils import nodes
from docutils.parsers.rst.languages import en as _fallback_language_module
if sys.version_info < (2,5):
    from docutils._compat import __import__


_directive_registry = {
      'attention': ('admonitions', 'Attention'),
      'caution': ('admonitions', 'Caution'),
      'code': ('body', 'CodeBlock'),
      'danger': ('admonitions', 'Danger'),
      'error': ('admonitions', 'Error'),
      'important': ('admonitions', 'Important'),
      'note': ('admonitions', 'Note'),
      'tip': ('admonitions', 'Tip'),
      'hint': ('admonitions', 'Hint'),
      'warning': ('admonitions', 'Warning'),
      'admonition': ('admonitions', 'Admonition'),
      'sidebar': ('body', 'Sidebar'),
      'topic': ('body', 'Topic'),
      'line-block': ('body', 'LineBlock'),
      'parsed-literal': ('body', 'ParsedLiteral'),
      'math': ('body', 'MathBlock'),
      'rubric': ('body', 'Rubric'),
      'epigraph': ('body', 'Epigraph'),
      'highlights': ('body', 'Highlights'),
      'pull-quote': ('body', 'PullQuote'),
      'compound': ('body', 'Compound'),
      'container': ('body', 'Container'),
      #'questions': ('body', 'question_list'),
      'table': ('tables', 'RSTTable'),
      'csv-table': ('tables', 'CSVTable'),
      'list-table': ('tables', 'ListTable'),
      'image': ('images', 'Image'),
      'figure': ('images', 'Figure'),
      'contents': ('parts', 'Contents'),
      'sectnum': ('parts', 'Sectnum'),
      'header': ('parts', 'Header'),
      'footer': ('parts', 'Footer'),
      #'footnotes': ('parts', 'footnotes'),
      #'citations': ('parts', 'citations'),
      'target-notes': ('references', 'TargetNotes'),
      'meta': ('html', 'Meta'),
      #'imagemap': ('html', 'imagemap'),
      'raw': ('misc', 'Raw'),
      'include': ('misc', 'Include'),
      'replace': ('misc', 'Replace'),
      'unicode': ('misc', 'Unicode'),
      'class': ('misc', 'Class'),
      'role': ('misc', 'Role'),
      'default-role': ('misc', 'DefaultRole'),
      'title': ('misc', 'Title'),
      'date': ('misc', 'Date'),
      'restructuredtext-test-directive': ('misc', 'TestDirective'),}
"""Mapping of directive name to (module name, class name).  The
directive name is canonical & must be lowercase.  Language-dependent
names are defined in the ``language`` subpackage."""

_directives = {}
"""Cache of imported directives."""

def directive(directive_name, language_module, document):
    """
    Locate and return a directive function from its language-dependent name.
    If not found in the current language, check English.  Return None if the
    named directive cannot be found.
    """
    normname = directive_name.lower()
    messages = []
    msg_text = []
    if normname in _directives:
        return _directives[normname], messages
    canonicalname = None
    try:
        canonicalname = language_module.directives[normname]
    except AttributeError as error:
        msg_text.append('Problem retrieving directive entry from language '
                        'module %r: %s.' % (language_module, error))
    except KeyError:
        msg_text.append('No directive entry for "%s" in module "%s".'
                        % (directive_name, language_module.__name__))
    if not canonicalname:
        try:
            canonicalname = _fallback_language_module.directives[normname]
            msg_text.append('Using English fallback for directive "%s".'
                            % directive_name)
        except KeyError:
            msg_text.append('Trying "%s" as canonical directive name.'
                            % directive_name)
            # The canonical name should be an English name, but just in case:
            canonicalname = normname
    if msg_text:
        message = document.reporter.info(
            '\n'.join(msg_text), line=document.current_line)
        messages.append(message)
    try:
        modulename, classname = _directive_registry[canonicalname]
    except KeyError:
        # Error handling done by caller.
        return None, messages
    try:
        module = __import__(modulename, globals(), locals(), level=1)
    except ImportError as detail:
        messages.append(document.reporter.error(
            'Error importing directive module "%s" (directive "%s"):\n%s'
            % (modulename, directive_name, detail),
            line=document.current_line))
        return None, messages
    try:
        directive = getattr(module, classname)
        _directives[normname] = directive
    except AttributeError:
        messages.append(document.reporter.error(
            'No directive class "%s" in module "%s" (directive "%s").'
            % (classname, modulename, directive_name),
            line=document.current_line))
        return None, messages
    return directive, messages

def register_directive(name, directive):
    """
    Register a nonstandard application-defined directive function.
    Language lookups are not needed for such functions.
    """
    _directives[name] = directive

def flag(argument):
    """
    Check for a valid flag option (no argument) and return ``None``.
    (Directive option conversion function.)

    Raise ``ValueError`` if an argument is found.
    """
    if argument and argument.strip():
        raise ValueError('no argument is allowed; "%s" supplied' % argument)
    else:
        return None

def unchanged_required(argument):
    """
    Return the argument text, unchanged.
    (Directive option conversion function.)

    Raise ``ValueError`` if no argument is found.
    """
    if argument is None:
        raise ValueError('argument required but none supplied')
    else:
        return argument  # unchanged!

def unchanged(argument):
    """
    Return the argument text, unchanged.
    (Directive option conversion function.)

    No argument implies empty string ("").
    """
    if argument is None:
        return u''
    else:
        return argument  # unchanged!

def path(argument):
    """
    Return the path argument unwrapped (with newlines removed).
    (Directive option conversion function.)

    Raise ``ValueError`` if no argument is found.
    """
    if argument is None:
        raise ValueError('argument required but none supplied')
    else:
        path = ''.join([s.strip() for s in argument.splitlines()])
        return path

def uri(argument):
    """
    Return the URI argument with whitespace removed.
    (Directive option conversion function.)

    Raise ``ValueError`` if no argument is found.
    """
    if argument is None:
        raise ValueError('argument required but none supplied')
    else:
        uri = ''.join(argument.split())
        return uri

def nonnegative_int(argument):
    """
    Check for a nonnegative integer argument; raise ``ValueError`` if not.
    (Directive option conversion function.)
    """
    value = int(argument)
    if value < 0:
        raise ValueError('negative value; must be positive or zero')
    return value

def percentage(argument):
    """
    Check for an integer percentage value with optional percent sign.
    """
    try:
        argument = argument.rstrip(' %')
    except AttributeError:
        pass
    return nonnegative_int(argument)

length_units = ['em', 'ex', 'px', 'in', 'cm', 'mm', 'pt', 'pc']

def get_measure(argument, units):
    """
    Check for a positive argument of one of the units and return a
    normalized string of the form "<value><unit>" (without space in
    between).

    To be called from directive option conversion functions.
    """
    match = re.match(r'^([0-9.]+) *(%s)$' % '|'.join(units), argument)
    try:
        assert match is not None
        float(match.group(1))
    except (AssertionError, ValueError):
        raise ValueError(
            'not a positive measure of one of the following units:\n%s'
            % ' '.join(['"%s"' % i for i in units]))
    return match.group(1) + match.group(2)

def length_or_unitless(argument):
    return get_measure(argument, length_units + [''])

def length_or_percentage_or_unitless(argument, default=''):
    """
    Return normalized string of a length or percentage unit.

    Add <default> if there is no unit. Raise ValueError if the argument is not
    a positive measure of one of the valid CSS units (or without unit).

    >>> length_or_percentage_or_unitless('3 pt')
    '3pt'
    >>> length_or_percentage_or_unitless('3%', 'em')
    '3%'
    >>> length_or_percentage_or_unitless('3')
    '3'
    >>> length_or_percentage_or_unitless('3', 'px')
    '3px'
    """
    try:
        return get_measure(argument, length_units + ['%'])
    except ValueError:
        return get_measure(argument, ['']) + default

def class_option(argument):
    """
    Convert the argument into a list of ID-compatible strings and return it.
    (Directive option conversion function.)

    Raise ``ValueError`` if no argument is found.
    """
    if argument is None:
        raise ValueError('argument required but none supplied')
    names = argument.split()
    class_names = []
    for name in names:
        class_name = nodes.make_id(name)
        if not class_name:
            raise ValueError('cannot make "%s" into a class name' % name)
        class_names.append(class_name)
    return class_names

unicode_pattern = re.compile(
    r'(?:0x|x|\\x|U\+?|\\u)([0-9a-f]+)$|&#x([0-9a-f]+);$', re.IGNORECASE)

def unicode_code(code):
    r"""
    Convert a Unicode character code to a Unicode character.
    (Directive option conversion function.)

    Codes may be decimal numbers, hexadecimal numbers (prefixed by ``0x``,
    ``x``, ``\x``, ``U+``, ``u``, or ``\u``; e.g. ``U+262E``), or XML-style
    numeric character entities (e.g. ``&#x262E;``).  Other text remains as-is.

    Raise ValueError for illegal Unicode code values.
    """
    try:
        if code.isdigit():                  # decimal number
            if sys.version_info < (3,):
                return unichr(int(code))
            else:
                return str(int(code))
        else:
            match = unicode_pattern.match(code)
            if match:                       # hex number
                value = match.group(1) or match.group(2)
                ### return unichr(int(value, 16))
                if sys.version_info < (3,):
                    return unichr(int(value, 16))
                else:
                    return str(int(value, 16))
            else:                           # other text
                return code
    except OverflowError as detail:
        raise ValueError('code too large (%s)' % detail)

def single_char_or_unicode(argument):
    """
    A single character is returned as-is.  Unicode characters codes are
    converted as in `unicode_code`.  (Directive option conversion function.)
    """
    char = unicode_code(argument)
    if len(char) > 1:
        raise ValueError('%r invalid; must be a single character or '
                         'a Unicode code' % char)
    return char

def single_char_or_whitespace_or_unicode(argument):
    """
    As with `single_char_or_unicode`, but "tab" and "space" are also supported.
    (Directive option conversion function.)
    """
    if argument == 'tab':
        char = '\t'
    elif argument == 'space':
        char = ' '
    else:
        char = single_char_or_unicode(argument)
    return char

def positive_int(argument):
    """
    Converts the argument into an integer.  Raises ValueError for negative,
    zero, or non-integer values.  (Directive option conversion function.)
    """
    value = int(argument)
    if value < 1:
        raise ValueError('negative or zero value; must be positive')
    return value

def positive_int_list(argument):
    """
    Converts a space- or comma-separated list of values into a Python list
    of integers.
    (Directive option conversion function.)

    Raises ValueError for non-positive-integer values.
    """
    if ',' in argument:
        entries = argument.split(',')
    else:
        entries = argument.split()
    return [positive_int(entry) for entry in entries]

def encoding(argument):
    """
    Verfies the encoding argument by lookup.
    (Directive option conversion function.)

    Raises ValueError for unknown encodings.
    """
    try:
        codecs.lookup(argument)
    except LookupError:
        raise ValueError('unknown encoding: "%s"' % argument)
    return argument

def choice(argument, values):
    """
    Directive option utility function, supplied to enable options whose
    argument must be a member of a finite set of possible values (must be
    lower case).  A custom conversion function must be written to use it.  For
    example::

        from docutils.parsers.rst import directives

        def yesno(argument):
            return directives.choice(argument, ('yes', 'no'))

    Raise ``ValueError`` if no argument is found or if the argument's value is
    not valid (not an entry in the supplied list).
    """
    try:
        value = argument.lower().strip()
    except AttributeError:
        raise ValueError('must supply an argument; choose from %s'
                         % format_values(values))
    if value in values:
        return value
    else:
        raise ValueError('"%s" unknown; choose from %s'
                         % (argument, format_values(values)))

def format_values(values):
    return '%s, or "%s"' % (', '.join(['"%s"' % s for s in values[:-1]]),
                            values[-1])
#@+node:ekr.20130415213138.10375: *3* Import docutils from leo/extensions
#@+node:ekr.20130415213138.10376: *4* Not used
#@+node:ekr.20130415105054.10371: *5* @@button rst to html
'''Convert rst text to html.'''

import docutils.core
in_f = g.fileLikeObject()
out_f = g.fileLikeObject()
in_f.write(p.b)
docutils.core.publish_file(source=in_f,destination=out_f,writer_name='html')
s = out_f.read()
# To do: strip out cruft: everything undil <div class="document"> and the last 3 lines.
print(s)
#@+node:ekr.20130415102055.10369: *5* << define html_s >> (F1) (not used)
# must start with a '<'

html_s = '''
<h3>Welcome to Leo's help system</h3>

To learn about <tt>&lt;Alt-X&gt;</tt> commands, type:
<pre>
    &lt;Alt-X&gt;help-for-minibuffer&lt;Enter&gt;
</pre>
To get a list of help topics, type:
<pre>
    &lt;Alt-X&gt;help-&lt;tab&gt;
</pre>
For Leo commands (tab completion allowed), type:
<pre>
    &lt;Alt-X&gt;help-for-command&lt;Enter&gt;
    &lt;a Leo command name&gt;&lt;Enter&gt;
</pre>   
To use Python's help system, type:
<pre>
    &lt;Alt-X&gt;help-for-python&lt;Enter&gt;
    &lt;a python symbol&gt;&lt;Enter&gt;
</pre>
'''
#@+node:ekr.20041219071407: *4* g.importExtension
def importExtension (moduleName,pluginName=None,verbose=False,required=False):

    '''Try to import a module.  If that fails,
    try to import the module from Leo's extensions directory.

    moduleName is the module's name, without file extension.'''

    module = g.importModule(moduleName,pluginName=pluginName,verbose=verbose)
    if not module:
        g.pr("Warning: '%s' failed to import '%s'" % (
            pluginName,moduleName))
    return module
#@+node:ekr.20041219095213.1: *4* g.importModule
def importModule (moduleName,pluginName=None,verbose=False):

    '''Try to import a module as Python's import command does.

    moduleName is the module's name, without file extension.'''

    # Important: g is Null during startup.
    trace = False and not g.unitTesting
    module = sys.modules.get(moduleName)
    if module:
        return module
    if verbose: g.blue('loading %s' % moduleName)
    exceptions = [] 
    try:
        theFile = None
        try:
            # New in Leo 4.7. We no longer add Leo directories to sys.path,
            # so search extensions and external directories here explicitly.
            for findPath in (None,'extensions','external'):
                if findPath:
                    findPath2 = g.os_path_finalize_join(
                        g.app.loadDir,'..',findPath)
                    findPath = [findPath2]
                if trace: g.trace('findPath',findPath)
                try:
                    data = imp.find_module(moduleName,findPath) # This can open the file.
                    theFile,pathname,description = data
                    if trace: g.trace(theFile,moduleName,pathname)
                    module = imp.load_module(moduleName,theFile,pathname,description)
                    if module: 
                        g.es("%s loaded" % moduleName)
                        break
                except Exception:
                    t, v, tb = sys.exc_info()
                    del tb  # don't need the traceback
                    v = v or str(t) # in case v is empty, we'll at least have the execption type
                    if trace: g.trace(v,moduleName,findPath)
                    if v not in exceptions:
                        exceptions.append(v)
            else:
                #unable to load module, display all exception messages
                if verbose:
                    for e in exceptions:
                        g.warning(e) 
        except Exception: # Importing a module can throw exceptions other than ImportError.
            if verbose:
                t, v, tb = sys.exc_info()
                del tb  # don't need the traceback
                v = v or str(t) # in case v is empty, we'll at least have the execption type
                g.es_exception(v)
    finally:
        if theFile: theFile.close()

    if not module and verbose:
        g.cantImport(moduleName,pluginName=pluginName,verbose=verbose)
    return module
#@+node:ekr.20130412173637.10330: *4* << define rst_s >> (F1)
rst_s = '''

**Welcome to Leo's help system.**

To learn about ``<Alt-X>`` commands, type::
    
    <Alt-X>help-for-minibuffer<Enter>
    
To get a list of help topics, type::
    
    <Alt-X>help-<tab>
    
For Leo commands (tab completion allowed), type::
    
    <Alt-X>help-for-command<Enter>
    <a Leo command name><Enter>
    
To use Python's help system, type::
    
    <Alt-X>help-for-python<Enter>
    <a python symbol><Enter>

'''
#@+node:ekr.20091224155043.6539: *4* << imports >> (leoImport)
# Required so the unit test that simulates an @auto leoImport.py will work!
import leo.core.leoGlobals as g

docutils = g.importExtension('docutils',pluginName='leoImport.py')
import string
if g.isPython3:
    import io
    StringIO = io.StringIO
else:
    import StringIO
    StringIO = StringIO.StringIO
#@+node:ekr.20050710151017: *4* << imports >> (leoEditCommands)
import leo.core.leoGlobals as g
import leo.core.leoFind as leoFind
import difflib   
docutils = g.importExtension('docutils',pluginName='leoEditCommands.py')
try:
    import enchant
except ImportError:
    enchant = None
import os
import re
if g.isPython3:
    from functools import reduce
import shlex
import string
import subprocess # Always exists in Python 2.6 and above.
import sys
#@+node:ekr.20100908120927.5971: *4* << imports >> (leoRst)
verbose = False
import leo.core.leoGlobals as g
docutils = g.importExtension('docutils',pluginName='leoRst.py',verbose=True)
if docutils:
    try:
        if verbose: print('leoRst.py',docutils)
        from docutils import parsers
        if verbose or not parsers: print('leoRst.py',parsers)
        from docutils.parsers import rst
        if verbose or not rst: print('leoRst.py',rst)
        if not parsers or not rst:
            docutils = None
    except ImportError:
        docutils = None
    except Exception:
        g.es_exception()
        docutils = None
if g.isPython3:
    import html.parser as HTMLParser
else:
    import HTMLParser
try:
    import leo.plugins.mod_http as mod_http
except ImportError:
    mod_http = None
except Exception:
    # Don't let a problem with a plugin crash Leo's core!
    # g.es_print('leoRst: can not import leo.plugins.mod_http')
    # g.es_exception()
    mod_http = None
# import os
import pprint
import re
try:
    import SilverCity
except ImportError:
    SilverCity = None
if g.isPython3:
    import io
    StringIO = io.StringIO
else:
    import StringIO
    StringIO = StringIO.StringIO
# import sys
# import tempfile
#@+node:ekr.20130421002947.10752: *3* consistency check (was in punctuation chars
import sys, re
import unicodedata

# Unicode punctuation character categories
# ----------------------------------------

unicode_punctuation_categories = {
    # 'Pc': 'Connector', # not used in Docutils inline markup recognition
    'Pd': 'Dash',
    'Ps': 'Open',
    'Pe': 'Close',
    'Pi': 'Initial quote', # may behave like Ps or Pe depending on usage
    'Pf': 'Final quote', # may behave like Ps or Pe depending on usage
    'Po': 'Other'
    }
"""Unicode character categories for punctuation"""


# generate character pattern strings
# ==================================

def unicode_charlists(categories, cp_min=0, cp_max=None):
    """Return dictionary of Unicode character lists.

    For each of the `catagories`, an item contains a list with all Unicode
    characters with `cp_min` <= code-point <= `cp_max` that belong to the
    category. (The default values check every code-point supported by Python.)
    """
    # Determine highest code point with one of the given categories
    # (may shorten the search time considerably if there are many
    # categories with not too high characters):
    if cp_max is None:
        cp_max = max(x for x in xrange(sys.maxunicode + 1)
                     if unicodedata.category(unichr(x)) in categories)
        # print cp_max # => 74867 for unicode_punctuation_categories
    charlists = {}
    for cat in categories:
        charlists[cat] = [unichr(x) for x in xrange(cp_min, cp_max+1)
                          if unicodedata.category(unichr(x)) == cat]
    return charlists


# Character categories in Docutils
# --------------------------------

def punctuation_samples():

    """Docutils punctuation category sample strings.

    Return list of sample strings for the categories "Open", "Close",
    "Delimiters" and "Closing-Delimiters" used in the `inline markup
    recognition rules`_.
    """

    # Lists with characters in Unicode punctuation character categories
    cp_min = 160 # ASCII chars have special rules for backwards compatibility
    ucharlists = unicode_charlists(unicode_punctuation_categories, cp_min)

    # match opening/closing characters
    # --------------------------------
    # Rearange the lists to ensure matching characters at the same
    # index position.

    # low quotation marks are also used as closers (e.g. in Greek)
    # move them to category Pi:
    ucharlists['Ps'].remove(u'‚') # 201A  SINGLE LOW-9 QUOTATION MARK
    ucharlists['Ps'].remove(u'„') # 201E  DOUBLE LOW-9 QUOTATION MARK
    ucharlists['Pi'] += [u'‚', u'„']

    ucharlists['Pi'].remove(u'‛') # 201B  SINGLE HIGH-REVERSED-9 QUOTATION MARK
    ucharlists['Pi'].remove(u'‟') # 201F  DOUBLE HIGH-REVERSED-9 QUOTATION MARK
    ucharlists['Pf'] += [u'‛', u'‟']

    # 301F  LOW DOUBLE PRIME QUOTATION MARK misses the opening pendant:
    ucharlists['Ps'].insert(ucharlists['Pe'].index(u'\u301f'), u'\u301d')

    # print u''.join(ucharlists['Ps']).encode('utf8')
    # print u''.join(ucharlists['Pe']).encode('utf8')
    # print u''.join(ucharlists['Pi']).encode('utf8')
    # print u''.join(ucharlists['Pf']).encode('utf8')

    # The Docutils character categories
    # ---------------------------------
    #
    # The categorization of ASCII chars is non-standard to reduce both
    # false positives and need for escaping. (see `inline markup recognition
    # rules`_)

    # matching, allowed before markup
    openers = [re.escape('"\'(<[{')]
    for cat in ('Ps', 'Pi', 'Pf'):
        openers.extend(ucharlists[cat])

    # matching, allowed after markup
    closers = [re.escape('"\')>]}')]
    for cat in ('Pe', 'Pf', 'Pi'):
        closers.extend(ucharlists[cat])

    # non-matching, allowed on both sides
    delimiters = [re.escape('-/:')]
    for cat in ('Pd', 'Po'):
        delimiters.extend(ucharlists[cat])

    # non-matching, after markup
    closing_delimiters = [re.escape('.,;!?')]

    # # Test open/close matching:
    # for i in range(min(len(openers),len(closers))):
    #     print '%4d    %s    %s' % (i, openers[i].encode('utf8'),
    #                                closers[i].encode('utf8'))

    return [u''.join(chars)
            for chars in (openers, closers, delimiters, closing_delimiters)]


# Matching open/close quotes
# --------------------------

# Rule (5) requires determination of matching open/close pairs. However,
# the pairing of open/close quotes is ambigue due to  different typographic
# conventions in different languages.

quote_pairs = {u'\xbb': u'\xbb', # Swedish
               u'\u2018': u'\u201a', # Greek
               u'\u2019': u'\u2019', # Swedish
               u'\u201a': u'\u2018\u2019', # German, Polish
               u'\u201c': u'\u201e', # German
               u'\u201e': u'\u201c\u201d',
               u'\u201d': u'\u201d', # Swedish
               u'\u203a': u'\u203a', # Swedish
              }

def match_chars(c1, c2):
    try:
        i = openers.index(c1)
    except ValueError:  # c1 not in openers
        return False
    return c2 == closers[i] or c2 in quote_pairs.get(c1, '')




# print results
# =============

if __name__ == '__main__':

    # (re) create and compare the samples:
    (o, c, d, cd) = punctuation_samples()
    if o != openers:
        print '- openers = ur"""%s"""' % openers.encode('utf8')
        print '+ openers = ur"""%s"""' % o.encode('utf8')
    if c != closers:
        print '- closers = ur"""%s"""' % closers.encode('utf8')
        print '+ closers = ur"""%s"""' % c.encode('utf8')
    if d != delimiters:
        print '- delimiters = ur"%s"' % delimiters.encode('utf8')
        print '+ delimiters = ur"%s"' % d.encode('utf8')
    if cd != closing_delimiters:
        print '- closing_delimiters = ur"%s"' % closing_delimiters.encode('utf8')
        print '+ closing_delimiters = ur"%s"' % cd.encode('utf8')

    # # test prints
    # print 'openers = ', repr(openers)
    # print 'closers = ', repr(closers)
    # print 'delimiters = ', repr(delimiters)
    # print 'closing_delimiters = ', repr(closing_delimiters)

    # ucharlists = unicode_charlists(unicode_punctuation_categories)
    # for cat, chars in ucharlists.items():
    #     # print cat, chars
    #     # compact output (visible with a comprehensive font):
    #     print (u":%s: %s" % (cat, u''.join(chars))).encode('utf8')
#@-all

# Put this @language after the @all as a kind of permanent unit test.

#@@language python # Override the default .txt coloring.

#@@pagewidth 60
#@-leo
